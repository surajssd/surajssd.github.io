[{
    "title": "Religious Rituals Don't Scale With Population",
    "date": "",
    "description": "How Hindus need to introspect and evolve!",
    "body": "There are numerous Hindu festivals celebrated throughout the year. The rituals associated with each festival differ to the extent these festivals are celebrated. I am here to recount the environmental impact of the customs Hindus are involved in.\nAlthough a ritual associated with each festival and its effect on the environment is insignificant on a small scale. But with Hindus in India being close to a billion (966 million as per the 2011 census, with the population growth rate that might be almost a billion), these rituals don\u0026rsquo;t scale very well. The practices Hindus have been following, believing them benign, just a form of worshipping their god of belief. Let us look at the environmental impact of various rituals performed on particular occasions (festivals, funeral).\nDiwali Diwali is one of the major festival celebrated across the country with pomp and is a major Hindu festival. Diwali is a festival of lights celebrated in Lord Ram\u0026rsquo;s memory returning to Ayodhya after defeating Ravan. For the three-four day span of this festival, the central part of the celebration, apart from lighting the diyas, is setting off firecrackers. These firecrackers, some of them intense in sound, others vivid in colours, cause a lot of smoke and sound, thus causing Air and Noise pollution. If colourful and less noisy, the firecracker produces a lot of smoke, while the ones that make less smoke produces a lot of noise.\nThe lighting of firecrackers was okay when fewer Hindus had a means of procuring it or when Hindus themselves were lesser in number. But with close to a billion people setting these explosives off, it is sure to contribute to four-day noise pollution and contributing to air pollution. Air pollution is already rampant and getting worse every year. Given the rapidly reducing tree cover (in India precisely and in the world in general), burning firecrackers is not helping it curb in any way. Among the top nine most polluted cities, as of this writing, in the world, eight are in India.\n This custom may have begun on the Indian subcontinent after 1400 CE when gunpowder started being utilised in Indian warfare.\n So the lighting of firecrackers started only in the fifteenth century, and I am pretty sure that there was no Gunpowder at the time of Lord Ram. Gunpowder was only invented in the ninth century CE. Also, no scriptures encourage or mention the usage of gunpowder-laced explosives to celebrate the festival. Hindu fanatics who vehemently support burning firecrackers and destroying our only home (this earth) defy the Hindu belief of treating this earth as one\u0026rsquo;s mother. They are ready to exploit her, ignorant of their hypocrisy. When rational people point this out, some conservatives start indulging in whataboutery, pointing at the firecracker bursting at a new year celebration. I would like to say that regardless of the reason to burn these firecrackers, I would still oppose it. But this blog is specific to the Hindu community not able to reform itself in the face of changing circumstances.\nI think the middle ground can be established by educating folks about the harm the current firecrackers are causing to the environment. If there is an innovation in creating these firecrackers like a firecracker that generates vivid colours but does not create the awful side effects like sound and air pollution. But the government has to intervene on environmental grounds by levying heavy taxes on the sale of firecrackers, so average Indian associates burning a firecracker as burning cash. But I don\u0026rsquo;t think any political party would come forward (at least aggressively) to find a permanent solution to this problem without taking a toll on their vote bank. Hence supreme court had to intervene at the inability of the governments to act. Subsequently, the Delhi state government acted only last year to ban firecrackers. Despite this ban, there was no enforcement; if there was any, the air quality index would have reflected that. I think Science and Technology is the only solution to this problem.\nLet us not politicise this issue and look at the long term. Since this practice was established by us, so can this be abolished by us. Let nobody fool you into thinking that this act of renunciation is in any way an attack on Hindu religion or western influence. What good a ritual (illegal to the scriptures) would be if it makes life unbearable for us today and for posterity?\n",
    "ref": "/post/2021/04/rituals-dont-scale/"
  },{
    "title": "Monitor your PC with Prometheus Grafana stack",
    "date": "",
    "description": "Get a fine grained view of the happenings on your system!",
    "body": "How do you monitor your own computer? Of course, using Prometheus, node-exporter and Grafana. You might ask why would you wanna do that when you can simply use the operating system provided, \u0026ldquo;System Monitor\u0026rdquo;. Well, yes, you can use that. But the data you get from the OS System Monitor is coarse-grained. OS system monitor is not configurable, but this stack is.\nIt is like running htop but where you can go back in history, unlike htop, which only shows the current state. Using this stack of Prometheus, node-exporter, and Grafana is a proactive approach than being reactive to the problems that occur on a PC. Instead of digging later to figure out what went wrong, you are already collecting metrics so you can see on dashboards what went wrong.\nNOTE: This blog is helpful to you only if you are using a Linux desktop that you would want to monitor. Here docker is used to run tools like node-exporter, and docker does not work natively neither on OSX or Windows. Docker on OSX and Windows runs in a VM, and there is no use in monitoring a VM; the objective of this blog is to monitor your PC.\nPrerequisites  Install Docker. Install docker-compose.  Installation Download the configuration manifests git clone https://github.com/surajssd/node-monitor cd node-monitor Install the configuration There are two ways to install this stack one as a user systemd, and the other is to dump configs to a directory and manually run docker-compose commands. I would recommend using the user systemd path because it is a cleaner approach. You can interact with the service using systemctl, and journalctl and the services restart automatically once the PC is started.\nExecute the following command to install it:\nmake install-user-systemd Now go to this URL localhost:3000 to access the Grafana already installed with the Node Exporter dashboard. The login user name and password by default is admin. You can choose to change it at login.\nDebug  To view the logs of the running containers, run the following command:  journalctl --user -u node-monitor  If containers fail to start, run the following command to delete the stale containers:  docker rm -f $(docker ps -aq)  I hope you find this helpful and learn exciting stuff just watching the dashboard about your system.\n",
    "ref": "/post/2021/03/node-monitor/"
  },{
    "title": "Kubernetes The Hard Way in "Vagrant"?",
    "date": "",
    "description": "The first step in your CKA preparation!",
    "body": "If you are studying for the Certified Kubernetes Administrator (CKA) exam, you might have come across folks recommending Kelsey Hightower\u0026rsquo;s Kubernetes the Hard Way. It is an excellent first step for someone who has no idea about the components that form a Kubernetes cluster. As the name suggests, it is created so that you learn the Kubernetes building blocks the \u0026ldquo;hard way\u0026rdquo;.\nBut all that can be intimidating to someone who hasn\u0026rsquo;t played with Kubernetes ever. Also, the guide uses Google Cloud as a platform to install everything, which mandates you to have a Google Cloud account. But don\u0026rsquo;t worry, there is a version of Kubernetes the Hard Way, which runs locally, hence free. Enter Kubernetes the Hard Way Vagrant!\nKubernetes the Hard Way Vagrant, was started by folks at Kinvolk. It is a set of scripts that does the exact same thing done in Kelsey\u0026rsquo;s KTHW. The only difference is that it is automated with scripts and, at the same time, gives you the flexibility to do things manually if you like. These scripts help any newbie understand how a Kubernetes cluster can be brought up one node at a time. They can study each and every step of cluster creation and replicate those steps manually. Another minor difference is that this repository uses cri-o as a container runtime instead of containerd used in Kelsey\u0026rsquo;s tutorial.\nTo get started, all you need is a machine with 4.8 GB of spare memory and enough CPU to run seven small VMs. Install Vagrant and VirtualBox to run these VMs. Once you have these prerequisites, all you need to do to get started is run the following steps:\ngit clone https://github.com/surajssd/kubernetes-the-hard-way-vagrant cd kubernetes-the-hard-way-vagrant ./scripts/install-tools ./scripts/setup That\u0026rsquo;s all. And the ./scripts/setup will start three controller machines and three worker machines, and a load balancer machine, install the Kubernetes components and run them. See the following image to understand what the architecture looks like:\nYou can run individual script invoked by ./scripts/setup manually to get an in-depth understanding of what is happening underneath. Find more explanation on each sub-script on the README.\nThis repository is an excellent first step to doing Kelsey\u0026rsquo;s KTHW on Vagrant. I hope you find this helpful.\n",
    "ref": "/post/2021/03/kthw-vagrant/"
  },{
    "title": "How we manage Kubernetes Bangalore Meetup?",
    "date": "",
    "description": "The method of stress-free event management!",
    "body": "I took the reins of the Kubernetes Bangalore Meetup back in 2017. I have been organising the meetup since then. Earlier with Suraj Narwade, Aditya Konarde and now with Prakash Mishra. Over time the meetup has grown a lot, now it boasts about 5000 members. Organising meetup earlier was a straightforward affair, especially with Narwade and Konarde being my colleagues and friends. We could chat about the upcoming meetup at any time we would like, and it was all spontaneous. Once they left in 2019, Prakash took over as co-organiser, and we are managing the meetup since then.\nAlthough Prakash and I never worked in the same company, we managed meetup over the phone, text and did tasks as we recalled them. Also, we met every month during the meetup, we brainstormed various ideas on what could be the potential topic or what our audience is looking for. With pandemic, that changed. Now only text or phone was not helping a lot.\nLast year I had a personal transformation with the book \u0026ldquo;Getting Things Done by David Allen\u0026rdquo; and Notion for my personal task management. I decided to give it a try with the help of Prakash to create a Notion page for our meetup management. It was a simple checklist of tasks to do on every meetup. We identified the tasks we do for every meetup that are recurring and added them to a template. So with every meetup, we create a new page from the existing template and fill in values.\nThis template has gone transformation multiple times, as is the nature of the changing world. Like earlier, when we were doing online meetups using StreamYard streaming directly to YouTube. If you wonder why StreamYard, well, Zoom was paid, Google Hangouts disabled the steam to YouTube feature and using Google Meet wasn\u0026rsquo;t an option because we needed the recording for folks to watch later on our YouTube channel. StreamYard was the only free option that allowed the stream to YouTube. But using StreamYard became monologue-ish because we only allowed speakers on StreamYard. The interaction between speakers and attendees was only via YouTube chat. That\u0026rsquo;s when we moved from StreamYard to Zoom (a sponsored account). After doing this switch, we changed the template.\nWe start out with the \u0026ldquo;New Meetup\u0026rdquo; button, which is just a template of tasks that we do on every meetup:\nOnce the page is created, then we start following the checklist. Some tasks are inherently assigned to one of the organisers. Like I am permanently assigned the task of creating the Zoom meeting invite. This is because Kinvolk (my employer) is sponsoring the Zoom for the meetup, and I have access to the Zoom account.\nA typical page for the meetup looks like the following. This page is from the planned March meetup:\nSome tasks will only be done before, during and after the meetup.\nThat\u0026rsquo;s about it. There is no rocket science behind this. You can use our Notion template to manage your meetup or create your own. To design your own, it will need some efforts and reflection. After moving the meetup management to the Notion checklist, the benefits we found were immense, like we did not miss anything in the plan. We were not panicking about finding specific information like we did before, and most importantly, it was all stress-free.\n",
    "ref": "/post/2021/03/k8s-blr-meetup/"
  },{
    "title": "On Compounder Skills",
    "date": "",
    "description": "The foundational skills which improves everything else.",
    "body": "There are specific skills which you should acquire early on in your life. These skills are the foundational skills. Everything you do after developing these skills becomes better, faster and easier. I call these skills Compounder Skills. Derived from the term \u0026ldquo;Compound Interest\u0026rdquo;. The idea is that once you are laced with a particular compounder skill, you can apply it in various fields of your life. An example of a compounder skill that most humans get exposed to is \u0026ldquo;school education\u0026rdquo;. The disadvantages of being unlettered are numerous viz. being dependent for information, gullible to most straightforward scams, the limited scope of jobs they can do, etc.\nThis blog tries to enlist some of the compounder skills I think everyone should have. This is certainly not an exhaustive list of all the skills a human should have. There is a limited time we have on this earth, and the possibilities of learning are endless. Sometimes it is better to get help from a professional, you can\u0026rsquo;t do everything by yourself. Also, this list is very much composed from my experience in life, and you might think that there are specific other skills that qualify as compounder skills, which is fine as well.\nü§î Infinite Curiosity We are super curious as infants, but it starts to recede as we grow old and get accustomed to things and start assuming things the way they are without considering why it is that way. Nurturing a genuine curiosity towards anything and everything, in general, is a great first thing to have. This curiosity is what will drive you to learn more and get into action. This will be your driving force for everything you do. I won\u0026rsquo;t classify \u0026ldquo;curiosity\u0026rdquo; as a skill but a learnable trait.\n The curious are also provided with much fun and wisdom long after formal education has ended. ‚Äî Charlie Munger\n üìñ Reading The first and foremost skill everyone needs to enhance is reading. After we complete our formal education, we might feel like it is the end of the learning for life. Someone with a fixed mindset like that generally sees a very slow upward trajectory in their job and life. Doing the same things repeatedly will not result in progress. What got you here might not be helpful to go upwards. There are so many things worth learning that are not taught in school education. Later in professional programs, the scope becomes even narrower. Every individual needs to study psychology, philosophy, history, business skills, finance, investment, etc.\nDon\u0026rsquo;t read something because someone recommended a book. Read things that interest you, so it does not feel like a chore but more of a fun thing to do. If you face some problem at work or in life in general, then read a book that promises to provide a solution to your problem.\nThe reading does not have to be restricted to books only, but blogs can also work. But the benefits that a book provides are immense, it is not condensed like a blog, and you understand concepts with contexts that blogs might not have. The bottom line is knowledge is power. Knowledge comes from reading. Power helps you propel upwards.\nRead what you love until you love to read.\n\u0026mdash; Naval (@naval) May 31, 2018  ‚úç Writing Once you get accustomed to reading, don\u0026rsquo;t just swallow it mindlessly. If it is fiction, it is okay. But if you are reading non-fiction, business, philosophy, or psychology, then take notes in your own words. Write down your digested thoughts. Highlights won\u0026rsquo;t cut it. Highlights are just out of context text for your future self. Please note down the feeling you had or the insight that came to you, and that made you highlight that text. Your notes will help your future self add meaning to the highlights and bring the feeling and insights back to life.\nOnce you form the habit of writing, especially writing it in public, you will start formulating your opinions and ideas. Since you will be writing about something to explain it to others, you will have a more precise understanding of the topic yourself. You will start forming mental models, and you will understand what fits where. You will create connections of non-relating topics, and out of these connections, you will come up with unique ideas that are original and your own.\nDo writing in public or not, it will help you clarify concepts. If you do it in public, you are making your thoughts open to others, thereby increasing your chances of serendipitous encounters. We, as knowledge-workers, have relied mainly on writing to convey our thoughts. With pandemic, as more and more people start working remotely, the importance of writing to share your ideas has never been so immense. Since you will present your thoughts very clearly and concisely, you will have an advantage over someone who cannot convey their thoughts in writing. You will be more productive.\n‚å® Typing \u0026ldquo;Typing\u0026rdquo;, yes, is a vital skill if you are a knowledge-worker. Like I mentioned before, the importance of writing has increased significantly since the pandemic for everyone and especially for anyone working remotely. But how do you write, of course not using your hand but using the keyboard! Hence along with writing, the typing also has to be faster and improved. If you are slow in typing, your keyboard will only come in your way of thinking. The tool will make you laboured; hence you might lose the train of thought. If you are wondering how to improve typing skills, read it here.\n‚úÖ Task Management Task management is something everyone should be well versed in. I think doing the right thing at the right time is very important. And for that, you need a system that guides you through your day to day, week to week, month to month and year to year. Without such a deliberate approach, you will just go with the flow or react to whatever life throws at you. Having a system of managing tasks makes you \u0026ldquo;proactive\u0026rdquo;. As per the second law of thermodynamics, things left on their own devices generally become messy and chaotic. You need to put extra work to keep them in equilibrium.\nA task management system does not matter how you implement it; it will make more room in your 24hrs for you to do work for which you could never find the time otherwise. Since the system will become your life operating system, everything will be kept track of. Hence you will have visibility of your long term goals, short term projects and immediate tasks. You will have a compass of your life and the things you wanna achieve.\nOnce you have such a system in place, you will be confident of finishing and seeing through any significant endeavour you undertake. It won\u0026rsquo;t guarantee 100% success, but it does ensure a 100% input. It will give you a clear picture of what went wrong or what things you did not track but should have followed.\nSince this system acts as a guard rail on what you are doing, it helps you focus and not get distracted by every new shiny thing that comes along and distracts you from your work.\nFocus requires saying no to great opportunities.\n\u0026mdash; Kelsey Hightower (@kelseyhightower) August 9, 2019  Reading the book \u0026ldquo;Getting things done by David Allen\u0026rdquo; is a great first thing to do in this direction.\nüß† Knowledge Management With the amount of knowledge you will be processing after reading and writing notes from that reading, you need a place to capture all that knowledge and wisdom. Generally, folks keep it in notebooks, but then once those notebooks are filled, they are shoved in some corner of the cupboard and never opened again. Such information is hard to find, and physical notebooks don\u0026rsquo;t feature \u0026ldquo;quick access\u0026rdquo; or \u0026ldquo;search\u0026rdquo;. You might keep such knowledge in google docs, but again even that can get scattered really quickly.\nCreate a system with accessibility in mind. Store the knowledge and wisdom in such a way that whenever your future self wants to read or find it, they can do it with as less hurdles as possible. Recollect an instance when you were thinking about this term or an excellent insight when reading something. You even wrote it down somewhere. Now you don\u0026rsquo;t know where and how to find it and what word to search for? So do your future self a favour by doing extra work now.\nThis will make sure you have a single place for your knowledge once you have that and you keep visiting this place time and again you keep that knowledge current, and there is a high chance that you form connections among the various pieces of knowledge.\nI have had quite a ride with my knowledge management experiences which are documented here. Now I follow the principles mentioned in the book \u0026ldquo;How to take smart notes by S√∂nke Ahrens\u0026rdquo; for taking notes.\nüí∞ Investment skills I think the whole talk of compounder skills is incomplete without talking about the compound effect on the money itself. Gain knowledge of investment early on in your life. The small investments you make early on, armed with compound interest, pay big time later in your life. Even if you start earning a lot later in your life, the compounded funds' value early on will give substantial results to you than what you will make. Learning how to invest even a small per cent of your earnings every month than squandering everything will help you in the time of crisis. You don\u0026rsquo;t need to do day trading, but knowing the tools and the jargons will definitely help you.\n The aforementioned list followed an order. Each skill created groundwork for the next one and so on. It is essential to have the skill to perfect the subsequent ones. Having one helps exponentially on the next ones.\nNow that you are well versed in reading, capturing what you are reading, presenting your thoughts, managing your tasks and investing money so that it is not rotting stagnant in one place. I think you have a solid foundation to build the Burj Khalifas of the world. The work does not end here. The task of identifying the compounder skills is continuous. Keep an eye; it is never too late to start learning them. But as the nature of compounding goes, it pays off if you hone these skills early on in your life. Compounder skills fall into the \u0026ldquo;Important but not urgent\u0026rdquo; quadrant of the Eisenhower matrix. I implore you to start working on them and improve your life as a result.\n",
    "ref": "/post/2021/03/compounder-skills/"
  },{
    "title": "Importance of Typing Skills",
    "date": "",
    "description": "The foundational skill which improves everything else.",
    "body": "Yes, today\u0026rsquo;s topic is typing skills. I think not many people stress about it, but it is a very underrated skill, yet useful in our daily lives.\nI typed the most organic way anybody starts doing it. Look at the keyboard when you are typing and fix mistakes after looking at the monitor. I moved my hand around on the keyboard and only used index fingers to touch the keys as if other fingers were glued together away from the keyboard. I learned the QWERTY keyboard\u0026rsquo;s keys placement while playing GTA Vice City with a friend since I typed cheat codes. Although I was still looking at the keyboard and typing, I did not search for the keys.\nFast forward from the school days of GTA Vice City to programming days during undergrad (engineering). Now I was more comfortable with my laptop keyboard and managed to type without looking at the keyboard. However, I was still moving my hand around and only using the index finger for alphabets and occasionally using thumb or pinky finger for special keys. Since I was only comfortable with my laptop\u0026rsquo;s keyboard, being fast enough to type on any other keyboard would feel cumbersome because either the keys were a different size or differently placed.\nIn 2015 I came across this term called touch-typing. It is the first thing that is taught in the DgpLUG. DgpLUG is an IRC based summer training conducted by Kushal Das and a bunch of other people. They teach basic things like touch-typing, git, Linux, communication, etc. I gave a desultory look at touch-typing and figured that I could not be bothered with learning that. Why spend extra effort to learn something as basic as \u0026ldquo;typing\u0026rdquo; anew when you get the job done?\nNow fast forward to my first job at Red Hat. I am a professional software engineer, and I still hadn\u0026rsquo;t gotten my typing any better. I was still using the index fingers, was only productive on one keyboard at a time, etc. While in one of the team meetings, I volunteered to take meeting notes, and it turns out I was terrible at it. That\u0026rsquo;s when I remembered touch-typing and decided to give it a go. I immediately googled for online resources and found out typingclub.com as a top result. I started following the tutorials, doing exercises every day for thirty minutes before beginning the office work (almost managed to annoy a co-worker with the keyboard noise). It was a grievous period since I was unlearning something as vital as typing the old way, and all the muscle memory was built around old habits and learning a new method of doing it. It can be correlated to unlearning walking forward and now learning to walk only sideways or backwards. But I relented and started doing touch-typing slowly. Over time as the new muscle memory was built, I surpassed the old typing speed. The kind of boost I felt after touch-typing is the same as when a limping person discovers running. Over the last three to four years, I have acquired good enough speed to take notes from the meetings.\nI think touch-typing is an essential skill that is not taught when kids take up computers. And then it is a painstaking job to unlearn old behaviours and learn new ones. But something to understand here is that as a knowledge worker, the computer keyboard is the only (unless you don\u0026rsquo;t mind pen and paper) way we will churn out information. Although speech to text converters are catching up, they do a cursory job and generally get in your way, thus breaking your train of thoughts. A keyboard is a tool, and if you are not productive at the devices you use, they will only get in your way of thinking and executing the actual thing. Typing is not the \u0026ldquo;actual task\u0026rdquo; but simply a means to an end. The content (code, essay, blog, tweet) you create by typing is the measure of how good you are. Nobody will see how fast or slow you have typed. Everyone else is concerned with the result only. But then, if you are typing, it should be second nature should happen involuntarily without you having to think about typing but only about the thing you are typing.\nFinally, improve this fundamental foundational skill of typing, and everything else you do with it will be better.\n",
    "ref": "/post/2021/02/typing-skills/"
  },{
    "title": "My Knowledge Management Journey",
    "date": "",
    "description": "The story of various tools I have used over last five years.",
    "body": "Photo by Patrick Tomasso on Unsplash.\n If you are reading this, you are definitely a Knowledge Worker. As Knowledge Workers, we rely a lot on the information we know or have access to for our day to day work. Occasionally, we will do the same thing twice, face the situation more than once, want to read that reference or try to understand the insights mentioned in that one particular blog. How do you keep track of such information? How do you find such information again after you have researched it once?! You need a knowledge management system that aids you in revisiting such information.\nThis blog is about my \u0026ldquo;Knowledge Management\u0026rdquo; journey. The various approaches I have applied over the years, how one became cumbersome over others, and the benefit of subsequent methods over the previous one. If you wanna skip the boring history and get to the solution, then you can just jump to that last two sections. But I think you would appreciate the importance of the last approach only if you know why it is better than others.\nGoogling is not the Best Strategy! If you find yourself googling, again and again, it is a clear sign you want that information quicker in terms of accessibility. Now it depends on the nature of the data. It could command you want to run again, or it could be some term in a specific blog you want to look up for your writing, etc.\nTweeting it out In my early days on Twitter, I was only a leecher. Once I struggled with the information storage problem, Twitter turned out to be the best platform to change my leecher hat for a seeder. I decided to tweet any new information I learnt with a tweet. The tweet format used to be something like Problem Statement: \u0026lt;solution link\u0026gt;.\nhttperf starter: https://t.co/Lo6Qv2hE6p\n$ ùöëùöùùöùùöôùöéùöõùöè --ùöëùöòùöê --ùöúùöéùöõùöüùöéùöõ \u0026lt;ùöéùöóùöçùöôùöòùöíùöóùöù\u0026gt; --ùöûùöõùöí \u0026quot;/\u0026quot; --ùöóùöûùöñ-ùöåùöòùöóùöó ùü∑ùü∏ùü∂ùü∂ùü∂ùü∂ùü∂ --ùöóùöûùöñ-ùöåùöäùöïùöï ùü∏ --ùöùùöíùöñùöéùöòùöûùöù ùüª --ùöõùöäùöùùöé ùü∫ùü∂ùü∂ùü∂ --ùöôùöòùöõùöù ùüæùü∂\n\u0026mdash; Suraj Deshmukh | ‡§∏‡•Å‡§∞‡§ú ‡§¶‡•á‡§∂‡§Æ‡•Å‡§ñ (@surajd_) November 27, 2020  Or if it is a command-line thing, then the structure would be Problem Statement: \u0026lt;solution commands in monospace\u0026gt; Source link: \u0026lt;link\u0026gt;.\nAn easier way to find if a service is active?\n```\n$ systemctl is-active gdm\nactive\n$ systemctl is-active docker\ninactive\n$ systemctl is-active foobar\ninactive\n```\nHelpful in scripting!\nCaution: But one problem is that it does not error when a service does not exists!\n\u0026mdash; Suraj Deshmukh | ‡§∏‡•Å‡§∞‡§ú ‡§¶‡•á‡§∂‡§Æ‡•Å‡§ñ (@surajd_) August 7, 2019  If I needed anything from my past tweets, I could simply go to my \u0026ldquo;Profile\u0026rdquo; and search for the problem statement. Once the tweets grew on my \u0026ldquo;Profile\u0026rdquo;, searching went out of hand. Also, the search feature of Twitter is cumbersome to use.\nThe Problem of searching tweets To overcome the search-on-your-Profile problem, I wondered what if I can use Twitter API to download all the tweets from my account and store them in a plain text format to grep it later? To materialise that idea, I wrote this script (tweetbase) that would download all the tweets and dump them into a file. The only downside was that I had to run it periodically so that the local file is up to date with the Twitter profile. This has served its purpose to date.\n$ ./run \u0026gt; plain-text-tweets.txt 2021/02/22 10:45:01 All clients ready 2021/02/22 10:45:02 Credentials verified 2021/02/22 10:45:03 Got max id: 1363704708418871301 2021/02/22 10:45:03 Tweets download started 2021/02/22 10:45:13 Tweets download complete 2021/02/22 10:45:13 Tweets dumped to ./alltweets.json $ grep -i hostpath -A 1 plain-text-tweets.txt 5683:Correction: Whitelisting `allowedHostPath` doesn\u0026#39;t help. Only way to get around it is to control permissions on PV‚Ä¶ https://t.co/CETzBGf2MQ 5684-URL: https://twitter.com/surajd_/status/1160967381100814342 Now I could just open the above link pointing to my tweet.\nSometimes I couldn\u0026rsquo;t remember what \u0026ldquo;Problem Statement\u0026rdquo; I had written with the tweet; then, I would be just guessing various terms to search; hence a lot of time would be wasted. Also, Twitter API trims the tweet content, so even if you had typed detailed information, it might not be searchable in the plain text file. You can see that the text is redacted with an ellipsis (...).\nWrite a script to automate There are specific commands that you need again-and-again. I would automate that by putting them into a single script.\nFor, e.g. certain websites disable pasting text into text boxes, and I found a solution to enable pasting. When I discovered it first, I tweeted about it.\nFor the websites that disable paste, override it on #chrome using the following #javascript code. Copy and paste it in the chrome console.\nSource: https://t.co/NGUNIz26di\nFor #firefox read the steps üëÜ pic.twitter.com/oDdMGG7EHT\n\u0026mdash; Suraj Deshmukh | ‡§∏‡•Å‡§∞‡§ú ‡§¶‡•á‡§∂‡§Æ‡•Å‡§ñ (@surajd_) December 17, 2020  I found myself visiting such websites multiple times. Thus, to copy the solution, I would see the Twitter profile page (until the page was searchable using Ctrl + F). Once the tweet on the profile got buried, I started digging (grepping) into the plain-text-tweets file and then jumping to the tweet.\n$ grep -i paste -A 1 plain-text-tweets.txt 470:Copy paste üëá 471- -- 773:var allowPaste = function(e){ 774- e.stopImmediatePropagation(); -- 777:document.addEventListener(\u0026#39;paste\u0026#39;, allowPaste, true); 778-URL: https://twitter.com/surajd_/status/1339602792143458304 When this happened more than a couple of times, I decided to script this thing and make this affair faster. The script can be found here. Now I just run the script, and the contents are copied to the clipboard to be executed into the chrome tab.\n$ make-chrome-copy-pastable.sh The code has been copied into the clip board. Now goto chrome and press Ctrl + Shift + I and paste into the console window. I have explained in this blog my scripts framework. I have used similar approaches to update the tools I need at their cutting edge. This way, I don\u0026rsquo;t have to search for how to update them every time. I just run the script.\nYour own blog should be easy to find I started writing blogs where tweets did not serve the purpose. My blogs' nature was problem-oriented (the problems that I encountered and the solution that worked for me in my environment). With blogs, I took an NIH approach (even if there were blogs already written about that topic, I thought I might write it in my words). It served two purposes; they made it easier to search for information for which I had a readymade solution, and the process of writing made my understanding clear. When you write something to explain to others, you must understand all the nitty-gritty details to explain the topic to others in easy to understand language. Since the blogs were created for public consumption, I ensured that I did not make assumptions or tried to make it generic and think from various perspectives.\nWith that being said, there\u0026rsquo;s still a limit on how much one person can process and distil into writing. My reading volume was many folds greater than the content I was writing. To store information reliably, I needed something that can capture the entire web page and have a search facility, so if I just entered a term, it would fetch all the pages with that term. I gave serious thought to creating a scraper using Python\u0026rsquo;s Scrapy framework but then never got time to do it by myself.\nSave to read later For the blogs or articles I read, I would save them into Pocket and still do. It is a great tool to keep anything with a web link instead of using just bookmarks. Pocket downloads the webpages by rendering them into a screen-adjustable format. Pocket solved what I was looking for, something that would download the web page\u0026rsquo;s contents to search later. Pocket\u0026rsquo;s search functionality was not so good, and sometimes it rendered pages poorly or only halfway through.\nPocket became my dumping ground for anything, and everything I would wanna read later. It was an endless¬†list¬†stack of blogs, articles, videos and podcasts. The common problem of all the hoarding methodologies (caused by collector\u0026rsquo;s fallacy) that I was employing (Twitter, tweetbase and Pocket) was that I was only hoarding information. Never gave a thought to how do I want my future self to find that information. Apart from writing blogs, the other methods didn\u0026rsquo;t make it easier to find information.\nMoving on from hoarding to ease-of-finding Enter the Notion. The solution to every problem is not a Notion page, as shown in this image, but a Notion database, yes.\nThe Notion database feature is very flexible. Each row is a page, unlike spreadsheets. The page\u0026rsquo;s recursive nature makes Notion databases powerful, within page or database within a page, plus the ability to refer one column of one database into another database.\nMy Notion system is built similar to what August Bradley has recommended (Media and Notes). I have two Notion databases; one stores \u0026ldquo;Media\u0026rdquo; (blogs, videos, podcasts, books), and the other one is called \u0026ldquo;Notes\u0026rdquo;, and both of these databases are linked.\nThe Media database has columns that allow me to store metadata like the type of content, category of content, how likely you think it will be relevant, etc. These columns can later be used to filter and create dashboards. Like if I want to view just the media of type articles and in the technology field, I can make a dashboard named \u0026ldquo;Read - Technology\u0026rdquo; and have a filtered view of a database. To import content into the Media database, I use the Notion web clipper. It captures all the content in that webpage and saves it as a Notion page in a database. Once that webpage is copied in Notion, now you can decide what you want to highlight or bold or italic, etc.\nWhile reading, if I have insights, I write those insights and original thoughts into the Notes database. Since these insights were drawn while reading an article, I refer to the Media database field in the Notes database to link them.\nThis approach is doing upfront work so that you help your future self. Pocket does not provide so much flexibility as Notion does, and there is no way to store your thoughts in Pocket, Pocket limits on how much you can achieve with the stored content, etc.\nFinally I am using Notion to store something I want to find later. There I can take notes and link the notes to the media source it came from. The Notion has a quick general search function that searches through all the pages. Your brain is the ultimate search engine. Writing notes in your own words helps you form a mental model by connecting various neural pathways. But for anything else, Notion web clipper with Notion databases is a saviour.\n",
    "ref": "/post/2021/02/kms-journey/"
  },{
    "title": "Book: How Innovation Works",
    "date": "",
    "description": "And Why It Flourishes in Freedom by Matt Ridley",
    "body": "Introduction I recently finished reading the book: \u0026ldquo;How Innovation Works: And Why It Flourishes in Freedom\u0026rdquo; by Matt Ridley. The book was published less than a year ago in May 2020, and it is a short read of fewer than four hundred pages. I am not sure how to categorise this book, it probably falls into business, science and/or technological history.\nWhile listening to Naval Ravikant\u0026rsquo;s podcast, I found this book when Matt Ridley, the author, was a guest in one episode. I was profoundly influenced by the introduction of the book I got in the podcast.\nI would suggest just read the introduction (The Infinite Improbability Drive) for a starter. You will be inspired to read the book. The book is an easy read for anyone and everyone. Specifically, if you want to be a part of innovation, identify innovation, convert ideas to reality, and become an innovator, this book is for you.\nAbout the book The author has promised the readers in the \u0026ldquo;Introduction\u0026rdquo; particular insights and conveyed them in the book\u0026rsquo;s subsequent chapters. The author\u0026rsquo;s primary purpose is to tell the stories of innovation across various periods of human history, learn from them, and build a perspective about innovation happening right now in your surroundings.\nThe author starts out with innovation stories across various human aspects like energy, health, transportation, food, everyday home technologies, computing and communication, and pre-historic humans' innovation. In the later chapters the author goes into the insightful introspection of innovation viz. graduality of innovation, where it thrives, the role of serendipity in innovation, innovation due to remixing of ideas, the importance of trial and error, innovation as a group activity, inevitability of innovation when the conditions are right. Furthermore, the nature of innovation, and how it is mostly a precursor to science than the implementation of science, the myth of job losses due to innovation. Finally, the author discusses how specific agents try to hinder the innovation viz. governments, law, incumbent organisations, investors, NGOs trying to stand for the public, etc.\n Innovations come in many forms, but one thing they all have in common, and which they share with biological innovations created by evolution, is that they are enhanced forms of improbability.\n Effect on me I could relate to the book remembering how inventors' stories were told to me when I was a child. The invention was generally attributed to an individual without telling the whole story of how the situation back then was perfect for innovation or how that person had got this idea from somewhere else, and how he managed to get the patent and had his name engraved in history. Such deep-rooted misbeliefs of lone inventors with eureka moments were uprooted by this book.\n innovation is nearly always a gradual, not a sudden thing. Eureka moments are rare, possibly non-existent, and where they are celebrated it is with the help of big dollops of hindsight and long stretches of preparation, not to mention multiple wrong turns along the way. Archimedes almost certainly did not leap out of his bath, shouting ‚ÄòHeureka‚Äô; he probably invented the story afterwards to entertain people.\n My most favourite part of the book is the book\u0026rsquo;s stories. And of course, the insights become much easier to understand because they are drawn from the stories. These stories could be a good conversation starter, in any social setting.\nThe book has made me an innovation optimist. I had become pessimist by the news that hyped up about the failed innovations or news of the innovation\u0026rsquo;s harmful outcomes. And my views were mostly sceptical of anything new that came out.\nWhen you are working on something, it is easy to get bored with everyday stuff, and you may feel the insignificance of day-to-day work. But this book kind of imbibed in me the patience that up-close things will look slower. It is only when you zoom out over timeline of months or years you can see the progress you have made, the innovation that has happened and you were a part of that innovation journey.\nOpen Source a beacon of hope for innovation The book propounds the importance of open sharing of ideas and information as a way to the progress of humankind. He has shown how the patent and IP system built to protect the individual inventor has come to stifle the human progress, and open sharing of knowledge is the way to go forward. I could especially relate to the book because I worked on open-source software and worked at Red Hat in the past. Both things are mentioned in the book. The amount of innovation that goes on in the openly shared information is tremendous and hard to match anywhere else.\nSuppose a single entity wholly contributes to creating something new without much help from outside. In that case, I think they are entitled to have a patent and reap the benefits that come out of that creation. But most of them who just assemble stuff from others and patent that gathered stuff without much investment should share their information freely. The sheer first-mover advantage will primarily benefit them if the innovation is correctly applied to harness scale economies.\n Inventing something gives you a first-mover advantage, which is usually quite enough to get you a substantial reward. Cunning inventors can throw their imitators off the trail with misleading details: Bosch was careful to let Haber reveal only the second-best catalyst recipe for fixing nitrogen.\n Innovation makes things accessible There is far too much importance given to the person who came up with the idea. Still, a lot of work goes behind bringing that idea into a product and to the masses. The real difference is created by the person who brings the cost of the innovation down so that the ordinary person can afford it. For example, Elon Musk is trying to get the cost of space travel down his team is doing a lot of innovation at SpaceX. He has identified that if humans want to colonise Mars, then the cost per take-off should be brought down substantially. This cost reduction goal is partially achieved if compared to the pre-SpaceX days.\nThe following quotes from the book corroborate what I am trying to say here:\n Again and again in the history of innovation, it is the people who find ways to drive down the costs and simplify the product who make the biggest difference. The unexpected success of mobile telephony in the 1990s, which few saw coming, was caused not by any particular breakthrough in physics or technology, but by its sudden fall in price.\n in the early years of computers, mobile phones and many other innovations, the inventors thought they were developing a luxury good for the upper-middle classes. It took a farmer‚Äôs son from Detroit to turn the car from a luxury invention into an everyman innovation: an affordable utility for ordinary people.\n Inevitability of Innovation The book teaches you to be an opportunist. If there is an obvious opportunity to do innovation, grab that opportunity and do it. If it is not you, then someone else will certainly do it. This certainty is possible because the conditions for such innovation are created.\nYou may have experienced this that you get an idea one day and think to yourself that, \u0026ldquo;Wow, that is such a novel idea!\u0026rdquo;. Still, later it turns out that someone somewhere has already thought about it. In some form, that idea is ready to be materialised, or it\u0026rsquo;s put out there textually. In such a situation, even if somebody is bringing the concept to life, there is no harm in giving it a try to materialise the vision yourself. With your skills and edge, maybe you might do a better job of doing justice to the idea!\n The lesson of wheeled baggage is that you often cannot innovate before the world is ready. And that when the world is ready, the idea will be already out there, waiting to be employed: in America, at least.\n In hindsight, you might feel that you could have done a better job had you bet on your idea and try to convert that into reality.\n Yet here is a paradox. There is an inevitability about both search engines and social media. If Larry Page had never met Sergei Brin, if Mark Zuckerberg had not got into Harvard, then we would still have search engines and social media. Both already existed when they started Google and Facebook. Yet before search engines or social media existed, I don‚Äôt think anybody forecast that they would exist, let alone grow so vast, certainly not in any detail. Something can be inevitable in retrospect, and entirely mysterious in prospect. This asymmetry of innovation is surprising.\n Parallels with thermodynamics The author has repeatedly tried to find a correlation between the entropy in thermodynamics and innovation. Innovation is a business of putting the atoms in a highly improbable structure which is highly efficient and leads to positive outcomes.\n Innovation, like evolution, is a process of constantly discovering ways of rearranging the world into forms that are unlikely to arise by chance ‚Äì and that happen to be useful. The resulting entities are the opposite of entropy: they are more ordered, less random, than their ingredients were before. And innovation is potentially infinite because even if it runs out of new things to do, it can always find ways to do the same things more quickly or for less energy.\n In the final chapter, the author concludes as follows:\n The peculiar fact that one species above all others has somehow got into the habit of rearranging the atoms and electrons of the world in such a way as to create new and thermodynamically improbable structures and ideas that are of practical use to the wellbeing of that species never ceases to amaze me. The future is thrilling and it is the improbability drive of innovation that will take us there.\n Concluding thoughts Apart from one part where I don\u0026rsquo;t share the author\u0026rsquo;s scepticism of the project hyperloop, rest of the book is an excellent collection of stories and insights.\nAs I said, the book had a profound perspective shift for me. And I think the book would do the same to you. I would highly encourage you to read the book and share how you could relate with the book and which parts of the book you highly appreciate.\n The main ingredient in the secret sauce that leads to innovation is freedom. Freedom to exchange, experiment, imagine, invest and fail; freedom from expropriation or restriction by chiefs, priests and thieves; freedom on the part of consumers to reward the innovations they like and reject the ones they do not.\n References and Links  Book Link on Goodreads. Link to the book highlights. DISCLAIMER: All the quotes in this blog are copyright of the author.  ",
    "ref": "/post/2021/02/book-hiw-learnings/"
  },{
    "title": "Enable TLS bootstrapping in a Kubernetes cluster",
    "date": "",
    "description": "Add a new node using a bootstrap token to Kubernetes",
    "body": "This blog is a recap of my old blog \u0026ldquo;Add new node to Kubernetes cluster with bootstrap token\u0026rdquo;. Like the aforementioned blog, we will look at how to enable TLS bootstrapping on an existing Kubernetes cluster at control plane level and add a new node (or modify existing ones) to the cluster using bootstrap tokens. At the end of this blog, you will learn what specific steps to take to enable TLS bootstrapping on any custom-built Kubernetes cluster.\nTextual 1. Kubernetes Cluster If you have a cluster running, you can skip this step. To demonstrate how TLS bootstrap tokens work, I have brought up a cluster using Kubernetes the Hard Way (Vagrant). This section shows how to bring up a multi-node, multi-worker Kubernetes cluster quickly.\n1.1. Setup Machine I am running this setup on a Ubuntu 20.04 machine. Install virtual-box and vagrant by executing the following commands as the root user:\napt-get update \u0026amp;\u0026amp; \\ apt-get -y upgrade \u0026amp;\u0026amp; \\ apt-get install -y make git byobu linux-generic systemctl reboot apt-get install -y linux-headers-`uname -r` \u0026amp;\u0026amp; \\ apt install -y virtualbox \u0026amp;\u0026amp; \\ apt-get install -y vagrant \u0026amp;\u0026amp; \\ dpkg-reconfigure virtualbox-dkms 1.2. Clone the Repository mkdir -p git/kubernetes-the-hard-way-vagrant git clone https://github.com/kinvolk/kubernetes-the-hard-way-vagrant git/kubernetes-the-hard-way-vagrant cd git/kubernetes-the-hard-way-vagrant 1.3. Install the Cluster ./scripts/install-tools ./scripts/setup 1.4. Verify the Cluster Installation kubectl get nodes 2. Prepare Control Plane 2.1. Setup RBAC When a new node tries to authenticate to the control plane, the only credential used is a bootstrap token. Such a node client (kubelet) is a member of group system:bootstrappers, we need to ensure that such a client has permissions to create Certificate Signing Requests (CSRs), get them approved, renew them, etc. The following ClusterRoleBindings will ensure that these nodes join without authorisation issues. This setting needs to be done only once and does not change from node-to-node.\nRun the following command to create the bindings that will allow the new nodes to connect to the cluster:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - # Enable bootstrapping nodes to create CSR. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: create-bootstrapping-csr subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:node-bootstrapper apiGroup: rbac.authorization.k8s.io --- # Approve all CSRs for the group \u0026#34;system:bootstrappers\u0026#34;. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: approve-csrs subjects: - kind: Group name: system:bootstrappers apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:nodeclient apiGroup: rbac.authorization.k8s.io --- # Approve renewal CSRs for the group \u0026#34;system:nodes\u0026#34;. apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata: name: approve-node-renewals subjects: - kind: Group name: system:nodes apiGroup: rbac.authorization.k8s.io roleRef: kind: ClusterRole name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient apiGroup: rbac.authorization.k8s.io EOF 2.2. Modify Kubernetes Apiserver Add the following set of flags to the kube-apiserver:\n--authorization-mode=Node,RBAC --enable-bootstrap-token-auth=true Above setting needs to be performed on each control plane node.\n2.3. Modify Kubernetes Controller Manager Add the following flag to the kube-controller-manager:\n--controllers=*,tokencleaner,bootstrapsigner Above setting needs to be performed on each control plane node.\n3. Modify / Add Workers 3.1. Create the Bootstrap Token Please note these tokens, we will need them in two places. One, while creating a bootstrap-token embedded Kubernetes Secret object and another in the bootstrap kubeconfig for the Kubelet:\nTOKEN_ID=$(openssl rand -hex 3) TOKEN_SECRET=$(openssl rand -hex 8) 3.2. Create Bootstrap Token Secret Kubernetes apiserver will use this secret to identify the node that is joining the cluster. Execute the following command to create the secret embedding the previously generated tokens:\ncat \u0026lt;\u0026lt;EOF | kubectl apply -f - apiVersion: v1 kind: Secret type: bootstrap.kubernetes.io/token metadata: name: bootstrap-token-$TOKEN_ID namespace: kube-system stringData: description: \u0026#34;Manually generated bootstrap token.\u0026#34; token-id: \u0026#34;$TOKEN_ID\u0026#34; token-secret: \u0026#34;$TOKEN_SECRET\u0026#34; usage-bootstrap-authentication: \u0026#34;true\u0026#34; usage-bootstrap-signing: \u0026#34;true\u0026#34; EOF 3.3. Create Bootstrap Kubeconfig for Kubelets Create bootstrap Kubeconfig for Kubelet(s):\nSERVER=$(kubectl config view -ojsonpath=\u0026#39;{.clusters[0].cluster.server}\u0026#39;) CA_CERT=$(kubectl config view --flatten -ojsonpath=\u0026#39;{.clusters[0].cluster.certificate-authority-data}\u0026#39;) cat \u0026lt;\u0026lt;EOF | tee kubeconfig apiVersion: v1 kind: Config clusters: - name: local cluster: server: \u0026#34;$SERVER\u0026#34; certificate-authority-data: \u0026#34;$CA_CERT\u0026#34; users: - name: kubelet user: token: \u0026#34;$TOKEN_ID.$TOKEN_SECRET\u0026#34; contexts: - context: cluster: local user: kubelet EOF Keep this file around it will be needed to connect worker(s) node using bootstrap token to control plane.\n3.4. Modify Kubelet Copy the bootstrap kubeconfig created in step 3.3 to the worker node you want to connect to control plane using bootstrap tokens. Save the kubeconfig on the node and provide the path to that file to flag --bootstrap-kubeconfig. The path supplied to --kubeconfig will have newly downloaded kubeconfig once the kubelet authenticates with the kube-apiserver.\nAdd the following set of flags to the kubelet:\n--kubeconfig=/var/lib/kubelet/kubeconfig --bootstrap-kubeconfig=/etc/kubernetes/kubeconfig --rotate-certificates NOTE: You can remove the file in /var/lib/kubelet/kubeconfig, especially if you modify an existing worker node, to ensure that Kubelet downloads the new kubeconfig at that location.\nYou can have a separate token for each worker node to reduce the attack surface. To generate distinct token repeat steps 3.1 to 3.4 for each worker node. But if you wish to use the same token for each worker node, repeat step 3.4 on each worker.\n4. Verify Ensure that the node has joined the control plane using TLS bootstrap token by executing the following command:\nkubectl get nodes Video Watch this video to understand the same process explained in this blog. Unlike my previous blog videos, I am narrating, so you know what is going on.\n  References  Authenticating with Bootstrap Tokens.  ",
    "ref": "/post/2021/02/k8s-bootstrap-token/"
  },{
    "title": "Kubernetes Cluster using Kubeadm on Flatcar Container Linux",
    "date": "",
    "description": "Simple steps to install the cluster based on Flatcar Container Linux",
    "body": "This blog shows a simple set of commands to install a Kubernetes cluster on Flatcar Container Linux based machines using Kubeadm.\nYou might wonder why this blog when one can go to the official documentation and follow the steps? Yep, you are right. You can choose to do that. But this blog has a collection of actions specific to Flatcar Container Linux. These steps have been tried and tested on Flatcar, so you don\u0026rsquo;t need to recreate and test them yourself. There are some nuances related to the read-only partitions of Flatcar, and this blog takes care of them at the control plane level and the CNI level both.\nTextual Steps All the commands are run by becoming root using sudo -i.\n1. Setup Nodes All the following sub-steps should be run all the nodes.\n1.1. Setup Networking systemctl enable docker modprobe br_netfilter cat \u0026lt;\u0026lt;EOF | tee /etc/modules-load.d/k8s.conf br_netfilter EOF cat \u0026lt;\u0026lt;EOF | tee /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 EOF sysctl --system 1.2. Download binaries and start Kubelet CNI_VERSION=\u0026#34;v0.8.2\u0026#34; CRICTL_VERSION=\u0026#34;v1.17.0\u0026#34; RELEASE_VERSION=\u0026#34;v0.4.0\u0026#34; DOWNLOAD_DIR=/opt/bin RELEASE=\u0026#34;$(curl -sSL https://dl.k8s.io/release/stable.txt)\u0026#34; mkdir -p /opt/cni/bin mkdir -p /etc/systemd/system/kubelet.service.d alias curl=\u0026#39;curl -sSL\u0026#39; curl \u0026#34;https://github.com/containernetworking/plugins/releases/download/${CNI_VERSION}/cni-plugins-linux-amd64-${CNI_VERSION}.tgz\u0026#34; | tar -C /opt/cni/bin -xz curl \u0026#34;https://github.com/kubernetes-sigs/cri-tools/releases/download/${CRICTL_VERSION}/crictl-${CRICTL_VERSION}-linux-amd64.tar.gz\u0026#34; | tar -C $DOWNLOAD_DIR -xz curl \u0026#34;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubelet/lib/systemd/system/kubelet.service\u0026#34; | sed \u0026#34;s:/usr/bin:${DOWNLOAD_DIR}:g\u0026#34; | tee /etc/systemd/system/kubelet.service curl \u0026#34;https://raw.githubusercontent.com/kubernetes/release/${RELEASE_VERSION}/cmd/kubepkg/templates/latest/deb/kubeadm/10-kubeadm.conf\u0026#34; | sed \u0026#34;s:/usr/bin:${DOWNLOAD_DIR}:g\u0026#34; | tee /etc/systemd/system/kubelet.service.d/10-kubeadm.conf curl --remote-name-all https://storage.googleapis.com/kubernetes-release/release/${RELEASE}/bin/linux/amd64/{kubeadm,kubelet,kubectl} chmod +x {kubeadm,kubelet,kubectl} mv {kubeadm,kubelet,kubectl} $DOWNLOAD_DIR/ systemctl enable --now kubelet systemctl status kubelet 1.3. Update Flatcar Verify if you need to update Flatcar by running the following two commands:\ncurl -sSL https://stable.release.flatcar-linux.net/amd64-usr/current/version.txt | grep FLATCAR_VERSION cat /etc/os-release | grep VERSION If the output of the above two commands match, your Flatcar is already up to date, you don\u0026rsquo;t need to run the following commands.\nupdate_engine_client -update systemctl reboot 2. Setup Controller Node Run these steps only on the controller node.\n2.1. Initialise Kubeadm Here we are ensuring that the Kubelet volume-plugins directory is writable, the default path used is under /usr which is read-only on Flatcar, therefore we are setting it to writable path /opt/libexec/kubernetes/kubelet-plugins/volume/exec/.\nFrom the kubeadm docs:\n On Linux distributions such as Flatcar Container Linux, the directory /usr is mounted as a read-only filesystem. For flex-volume support, Kubernetes components like the kubelet and kube-controller-manager use the default path of /usr/libexec/kubernetes/kubelet-plugins/volume/exec/, yet the flex-volume directory must be writeable for the feature to work.\n cat \u0026lt;\u0026lt;EOF | tee kubeadm-config.yaml apiVersion: kubeadm.k8s.io/v1beta2 kind: InitConfiguration nodeRegistration: kubeletExtraArgs: volume-plugin-dir: \u0026#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\u0026#34; --- apiVersion: kubeadm.k8s.io/v1beta2 kind: ClusterConfiguration networking: podSubnet: 192.168.0.0/16 controllerManager: extraArgs: flex-volume-plugin-dir: \u0026#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\u0026#34; EOF kubeadm config images pull kubeadm init --config kubeadm-config.yaml mkdir -p $HOME/.kube cp -i /etc/kubernetes/admin.conf $HOME/.kube/config 2.2. Install CNI Here I have chosen Calico as the CNI to install because it is the one that I am familiar with, but you can choose to install any other CNI.\ncat \u0026lt;\u0026lt;EOF | tee calico.yaml # Source: https://docs.projectcalico.org/manifests/custom-resources.yaml apiVersion: operator.tigera.io/v1 kind: Installation metadata: name: default spec: # Configures Calico networking. calicoNetwork: # Note: The ipPools section cannot be modified post-install. ipPools: - blockSize: 26 cidr: 192.168.0.0/16 encapsulation: VXLANCrossSubnet natOutgoing: Enabled nodeSelector: all() flexVolumePath: /opt/libexec/kubernetes/kubelet-plugins/volume/exec/ EOF kubectl create -f https://docs.projectcalico.org/manifests/tigera-operator.yaml kubectl apply -f calico.yaml kubectl taint nodes --all node-role.kubernetes.io/master- kubectl get pods -A kubectl get nodes -o wide 2.3. Generate worker config The general way to connect a worker to the control plane is by running the kubeadm join command generated at the end of the kubeadm init output. But you can run the following set of steps at any time to create a worker config.\nURL=$(kubectl config view -ojsonpath=\u0026#39;{.clusters[0].cluster.server}\u0026#39;) prefix=\u0026#34;https://\u0026#34; short_url=${URL#\u0026#34;$prefix\u0026#34;} cat \u0026lt;\u0026lt;EOF apiVersion: kubeadm.k8s.io/v1beta2 kind: JoinConfiguration discovery: bootstrapToken: apiServerEndpoint: $short_url token: $(kubeadm token create) caCertHashes: - sha256:$(openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2\u0026gt;/dev/null | openssl dgst -sha256 -hex | sed \u0026#39;s/^.* //\u0026#39;) controlPlane: nodeRegistration: kubeletExtraArgs: volume-plugin-dir: \u0026#34;/opt/libexec/kubernetes/kubelet-plugins/volume/exec/\u0026#34; EOF Copy the generated config to worker nodes and save it in a file named worker-config.yaml.\n3. Join Workers to Controllers Run the following steps only on worker nodes.\n3.1. Connect to Control-Plane kubeadm join --config worker-config.yaml 4. Verify Go to the controller node and run the command kubectl get nodes to verify if the worker has joined the cluster.\nVideo   References  Installing kubeadm. Creating a cluster with kubeadm. Troubleshooting kubeadm. Kubeadm configuration reference. Manually generating discovery-token-ca-cert-hash. Quickstart for Calico on Kubernetes.  ",
    "ref": "/post/2021/01/kubeadm-flatcar/"
  },{
    "title": "Exec in container environment",
    "date": "",
    "description": "The correct way to use exec and the signal passing.",
    "body": "If you use exec in your container script, then the container or Kubernetes pod might exit after the command that is exec-ed into has exited. But if that\u0026rsquo;s what you wanted, then it\u0026rsquo;s okay. This blog tries to explain how to pass the signals to the applications, how they work differently when invoked uniquely and what to do if the application does handle them.\nWhat are the \u0026ldquo;Signals\u0026rdquo;? Signals are messages one process can send to another process, mostly used in UNIX like operating systems.\nHow exec works in Linux? This is copied from the man page:\nAn application spawned by a shell script. To ensure that the signals are passed effectively to the real application, spawned by a shell script, you can do something similar to what is done in this snippet. Use trap to call a function which does the cleanup after receiving any of the registered signals like SIGHUP, SIGTERM or SIGINT.\n#!/bin/bash  function cleanup() { kill \u0026#34;${pid}\u0026#34; exit } trap cleanup SIGHUP SIGTERM SIGINT echo \u0026#34;sleeping\u0026#34; sleep infinity \u0026amp; pid=\u0026#34;${!}\u0026#34; wait \u0026#34;${!}\u0026#34; If a process runs in foreground spawned by a bash script, then the bash script does not respond to the signals, so the trap is rendered useless. Hence run it in the background and wait on it using wait. Make sure that the script is run via ENTRYPOINT the way it is done in this Dockerfile:\nFROMquay.io/surajd/fedora-networkingCOPY ./cleanup.sh /cleanup.shENTRYPOINT /cleanup.shSince the above script, cleanup.sh is listening on three types of signals: SIGHUP, SIGTERM, SIGINT if any of these is sent to the script the container will stop working. So the following commands will work just fine:\nHere is a video that shows how the above commands works:\n  An application spawned directly. Here is a golang application that handles signals. This golang app can be run directly via ENTRYPOINT.\nFROMquay.io/surajd/fedora-networkingCOPY ./signals /signalsENTRYPOINT /signalsThis golang code is also listening on the same signals: SIGHUP, SIGTERM, SIGINT. The code has a main goroutine which spawns another goroutine. The background goroutine is listening to the aforementioned signals. While the main goroutine is waiting (‚Üêdone) for the background goroutine to receive signal and finally exit. In the Dockerfile, we have simply copied the binary directly and spawn it using ENTRYPOINT, this ensures that the application receives the signals directly. ENTRYPOINT is actually starting the given process using exec.\nWatch the video below, which shows how the signals are passed to the application:\n  When should you use exec? Now consider that you have an application which listens to those signals, but for some reason, you need to spawn that application via a shell script. This is when you use exec. We are using the same golang application as before but spawning it from a bash script. Notice that this script is invoking that golang binary using exec, this replaces the bash script with golang binary as PID 1.\n#!/bin/bash  echo \u0026#34;spawning the golang app\u0026#34; # This app can handle the signals so no need to handle them on # behalf of the application here in the bash script. exec ./signals FROMquay.io/surajd/fedora-networkingCOPY ./signals /signalsCOPY ./startup.sh /startup.shENTRYPOINT /startup.shIf we get shell access of the container, you will see that the golang signals app has become PID 1.\nHere is the video that shows these signals passing in action:\n  Conclusion I hope this gives you some clearer picture on how to use exec sanely and make sure that the applications are spawned correctly to get the signals sent by their environment be it systemd, docker or Kubernetes.\nReferences  Signal IPC. Bash does not process signals until foreground process returns. Handling signals with sleep. Docker ENTRYPOINT and signal handling. List of signals you can send to your application. How to use trap in your bash script. How signal handlers work? Code snippets used in this blog.  ",
    "ref": "/post/2021/01/shell-exec/"
  },{
    "title": "Monitor releases of your favourite software",
    "date": "",
    "description": "Be on top of the releases of the software you rely upon!",
    "body": "There are various ways to know about the release of your favourite new software, follow the mailing list, check the Github release page periodically, follow the project\u0026rsquo;s Twitter handle, etc. But do you know there is even more reliable way to track the releases of your favourite software released on Github.\nGithub Releases and RSS feeds For every repository on Github, if the project is posting their releases, you can follow the RSS feed of that project\u0026rsquo;s release. The RSS feed link for any project\u0026rsquo;s release is:\nhttps://github.com/projectrepo/projectname/releases.atom For example, you can follow the Istio project release RSS feed here.\nRSS Reader App You can find many RSS reader apps out there, read this Zapier article about the other options. In these apps add the *.atom links to follow the feed.\nHere I will show how to track releases with Slack.\nStep 1: Go to Apps section in Slack sidebar. Step 2: Search for RSS. Once you click the \u0026ldquo;Add\u0026rdquo; it will open up browser.\nStep 3: Add to Slack. Step 4: Add the *.atom URLs. In the \u0026ldquo;Add a Feed\u0026rdquo; section start adding the RSS feed links of the projects you want to monitor:\nStep 5: Check the subscribed feeds. Step 6: Add from Slack comments section. You can also start watching a feed of a project from the Slack comment section. Type the following command in a channel you want to those release notifications:\n/feed subscribe https://github.com/kubernetes/kubernetes/release.atom You will see a comment like this:\nClosing remarks I hope you don\u0026rsquo;t miss out on the latest release of your favourite application.\n",
    "ref": "/post/monitor-releases-of-your-favourite-software/"
  },{
    "title": "Mental models for understanding Kubernetes Pod Security Policy",
    "date": "",
    "description": "Getting the game of PSP right!",
    "body": "PodSecurityPolicy (PSP) is hard to get right in the first attempt. There has never been a situation when I haven\u0026rsquo;t banged my head to get it working on the cluster. It is a frustrating experience, but it is one of the essential security features of Kubernetes. Some applications have started shipping the PSP configs with their helm charts, but if a helm chart does not ship a PSP config, it must be handcrafted by the cluster-admin to make the application work reliably in the cluster.\nThis post assumes that you already know what Pod Security Policies and Role-based access control (RBAC) are. You can find the best explanation of the basics from the upstream documentation.\nThis post will define a nomenclature for PSP types, how a cluster-admin can create a PSP that is allowed for all workloads, and how a developer can generate PSP from scratch. This post won\u0026rsquo;t try to define steps on achieving above but will give guidelines and point you into the right direction for securing your cluster with PSPs.\nPolicy Order Some PSPs also mutate the pod specification besides allowing or disallowing pods. The policy order of Kubernetes documentation states the following two ways a PSP is chosen for a pod:\n If a PSP allows the pod specification as is without a mutation, then that PSP is used. If the above condition fails, then the fist PSP is chosen from an allowed-PSP list, and the pod is mutated accordingly.  So from the above policy order, we can infer that we will need two sets of PSPs in a cluster:\n Application-Specific PSP: This ensures that the pod passes as it is without a mutation. The application developers have to ensure that the PSP config they ship with the deployment manifests should match exactly otherwise their application could either be disallowed from working in a PSP enabled cluster or runs at reduced privileges (due to mutation) enforced by cluster-wide PSP. Cluster-wide PSP: The cluster should have a \u0026ldquo;restrictive\u0026rdquo; policy which comes into the picture as per the second point in the policy order. Such a restrictive PSP should be allowed to all the workloads.  Application-specific PSP These are the general steps I take when crafting the PSP that is application-specific:\n Lockdown the application with the help of pod\u0026rsquo;s security context. Explore each option in the security context and decide what is suitable for the application. Ensure that the application is working fine in the above-restricted settings previously unknown to the application. Start from privileged PSP (see below) and keep restricting it as per the application pod specification.  apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: privileged annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;*\u0026#39; spec: privileged: true allowPrivilegeEscalation: true allowedCapabilities: - \u0026#39;*\u0026#39; volumes: - \u0026#39;*\u0026#39; hostNetwork: true hostPorts: - min: 0 max: 65535 hostIPC: true hostPID: true runAsUser: rule: \u0026#39;RunAsAny\u0026#39; seLinux: rule: \u0026#39;RunAsAny\u0026#39; supplementalGroups: rule: \u0026#39;RunAsAny\u0026#39; fsGroup: rule: \u0026#39;RunAsAny\u0026#39; source: Kubernetes PSP documentaton, privileged PSP.\nOnce the above conditions are satisfied, it does not matter what you name it, because this PSP will be given first priority over any other allowed PSP since it enables the pod created by your application to pass precisely without any mutation.\nIn the above gif, you can see that the wall has a hole (silhouette) of Bugs Bunny, think of it as if the Kubernetes cluster only allows PSP with bunny properties lets call it bunny-psp. Now the Elmer Fudd tries to enter through it but cannot since no PSP allows his properties to pass through. So the cluster-admin has to either create a PSP that matches his profile, and he can pass through the wall without hitting it.\nYou can also use tools like PSP advisor from sysdig, but please perform the above step 1 and step 2 from the above list before you use this tool. This tool basically automates the step 3 only.\nCluster-wide PSP These PSPs apply to the workloads that don\u0026rsquo;t ship their own PSP manifests. You would want your clusters to have a catch-all PSP config. Such a generalised PSP should ideally be restrictive in nature. The PSP should have everything locked down like no root user, no privilege escalation using privileged field, all the host namespace fields are disallowed, etc.\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: aa-restricted annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;docker/default,runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/allowedProfileNames: \u0026#39;runtime/default\u0026#39; seccomp.security.alpha.kubernetes.io/defaultProfileName: \u0026#39;runtime/default\u0026#39; apparmor.security.beta.kubernetes.io/defaultProfileName: \u0026#39;runtime/default\u0026#39; spec: privileged: false # Required to prevent escalations to root. allowPrivilegeEscalation: false # This is redundant with non-root + disallow privilege escalation, # but we can provide it for defense in depth. requiredDropCapabilities: - ALL # Allow core volume types. volumes: - \u0026#39;configMap\u0026#39; - \u0026#39;emptyDir\u0026#39; - \u0026#39;projected\u0026#39; - \u0026#39;secret\u0026#39; - \u0026#39;downwardAPI\u0026#39; # Assume that persistentVolumes set up by the cluster admin are safe to use. - \u0026#39;persistentVolumeClaim\u0026#39; hostNetwork: false hostIPC: false hostPID: false runAsUser: # Require the container to run without root privileges. rule: \u0026#39;MustRunAsNonRoot\u0026#39; seLinux: # This policy assumes the nodes are using AppArmor rather than SELinux. rule: \u0026#39;RunAsAny\u0026#39; supplementalGroups: rule: \u0026#39;MustRunAs\u0026#39; ranges: # Forbid adding the root group. - min: 1 max: 65535 fsGroup: rule: \u0026#39;MustRunAs\u0026#39; ranges: # Forbid adding the root group. - min: 1 max: 65535 readOnlyRootFilesystem: false source: Kubernetes PSP documentaton, restrictive PSP\nSuch a policy is allowed to every authenticated user (including service accounts) via the group system:authenticated. In the ClusterRoleBinding, when you provide subjects list the aforementioned group as one of them and the ClusterRole mentioned under the roleRef is bound to each and every authenticated user on the cluster.\n- kind: Group apiGroup: rbac.authorization.k8s.io name: system:authenticated source: Kubernetes PSP documentaton\nAllowing this policy to every authenticated user (even workload users a.k.a. service accounts) is not enough. You also need to make sure that this policy is on the top of the alphabetically sorted list. Ensure that the policy name starts with aa, so it is always on the top of the list.\nsource: What Cheese Looks Like Around The World by Insider.\nA generic restrictive PSP that mutates the pods looks like above, cheese cutting/moulding machine. It is ensuring that whatever comes out is of fixed mould, in case of Kubernetes the pod has fixed security restrictions.\nConclusion I hope this blog gives you some mental models of understanding how PSPs works in general. Although PSP is being discussed in upstream for replacing with something robust like the OPA Gatekeeper, it is not official yet. It is not clear when will that materialise, but until then, PSP is the saviour we have. If you are still confused about this, please reach out to me I will happy to explain.\nEDIT: The PSP is officially deprecated, I was wrong before to mention it is not deprecated yet. This was pointed out to me on a Reddit thread. According to this Kubernetes PR, PSP will be deprecated in Kubernetes v1.21 and removed entirely in v1.25.\n",
    "ref": "/post/mental-models-for-understanding-kubernetes-pod-security-policy/"
  },{
    "title": "Linux Partitioning Guide",
    "date": "",
    "description": "How to create partitions for Linux?",
    "body": "I use Fedora Linux as my primary desktop OS. Every time there is a fresh install, I find myself confounded on how to partition the OS. So I thought I might as well make a permanent note of how I do it so that I always have a place to come back to.\nPartitioning Scheme This is how I partition my Fedora during installation:\n   Partition Mounted On Size Encrypted Filesystem     Boot /boot 512M No ext4   Boot EFI /boot/efi 200M No vfat   Swap - 1.5 times the RAM Yes swap   Home /home 265G Yes ext4   Root / 211G No ext4    Encryption Note that Swap and Home partitions have to be encrypted. Swap extends the RAM and can have a sensitive copy of RAM data like passwords, keys, etc. Hence always ensure to encrypt the Swap partition. Home partition is equally essential to be encrypted because this is where your data will live. Things like configuration, SSH keys, GPG keys, API keys are all stored in the home in various directories. So it is of utmost importance that you encrypt these two directories.\nSize My disk size is 512G. I have divided Home and Root as per convenience. So depending on your usage of each partition you can decide what is best for you.\nI hope you find this sort of partitioning scheme helpful. It has worked for me for the past five years.\n",
    "ref": "/post/fedora-partitioning/"
  },{
    "title": "Book Review: How to Take Smart Notes",
    "date": "",
    "description": "One Simple Technique to Boost Writing, Learning and Thinking ‚Äì for Students, Academics and Nonfiction Book Writers by S√∂nke Ahrens",
    "body": "Introduction How to Take Smart Notes: One Simple Technique to Boost Writing, Learning and Thinking ‚Äì for Students, Academics and Nonfiction Book Writers by S√∂nke Ahrens is a small (171 pages) non-fiction genre book. The book is a manual explaining Zettlekasten method designed by Niklas Luhmann. S√∂nke has used straightforward and simple English to explain the concepts. For anyone who is a knowledge curator or wishes to publish non-fictional content in any form (text, video or audio), this book is a must-read. I came across this book when I was watching a video by Ali Abdaal named \u0026ldquo;How I Remember Everything I Read\u0026rdquo;. Here he explains various levels of note-taking, how this book has influenced his note-taking capabilities and the foremost reason for making the video. I saw the book wasn\u0026rsquo;t that huge, I bought it and started reading immediately.\nWhat is Zettlekasten and who is Niklas Luhmann? Zettlekasten (German of slip box) was a technique created by German sociologist Niklas Luhmann. In physical terms, it is just a cupboard with multiple drawers holding slips. And in virtual expression, it contains knowledge storing framework similar to our brain. Luhmann used this framework of knowledge storage to think about ideas, and he created a plethora of books, articles and other published material. On average, he published at least two books every year until he was alive.\nWhat is the book really about? Suppose you are planning to research in some area. In that case, this book promises to give you techniques and tools. To build knowledge around that topic, keeping the references in an organised way, think critically on all aspects of that topic, form connections, draw pros and cons on your thinking and finally generate a manuscript based on the cluster of knowledge and ideas.\nNowadays, people are bogged down by the collector\u0026rsquo;s fallacy, and this book teaches you how to be a writer and not be an archivist.\n The way people choose their keywords shows clearly if they think like an archivist or a writer. The archivist asks: Which keyword is the most fitting? A writer asks: In which circumstances will I want to stumble upon this note, even if I forget about it? It is a crucial difference.\n Usually, the approach of note-taking many folks follow is to highlight in the book and write a couple of lines in the margin of the book. Some copy the highlights verbatim, and people forget what that highlights meant when they come back to it later, if at all, to read those highlights. In this book, the author teaches you a new way to take notes. The book emphasises on not highlighting but paraphrasing the content or insights in your own words. Put those insights into a temporary system (fleeting notes) and process these fleeting notes daily and put them into your Zettlekasten in the form of permanent notes. Once you have multiple permanent notes (in the author\u0026rsquo;s words, \u0026ldquo;critical mass\u0026rdquo;), you can link them in the form of a graph.\n If you want to really understand something, you have to translate it into your own words.\n The author tries to break the habit of traditional note-taking where the skeleton of notes is very similar to the content it was taken from, this format is linear. While the note-taking technique in the book is natural, identical to the way brain stores information in neurons. The data is stored with context links. The more contexts you have about specific information, the more you understand and remember it better hence the more ideas you can develop around such a topic. Ideally, your Zettlekasten should resemble your brain and thus help you form those context explicitly rather than you stumbling across such connections in the form of epiphanies. This way of note-taking makes developing insight a structured thing, unlike the usual messy approach.\n By focusing on what is interesting and keeping written track of your own intellectual development, topics, questions and arguments will emerge from the material without force.\n Internalising this technique takes discipline and unlearning old ways (which is the hardest thing). You have to be disciplined enough to take fleeting notes while reading the text and process those fleeting notes within a couple of days to form your permanent notes. Once you have a note which has multiple notes connected to it in the various context, you can create a manuscript out of it. So unlike the traditional method, you don\u0026rsquo;t need to start from blank paper to create content. You already have so much knowledge, facts, ideas and opinions built up, while reading and researching, that you just have to find a way to put it into a linear method on paper.\nStyle of the book The author has a humorous style of writing; thus, it makes an exciting read for a non-fiction genre book. I found myself giggling in between. In the start, I was a bit confused about the Zettlekasten system and found it really hard to wrap my head around this whole notion of clustered thinking. But as I kept going, I started grasping the book. Like, in the later chapters, the author does a walkthrough of converting an insight read from the text into a fleeting note and then into a permanent note. This helped me a lot with understanding how to think while reading, what questions to ask when taking fleeting notes and how to approach while generating permanent note out of a fleeting note.\nSince the author emphasises a lot on writing and its importance in thinking. So to corroborate his claims, he has given numerous examples of how it has been proven by various research that writing is critical to thinking and generating insights.\nPersonal Impact While reading the book, I had one question that should I start taking notes on anything and everything? That will seriously slow my knowledge consumption rate? I found this answer in a book club meeting. Where S√∂nke answered that you don\u0026rsquo;t need to take note about everything you read but only about the topics you are doing research and want to build \u0026ldquo;critical mass\u0026rdquo; around.\n Reading with a pen in the hand, for example, forces, us to think about what we read and check upon our understanding. It is the simplest test: We tend to think we understand what we read ‚Äì until we try to rewrite it in our own words. By doing this, we not only get a better sense of our ability to understand, but also increase our ability to clearly and concisely express our understanding ‚Äì which in return helps to grasp ideas more quickly. If we try to fool ourselves here and write down incomprehensible words, we will detect it in the next step when we try to turn our literature notes into permanent notes and try to connect them with others.\n So while reading anything new like a blog, I start out without a presumption of taking a note. Still, if I find something interesting, then I take a note in notes app or on a paper notebook. And I process it later and put it into my notes system. After reading this book, I haven\u0026rsquo;t become a fanatic notetaker. Because there are some concepts and ideas, I just want to be exposed to and not necessarily do a detailed study on it.\nI haven\u0026rsquo;t stopped highlighting in books. But taking notes in my own words has increased significantly. I am still trying to find a perfect tool for note-taking. But for now, I have started collecting notes in my Notion database. I think this will evolve as my experience with the system and content evolves.\nFor someone reading this book, having finished a couple of chapters, think about how you can take smart notes whenever you read something related to your research area. This exercise, although frustrating at first, will help you understand the rest of the book better. But again the book is worth coming back to, to calibrate your way of taking notes.\nMy personal take away in one line is \u0026ldquo;Writing is Thinking\u0026rdquo;.\nConclusion If you are looking for a way to do personal knowledge management, then this book is not for you. But if you want to research a topic and later publish stuff, then this is a good manual. Go ahead give it a try, it is not a giant book to complete cover to cover.\nPointers  Book\u0026rsquo;s Goodreads Link. My book highlights from Goodreads. S√∂nke\u0026rsquo;s website. A good source of Zettlekasten method (not affliated to this book). YouTube channel related to above website. Digital version of Luhmann\u0026rsquo;s Zettlekasten (still a work in progress). Tiago Forte\u0026rsquo;s Blog on the book.  ",
    "ref": "/post/book-review-how-to-take-smart-notes/"
  },{
    "title": "Book Review: Algorithms to Live by ‚Äî The Computer Science of Human Decisions",
    "date": "",
    "description": "Authors: Brian Christian and Tom Griffiths",
    "body": "Introduction The book \u0026ldquo;Algorithms to Live by ‚Äî The Computer Science of Human Decisions\u0026rdquo; is written by \u0026ldquo;Brian Christian and Tom Griffiths\u0026rdquo;. It fits into the genre non-fiction, psychology and computer science. The book is written lucidly. If you have a background in computer science, then this book is easy to follow. The book creates analogies of computer science algorithms with real-life situations. I felt that some metaphors sound good in reading than their application, so if you plan on applying the things explained in the book directly to your life, they might not work. Because real-life has a lot of constraints that can be simplified in a computer algorithm to solve a problem, so the algorithms don\u0026rsquo;t apply vis-√†-vis.\nAbout Book The authors are qualified to write this book because Brian holds degrees in Philosophy and Computer Science. At the same time, Tom is professor of Psychology and Cognitive Sciences at the University of California Berkeley. So they have a perfect mix of what was needed for the book.\nApart from drawing the parallels between algorithms and real-life, the authors have done an exceptional job in removing the stigma around the word \u0026ldquo;Algorithms\u0026rdquo; by making it more accessible to the layman who has never learnt a \u0026ldquo;C\u0026rdquo; of Computer Science. But my opinions here cannot be impersonal because I knew algorithms already and I was only revising. For a layman, I cannot exactly say if they can go and read algorithms after reading this book and be able to grasp all of it.\nThe authors have succeeded in persuading the reader that solving a real-life condition, the way algorithms do it, is the most efficient way to do. The book does accomplish its goal of conveying the insights from computer algorithms and trying to apply it in real-life. But the aspects of their usage in real-life is questionable as I mentioned earlier. The book is written in an easy to understand language.\nThe authors have chosen one part of computer science in each chapter. The chapter commences with an explanation of a real-life problem. Then it dwells into the solution computer science provides us in this case. Finally, what readers can do in the various real-life restricted situation and finally ends with enlightenments from the chapter on what to do in certain aspects. For, e.g. this is the quote from the last paragraph of the chapter \u0026ldquo;5. Scheduling: First Things First\u0026rdquo;:\n you should try to stay on a single task as long as possible without decreasing your responsiveness below the minimum acceptable limit. Decide how responsive you need to be‚Äîand then, if you want to get things done, be no more responsive than that.\n Unlike the above, not all suggestions will be instantly relevant.\nThe book is not academic as some might feel from the title. Still, the mathematical aspects of algorithms are incredibly minimal or close to none. The book has humour in between while reading I found myself chuckling on one line or another. For, e.g. read this:\n Tom\u0026rsquo;s otherwise extremely tolerant wife objects to a pile of clothes next to the bed, despite his insistence that it\u0026rsquo;s in fact a highly efficient caching scheme. Fortunately, our conversations with computer scientists revealed a solution to this problem too. Rik Belew of UC San Diego, who studies search engines from a cognitive perspective, recommended the use of a valet stand.\n Impressions on me I was already familiar with many CS algorithms. This book cemented my understanding by giving me something that I can easily visualise. My perception earlier was very computer-specific. I used to think only in terms of numbers and strings. Now it is more concrete, and I find myself identifying patterns where I can quickly apply a particular algorithm.\nMy favourite chapters from the book are \u0026ldquo;Scheduling: First Things First\u0026rdquo; and \u0026quot; Game Theory: The Mind of Others \u0026ldquo;. The former chapter helped me personally to devise a formula to prioritise the tasks that I do in my day to day life. Earlier I used to do it manually, but now I have added a formula in my Notion set up. With the later chapter, I had a lot of aha moments. It is because I had only heard about Game Theory but never knew what it was. This chapter gave real-life examples to explain what it is.\n Imagine two shopkeepers in a small town. Each of them can choose either to stay open seven days a week or to be open only six days a week, taking Sunday off to relax with their friends and family. If both of them take a day off, they\u0026rsquo;ll retain their existing market share and experience less stress. However, if one shopkeeper decides to open his shop seven days a week, he\u0026rsquo;ll draw extra customers‚Äîtaking them away from his competitor and threatening his livelihood. The Nash equilibrium, again, is for everyone to work all the time.\n This book has removed one misconception that I had, that computers are very accurate with the results except in data science. I knew that in data science (machine learning) there is not a single answer, so there we can accept answers in probability. But it turns out there are so many situations in which computers rely on acceptable results and are not accurate. The reasoning behind these solutions not being exact is that it is costly to find a precise answer. We, as end-users, don\u0026rsquo;t realise it because the results are such and the probability of the solution being wrong is negligible.\n \u0026ldquo;The idea of the error tradeoff space‚ÄîI think the issue is that people don\u0026rsquo;t associate that with computing. They think computers are supposed to give you the answer. So when you hear in your algorithms class, \u0026lsquo;It\u0026rsquo;s supposed to give you one answer; it might not be the right answer\u0026rsquo;‚ÄîI like to think that when [students] hear that, it focuses them. I think people don\u0026rsquo;t realize in their own lives how much they do that and accept that.\u0026rdquo;\n One instance is in encryption:\n In practice, modern cryptographic systems, the ones that encrypt Internet connections and digital transactions, are tuned for a false positive rate of less than one in a million billion billion. In other words, that\u0026rsquo;s a decimal that begins with twenty-four zeros‚Äîless than one false prime for the number of grains of sand on Earth. This standard comes after a mere forty applications of the Miller-Rabin test. It\u0026rsquo;s true that you are never fully certain‚Äîbut you can get awfully close, awfully quick.\n Closing Thoughts I would highly recommend this book to everyone. Anyone with a computer science background would highly enjoy the book because you can correlate with real-life circumstances. If you are studying computer science, then this is a highly recommended book because you will have another layer of understanding after reading this book. I have taken a ton of notes while reading this book, so I would not recommend the audio version of the book. There are charts and graphs in the book, not sure how they can be comprehended when listening to an audiobook. Look at the highlights from the book and maybe decide for yourself if this book is for you.\nReferences and Links  More information about the book on goodreads. My highlights from the book. Following videos helped me before I started reading this book.     and    DISCLAIMER: All the quotes in this blog are copyright of the authors.  ",
    "ref": "/post/book-review-algorithms-to-live-by-the-computer-science-of-human-decisions/"
  },{
    "title": "How to gracefully kill Kubernetes Jobs with a sidecar?",
    "date": "",
    "description": "A sidecar in a Kubernetes Job, what? Yeah you might need one and here is how to shut it.",
    "body": "Have you ever had a sidecar in your Kubernetes Job? If no, then trust me that you are lucky. If yes, then you will have the frustration of your life. The thing is Kubernetes Jobs are meant to exit on completion. But if you have a long-running sidecar, then that might twist things for Kubernetes and in turn of you.\nWhy would you even want a sidecar for Job? Well, one of the most prevalent use case is when using service mesh proxy. There could be something else as well like metrics endpoint, log collection or whatever. Given the complexity and heterogeneity of the workloads, there could be any kind of use case that involves having sidecar for a Job pod.\nThis blog post will showcase how to cleanly exit from a Job pod if you have a long-running sidecar.\nNormal Job Let\u0026rsquo;s see how a normal Job workflow looks like. I have a Job that runs the following script:\n#!/bin/bash  for num in $(seq 10 -1 1); do echo $num sleep 1 done echo \u0026#34;And it is a lift off!\u0026#34; See the above script in the Github repository here.\nAnd the Kubernetes Job configuration looks like this:\napiVersion: batch/v1 kind: Job metadata: name: foojob namespace: default spec: template: spec: restartPolicy: OnFailure containers: - name: foojob image: fedora:32 command: [\u0026#34;/bin/bash\u0026#34;] args: [\u0026#34;/scripts-dir/run.sh\u0026#34;] volumeMounts: - name: scripts-vol mountPath: /scripts-dir volumes: - name: scripts-vol configMap: name: scripts-configmap See the full definition of the Job configuration here. To understand why the Job configuration has volumes and volumeMounts, read my other blog where I explain how configmaps are the best way to use inject scripts into containers.\nA typical run of above will look like this:\n$ helm install --generate-name job/ NAME: job-1598685079 LAST DEPLOYED: Sat Aug 29 12:41:19 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None $ kubectl get pods NAME READY STATUS RESTARTS AGE foojob-86rlf 0/1 ContainerCreating 0 1s $ kubectl logs foojob-86rlf 10 9 8 7 6 5 4 3 2 1 And it is a lift off! $ kubectl get pods NAME READY STATUS RESTARTS AGE foojob-86rlf 0/1 Completed 0 14s So pod exits with STATUS field set to Completed.\nWhat happens when a Job has Sidecar? To add a sidecar to our existing Job I added a container which just sleeps forever. See following diff to understand what has changed:\ndiff --git job/templates/job.yaml job/templates/job.yaml index 04e7175..1bb5a08 100644 --- job/templates/job.yaml +++ job/templates/job.yaml @@ -9,6 +9,10 @@ spec:  spec: restartPolicy: OnFailure containers: + - name: endlesssidecar + image: fedora:32 + command: [\u0026#34;/bin/bash\u0026#34;] + args: [\u0026#34;-c\u0026#34;, \u0026#34;sleep infinity; echo \u0026#39;stopping now!\u0026#39;\u0026#34;]  - name: foojob image: fedora:32 command: [\u0026#34;/bin/bash\u0026#34;] See the new full configuration of Job manifest here.\n NOTE: In your case, it could be any other sidecar, for illustration purposes I have added a dummy sidecar.\n Now let\u0026rsquo;s see the full run of this setup:\n$ helm install --generate-name job/ NAME: job-1598685624 LAST DEPLOYED: Sat Aug 29 12:50:25 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None $ kubectl get pods NAME READY STATUS RESTARTS AGE foojob-kd9sl 0/2 ContainerCreating 0 1s $ kubectl logs foojob-kd9sl foojob 10 9 8 7 6 5 4 3 2 1 And it is a lift off! $ kubectl get pods NAME READY STATUS RESTARTS AGE foojob-kd9sl 1/2 NotReady 0 21s If you see everything went fine in the foojob container of the Job pod. But the pod STATUS has changed to NotReady. This is because one container has stopped successfully, and the other is still running. This is the problem with sidecar in the Job. Now Kubernetes does not concede this as Completed because not all containers have stopped in the pod.\nJob Sidecar with unique setup! Since the long running sidecar does not stop gracefully, we should make sure that it is halted. So the onus is on the foojob to kill the long running sidecar. So this is what I have added to our simple script:\ndiff --git job/scripts/run.sh job/scripts/run.sh index 8fed959..5932149 100755 --- job/scripts/run.sh +++ job/scripts/run.sh @@ -6,3 +6,6 @@ for num in $(seq 10 -1 1); do  done echo \u0026#34;And it is a lift off!\u0026#34; + +pkill sleep +true But have you realised how does a script running in one container has access to the process in another container? Well, we have a Kubernetes native answer to that: shareProcessNamespace. Here is what it mean straight from the docs:\n Share a single process namespace between all of the containers in a pod. When this is set containers will be able to view and signal processes from other containers in the same pod, and the first process in each container will not be assigned PID 1. HostPID and ShareProcessNamespace cannot both be set.\n In simple terms set the field shareProcessNamespace to true in pod.spec and all containers now share the process namespace and can see each other. Due to this enablement pkill sleep from the foojob container can kill its sidecar or sidecar\u0026rsquo;s main process.\nHere is the diff from the Job YAML file:\ndiff --git job/templates/job.yaml job/templates/job.yaml index 1bb5a08..61d75cf 100644 --- job/templates/job.yaml +++ job/templates/job.yaml @@ -7,14 +7,15 @@ metadata:  spec: template: spec: + shareProcessNamespace: true  restartPolicy: OnFailure containers: command: [\u0026#34;/bin/bash\u0026#34;] - name: foojob - image: fedora:32 + image: surajd/fedora32-pgrep  command: [\u0026#34;/bin/bash\u0026#34;] args: [\u0026#34;/scripts-dir/run.sh\u0026#34;] volumeMounts: The image has been changed from plain fedora:32 to surajd/fedora32-pgrep because the default fedora image does not ship pkill so I installed the package procps and now all the \u0026ldquo;process-killing\u0026rdquo; tools are available in the new docker image. See the dockerfile here.\nNow let\u0026rsquo;s see this in operation:\n$ helm install --generate-name job/ NAME: job-1598689001 LAST DEPLOYED: Sat Aug 29 13:46:41 2020 NAMESPACE: default STATUS: deployed REVISION: 1 TEST SUITE: None $ kubectl get pods NAME READY STATUS RESTARTS AGE foojob-wr9hl 0/2 ContainerCreating 0 1s $ kubectl logs foojob-wr9hl foojob 10 9 8 7 6 5 4 3 2 1 And it is a lift off! $ kubectl logs foojob-wr9hl endlesssidecar Terminated stopping now! $ kubectl get pods NAME READY STATUS RESTARTS AGE foojob-wr9hl 0/2 Completed 0 32s If you notice there is output from the sidecar as well, it first says Terminated which is as a result of the pkill we added. So that is the easiest way to kill the sidecar and satisfy Kubernetes.\nUpstream efforts  There is an issue in Kubernetes upstream for this, find it here. A KEP is present here. And this blog is very much inspired by this comment.  Conclusion We will just have to wait until we have the upstream with better support to segregate sidecar from primary containers in the pod. So until then, this hack is all we\u0026rsquo;ve got. You can find all the configuration used in this blog here.\n",
    "ref": "/post/how-to-gracefully-kill-kubernetes-jobs-with-a-sidecar/"
  },{
    "title": "Use Configmap for Scripts",
    "date": "",
    "description": "A new way to ship scripts to container images.",
    "body": "We generally use some sort of scripts in application container images. They serve various purposes. Some scripts might do an initial setup before the application starts, others may have the whole logic of the container image, etc. Whatever the goal may be the general pattern is to copy the script into the container image, build the image and then the script is available when you consume the image.\nCons of the Traditional Method The round trip time during development and testing of such script is very long. You make some change to the script, you need to build the image, push it and then it is downloaded again. On an average for every change adds a couple of minutes to your feedback loop. Bash scripts are generally precarious in nature. You have to hammer it down, consider edge cases and thereby make it robust. This, of course, takes a lot of iterations. And with iterations comes the added time. So the question is, how do we reduce this feedback loop?\nUse Kubernetes Configmap for Scripts Yes, put the scripts into Kubernetes Configmap. Let me explain. Instead of baking the script into the container image, put it into a configmap and then mount this configmap into the Kubernetes pod. Every time you make changes to the script, just update the configmap and kill the pod, so it picks up a new script from the configmap. The developmental round trip time now changes from a couple of minutes to a couple of seconds.\nDemo Here we will see how to convert a traditionally built container image with a script to configmap based script delivery mechanism.\nTraditional Method See the following directory structure. There is a helm chart in echoscript directory. Container image related configuration resides at the root of the project directory.\n. ‚îú‚îÄ‚îÄ Dockerfile ‚îú‚îÄ‚îÄ echoscript ‚îÇ¬†‚îú‚îÄ‚îÄ Chart.yaml ‚îÇ¬†‚îú‚îÄ‚îÄ templates ‚îÇ¬†‚îÇ¬†‚îî‚îÄ‚îÄ deployment.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ values.yaml ‚îî‚îÄ‚îÄ run.sh 2 directories, 5 files The script does not do anything fancy, just prints some text and then sleeps, this is good for demo purposes. The docker file builds an image based on fedora:32 base.\n$ cat Dockerfile FROM fedora:32 COPY run.sh / ENTRYPOINT [ \u0026#34;/run.sh\u0026#34; ] $ cat run.sh #!/bin/bash echo \u0026#34;Hello from the script baked into the container\u0026#34; echo \u0026#34;Sleeping for eternity!\u0026#34; sleep infinity Let us build and push the image so that when we deploy the chart, the image is pulled successfully: docker build -t surajd/echoscript . \u0026amp;\u0026amp; docker push surajd/echoscript.\nThe deployment configuration is generic and there is nothing noteworthy in it.\n$ cat echoscript/templates/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: name: echoscript labels: app: echoscript spec: replicas: 1 selector: matchLabels: app: echoscript template: metadata: labels: app: echoscript spec: containers: - name: echoscript image: surajd/echoscript Deploy the chart using command helm install test echoscript/ and see it working:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE echoscript-f6499985c-tlr97 1/1 Running 0 57s $ kubectl logs -f echoscript-f6499985c-tlr97 Hello from the script baked into the container Sleeping for eternity! ^C Now every time you want to make changes to the run.sh script you will have to go through the build push cycle.\nConfigmap method Look at this new directory structure. There is no Dockerfile. It is because the script does not need anything other than built-in stuff from a container image if you need specific packages to be installed then build a container image. Also, see that the script run.sh is moved to scripts directory in the helm chart.\n. ‚îî‚îÄ‚îÄ echoscript ‚îú‚îÄ‚îÄ Chart.yaml ‚îú‚îÄ‚îÄ scripts ‚îÇ¬†‚îî‚îÄ‚îÄ run.sh ‚îú‚îÄ‚îÄ templates ‚îÇ¬†‚îú‚îÄ‚îÄ configmap.yaml ‚îÇ¬†‚îî‚îÄ‚îÄ deployment.yaml ‚îî‚îÄ‚îÄ values.yaml 3 directories, 5 files There is a new configuration file called configmap.yaml. Look at the contents of that file:\n$ cat echoscript/templates/configmap.yaml apiVersion: v1 kind: ConfigMap metadata: name: scripts-configmap data: {{ (.Files.Glob \u0026#34;scripts/run.sh\u0026#34;).AsConfig | indent 2 }} So this will automatically load the run.sh from scripts directory into this configmap. I don\u0026rsquo;t have to keep the bash script in a YAML file indented. In this way, I have the independence of editing and viewing file in its original format.\nThere are certain changes made to the deployment manifest as well. Like mentioned earlier there is no specially built image so I can use my base image from the Dockerfile as the container image here. Then there is a config to invoke this bash script. And then parameters to bring the file from configmap into the container image by using volume mount.\n$ git diff echoscript/templates/deployment.yaml diff --git echoscript/templates/deployment.yaml echoscript/templates/deployment.yaml index 34fa7cb..3a10ca2 100644 --- echoscript/templates/deployment.yaml +++ echoscript/templates/deployment.yaml @@ -16,4 +16,13 @@ spec:  spec: containers: - name: echoscript - image: surajd/echoscript + image: fedora:32 + command: [\u0026#34;/bin/bash\u0026#34;] + args: [\u0026#34;/scripts-dir/run.sh\u0026#34;] + volumeMounts: + - name: scripts-vol + mountPath: /scripts-dir + volumes: + - name: scripts-vol + configMap: + name: scripts-configmap Let\u0026rsquo;s install it using command helm install configmapmethod echoscript/ again and see the result:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE echoscript-6479bf46cd-8t2dj 1/1 Running 0 25s $ kubectl logs -f echoscript-6479bf46cd-8t2dj Hello from the script residing in helm chart. Sleeping for eternity! ^C Now let\u0026rsquo;s make changes to the run.sh file. I have modified the file to look something like this:\n#!/bin/bash echo \u0026#34;Hello from the script residing in helm chart.\u0026#34; echo \u0026#34;New line added here.\u0026#34; echo \u0026#34;Sleeping for eternity!\u0026#34; sleep infinity Now apply these changes by running the command helm upgrade configmapmethod echoscript/. And kill the pod using command kubectl delete pod echoscript-6479bf46cd-8t2dj. Now let\u0026rsquo;s see if the new line shows up:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE echoscript-6479bf46cd-mzgn7 1/1 Running 0 38s $ kubectl logs -f echoscript-6479bf46cd-mzgn7 Hello from the script residing in helm chart. New line added here. Sleeping for eternity! ^C So you can see from making the change to witnessing its effects it took less than a couple of seconds.\nCons of Configmap method  Platform: The most significant assumption this method adds is that you are using Kubernetes. Loose coupling: If the container image is used in multiple places, this requires that the user should create corresponding configmaps and volume mount configs. Ease of update: If you are not using helm or kustomize, then making changes to the configmap can be cumbersome, well, you can always do something like this:  kubectl create cm scripts-configmap --from-file echoscript/scripts/run.sh \\  --dry-run=client -o yaml | kubectl apply -f - Conclusion Both approaches have their own pros and cons. But to harness the benefits of both methods, create one config during development and use something else in deployment. That way, you will have the best of both worlds. You will have a faster round trip time in the development phase, and you don\u0026rsquo;t have to repeat the config in the deployment phase. Let me know what is your productivity hack with scripts when iterating on them in this container-dominant world. Happy Hacking!\nLinks  Repository with helm chart: https://github.com/surajssd/echoscript.  ",
    "ref": "/post/use-configmap-for-scripts/"
  },{
    "title": "Being Productive with Git",
    "date": "",
    "description": "Tips and tricks to make your day to day usage easier.",
    "body": "Contents  Introduction Bash Aliases  Configuration Installation   Global Git Configuration  Configuration Installation   Repository Specific Git Settings  Configuration Installation   Bash Git Prompt  Configuration Installation   Git Push PR Reviews  Configuration Installation   Demo Conclusion  Introduction Git is a day to day tool for version control. It has become a de facto method of source code versioning, it has become ubiquitous with development and its an essential skill for a programmer. I use it all the time.\nBecause of its usage frequency, I wanted to optimise the number of keystrokes I made to get things done with it. I have made some tweaks and personalisation to my git workflow. Obviously, it involves aliases, scripts and other bash graphic tools.\nBash Aliases The most straightforward way to shorten any long Linux command is to use an alias. And I use quite a bunch of aliases for everyday git commands. I have aliases for things like checking out the master branch, pulling stuff from remotes commonly named upstream or origin, looking at git commits, rebasing and amending commits.\nConfiguration alias gpum=\u0026#34;git pull --ff upstream master\u0026#34; alias gpom=\u0026#34;git pull --ff origin master\u0026#34; alias gcm=\u0026#34;git checkout master\u0026#34; alias gs=\u0026#34;git status\u0026#34; alias gcmt=\u0026#34;git commit -s\u0026#34; alias gl=\u0026#34;git log --graph --pretty=format:\u0026#39;%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)\u0026lt;%an\u0026gt;%Creset\u0026#39; --abbrev-commit\u0026#34; alias grba=\u0026#34;git rebase -i --autosquash \u0026#34; alias gca=\u0026#34;git commit --amend --no-edit\u0026#34; Installation Above aliases you saw are placed in ~/.bashrc. You can find mine here.\nGlobal Git Configuration Configuration The contents of gitconfig file look like this. I don\u0026rsquo;t have a lot of customisation just basic stuff. In the following snippet code means Visual Studio Code. I use vscode as my primary editor so I prefer it to be my difftool as well. The entire gitconfig is stored in my dotfiles repository.\n[user] name = Suraj Deshmukh [color] diff = auto status = auto branch = auto interactive = auto ui = auto pager = true [diff] tool = default-difftool noprefix = true [difftool \u0026quot;default-difftool\u0026quot;] cmd = code --wait --diff $LOCAL $REMOTE [core] excludesfile = ~/.gitignore editor = code --wait Installation My global git configuration ~/.gitconfig is not something I create manually. I have an unusual way of creating it. So if you look at the configuration, it is just a symlink to the actual file which resides in my dotfiles repository. I have a special way of creating these symlinks, which is explained in my previous blog here.\n$ ls -al ~/.gitconfig lrwxrwxrwx@ 43 surajd 27 Jul 1:05 /home/surajd/.gitconfig -\u0026gt; /home/surajd/git/dotfiles/configs/gitconfig Repository Specific Git Settings There are some settings I want only for that particular repository. For instance, for my work, I sign-off all my commits with my work email id. For those repository-specific settings, I have a script which does the job for me.\nConfiguration Right now this is all I do for work repository, so that\u0026rsquo;s all I need.\ngit config user.email \u0026#34;me@work.com\u0026#34; Installation You can copy above snippet in a file make it executable and place it in your PATH. I have this file in a dotfiles repository from where a script creates a symlink for this executable file and places in the PATH. All this script management framework is explained in my previous blog post here. You can find the exact file here.\nBash Git Prompt Now when you are using plain old bash, it is not very helpful in terms of the context of the directory you are in. When you don\u0026rsquo;t have a visual aid, it is easy to make mistakes like commit something on the wrong branch or other undesired things. So I use this helper utility called bash-git-prompt which provides me with the git directory context. Like it tells me the path to the repository, the branch, the number of files changed, if the files are committed or untracked, exit code of the previous command, current time, etc. The bash prompt will change to look something like this when you are in a git repository:\n‚úò-127 ~/git/blog_contents [foobar L|‚úö 1‚Ä¶1] 23:01 $ Configuration You need to have these two settings in your ~/.bashrc. So that the bash git prompt scripts are sourced every time the terminal starts.\n# Bash Git Prompt source ~/.bash-git-prompt/gitprompt.sh GIT_PROMPT_ONLY_IN_REPO=1 Installation Again this is installed easily using this helper script I have here. So I just have to run the command update-git-prompt, and then the repository is installed to be consumed.\nGit Push I have a script to push my changes to the PR I am working on. So every time I am done committing, I just need to do gpo.sh, and the changes are pushed to your origin remote. Behind the scene, there is a simple command which finds out what the current branch is and then push the current branch to origin.\ngit push origin $(git branch --show-current) \u0026#34;$@\u0026#34; PR Reviews For PR reviews, it is tough to comprehend all the code just by looking on Github. So I like to pull those changes locally. There is a handy script for it that I use. All I run is something like pr.sh 704 origin, so this is pulling PR number 704 from the remote named origin. If you have to pull changes from upstream then specify such or whatever your remote name maybe.\nConfiguration random=\u0026#34;${RANDOM}${RANDOM}\u0026#34; git fetch \u0026#34;${remote}\u0026#34; \u0026#34;pull/${id}/head:pr_${id}_${remote}_${random}\u0026#34; git checkout \u0026#34;pr_${id}_${remote}_${random}\u0026#34; Installation I use my regular framework to install above pr.sh. You can find the entire file here.\nDemo  Step 1: Create branch:  git checkout foobar  Step 2: Make changes as needed. Look at the changes:  gs  Step 3: Add the changes:  git add .  Step 4: Commit these changes:  gcmt  Step 5: Push them:  gpo.sh  Step 6: CI failed, and now you need to make changes. Make necessary fixes and follow step 2 to 3. Then to amend the last commit just run:  gca  Step 7: Look at the git commits in the branch  gl  Step 8: Force push changes now:  gpo.sh -f Conclusion That was my tweaked Git workflow. Let me know your productivity hacks with git, in the comments or on twitter. Thanks for reading. Happy Hacking!\n",
    "ref": "/post/being-productive-with-git/"
  },{
    "title": "Being Productive with Kubectl",
    "date": "",
    "description": "Tips and tricks to make your day to day usage easier",
    "body": "This blog will showcase my productivity tips with kubectl . This does not venture into any plugins per se. But only using bash aliases to achieve it.\nBash Aliases # k8s alias alias k=kubectl alias kg=\u0026#34;kubectl get\u0026#34; alias kgp=\u0026#34;kubectl get pods\u0026#34; alias kgs=\u0026#34;kubectl get services\u0026#34; alias kge=\u0026#34;kubectl get events\u0026#34; alias kgpvc=\u0026#34;kubectl get pvc\u0026#34; alias kgpv=\u0026#34;kubectl get pv\u0026#34; alias kd=\u0026#34;kubectl describe\u0026#34; alias kl=\u0026#34;kubectl logs -f\u0026#34; alias kc=\u0026#34;kubectl create -f\u0026#34; I have above aliases setup in the ~/.bashrc file. The beauty of the aliases is that you can append more flags and parameters to the existing smaller alias. For, e.g. I have an alias for kubectl get pods as kgp, but if I want to get pods from all the namespaces, I use kgp -A.\nMost used of the above aliases are kgp, kd, kl and for anything else just k when I want some other command not aliased.\nBash Auto-Completion You might wonder with all those aliases how do I get the benefit of auto-completion? Well, I don\u0026rsquo;t get the benefit with the aliases except for the alias of kubectl which is k. For that, I have following snippet added in ~/.bashrc file. Take a note of the line source \u0026lt;(kubectl completion bash | sed 's/kubectl/k/g').\nwhich kubectl \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 if [ $? -eq 0 ]; then source \u0026lt;(kubectl completion bash) source \u0026lt;(kubectl completion bash | sed \u0026#39;s/kubectl/k/g\u0026#39;) fi NOTE: The above bash-completion has a check for the binary before sourcing its bash-completion code. This is done to avoid the errors like bash: kubectl: command not found... when starting the terminal.\nChanging namespaces I have two functions for this, also added in the ~/.bashrc.\n# find what the current namespace on the cluster is function current-ns() { kubectl get sa default -o jsonpath=\u0026#39;{.metadata.namespace}\u0026#39; echo } # changing namespace function change-ns() { namespace=$1 if [ -z $namespace ]; then echo \u0026#34;Please provide the namespace name: \u0026#39;change-ns mywebapp\u0026#39;\u0026#34; return 1 fi kubectl config set-context $(kubectl config current-context) --namespace $namespace } For more information, read my other post on the topic of changing namespace in Kubernetes.\nEasy access scripts For updating kubectl, minikube, helm or kustomize I have scripts which update or download(if the binary is never downloaded) the binaries in place. I wrote a post a few weeks back explaining how I manage my bespoke scripts and binaries.\nThere are other scripts for things that need many flags like starting minikube.\nFind the jsonpath I use the JSON path a lot when trying to get a specific field from the YAML output. Now those of you who don\u0026rsquo;t know what I am talking about, let\u0026rsquo;s say you want to find out what time a pod was created. Then you can query it like this:\n$ kgp kube-proxy-hp7kq -o jsonpath=\u0026#39;{.metadata.creationTimestamp}\u0026#39; 2020-08-02T10:39:59Z Here I am trying to find out when was the kube-proxy pod created. Now to even construct that JSON path of highly nested fields, it can become cumbersome. So I use a tool called jiq. See the following small video to understand how it is used:\n  Conclusion I hope this was helpful. I don\u0026rsquo;t use any fancy plugins. But there are many ways out there if you are an Ops person dealing with Kubernetes in your diurnal life let me know what your productivity tips with Kubectl and Kubernetes in general are. Happy hacking!\n",
    "ref": "/post/being-productive-with-kubectl/"
  },{
    "title": "How to backup and restore Prometheus?",
    "date": "",
    "description": "Backing up Prometheus??!!",
    "body": "This blog will show you how to take a backup from a running Prometheus and restore it in some other Prometheus instance. You might ask why would you even want to do something like that? Well, sometimes you want the Prometheus metrics because they were collected for some particular purpose and you want to do some analysis later.\nPrerequisites/Assumptions This blog assumes that you have a Prometheus running that is deployed using prometheus-operator in monitoring namespace. But even if you have deployed it in some other way modify the commands in few places.\nSteps Step: Enable Admin API. Find out what is the name of your Prometheus object:\nkubectl -n monitoring get prometheus Set the value of spec.enableAdminAPI to true. Run the following command to do so:\nkubectl -n monitoring patch prometheus prometheus-operator-prometheus \\  --type merge --patch \u0026#39;{\u0026#34;spec\u0026#34;:{\u0026#34;enableAdminAPI\u0026#34;:true}}\u0026#39; Step: Verify that the admin API is enabled. $ kubectl -n monitoring get sts prometheus-prometheus-operator-prometheus \\  -o yaml | grep admin - --web.enable-admin-api Step: Verify that the pod is up. Wait for the Prometheus pod to be up. It can take some time to be up if you have a lot of data. Move to next step once all the containers are in READY state.\n$ kubectl -n monitoring get pods prometheus-prometheus-operator-prometheus-0 NAME READY STATUS RESTARTS AGE prometheus-prometheus-operator-prometheus-0 3/3 Running 0 33m Step: Port Forward. Open a separate window to port forward and keep it running in the foreground:\nkubectl -n monitoring port-forward svc/prometheus-operator-prometheus 9090 Step: Create a snapshot. Run following command to\n$ curl -XPOST http://localhost:9090/api/v2/admin/tsdb/snapshot {\u0026quot;name\u0026quot;:\u0026quot;20200731T123913Z-6e661e92759805f5\u0026quot;} Step: Locate snapshot. Find the snapshot you have just created in the Prometheus pod. If you have provided the --storage.tsdb.path flag then the snapshot is under:\n\u0026lt;tsdb-dir\u0026gt;/snapshots/\u0026lt;above-dir\u0026gt; To figure out the path just exec into the pod:\n$ kubectl -n monitoring exec -it prometheus-prometheus-operator-prometheus-0 \\  -c prometheus -- /bin/sh -c \\  \u0026#34;ls /prometheus/snapshots/20200731T123913Z-6e661e92759805f5\u0026#34; 01EE25G1ZTKBFBBHFPHNBF99KJ 01EEFF7TE5ENDAGDR5K7ERW3BX ... Step: Copy it locally. Then copy the content from the prometheus container locally by running following command:\nkubectl cp -n monitoring \\  prometheus-prometheus-operator-prometheus-0:/prometheus/snapshots/20200731T123913Z-6e661e92759805f5 \\  -c prometheus . This might take a long time if you have a lot of data.\nStep: Upload it to new Prometheus pod. Delete the data in the existing --storage.tsdb.path. By running following command:\nkubectl -n newprom exec -it prometheus -- /bin/sh -c \u0026#34;rm -rf /prometheus/*\u0026#34; Now upload the old data to the new pod:\nkubectl -n newprom cp ./* prometheus:/prometheus/ NOTE: My new pod\u0026rsquo;s --storage.tsdb.path points to /prometheus.\nI hope this helps, happy hacking!\n",
    "ref": "/post/how-to-backup-and-restore-prometheus/"
  },{
    "title": "Book Review: Getting Things Done",
    "date": "",
    "description": "The Art of Stress-free Productivity - David Allen",
    "body": "Introduction Recently I completed the book called Getting Things Done: The Art of Stress-free Productivity by David Allen. I read the book on my kindle e-reader device, and as the name suggests, it is a self-help category book and about three hundred pages long.\nThe book is an extraordinary walkthrough of how to set up a system that will help you navigate your daily tasks without missing any of them. This system then enables you to patch up the crevices of your memory from which day-to-day tasks fall through.\nIf you have been in a situation where you forget to do something because something urgent was highjacking all the attention? Then this book is an excellent guide for you. The book explains how to build a paper-based task management system, but you can use whatever tool to make a similar system digital or not. I used a tool named Notion for implementing my GTD system.\n Most stress they experience comes from inappropriately managed commitments they make or accept.\n Content and Context The book promises to improve your productivity at the same time, reducing stress about remembering to do the work. This reduced stress frees you to do more work or think about other prospects of your career. In effect, this book not only improves your productivity but also increase your throughput. This book teaches you to break things and tasks into the smallest possible granular actions. These actions can be performed right away if time is allotted to it.\n The very broad and simple definition of a project that I have given (more than one action needed to achieve a desired result) provides an important net to capture the more subtle things that pull or push on your consciousness.\n Author‚Äôs primary purpose is to make sure that people don‚Äôt rely on their brains to remember stuff. But to use it to do creative work. And he emphasises a lot on doing a mind sweep into an external system and having improved productivity without increased stress. This place where you put all your instant ideas about things to do is called In-basket. And then you process each item from the In-basket by following the flowchart. Each item ends up in one of the final baskets. From these different baskets/queues, you handle the work item according to its own triggers.\nWe generally do things ad-hoc or whatever is consuming our attention the most gets the highest priority. But there are better ways of handling and doing things. This book conveys a better method of planning, queuing and executing jobs. What we don‚Äôt realise and nobody tells us is that there are better ways of managing our tasks or things to do. We very well know how to do things we don‚Äôt know how to manage them, and it is not taught in any school or college. This book fills that gap!\n Most decisions for action and focus are driven by the latest and loudest inputs, and are based on hope instead of trust.\n The author has worked in this industry of productivity for more than three decades now, which gives him enough credibility and to the methods introduced in the book. He did workshops with top rank executives of companies. There he helped them build their system to channel information, funnel decisions and build a task administration system. This experience has enabled him to tweak, tune and perfect the system of GTD. It is up to the reader to follow the book religiously and create a framework of their own.\nThe author has done a great job in handholding someone new into creating a system from the ground up. And then the author gives tips on how to use the system, how to keep it up to date? How to make sure you don‚Äôt lose on information or idea and how to create capture tools.\n Most of us have, in the past seventy-two hours, received more change-producing, project-creating, and priority-shifting inputs than our parents did in a month, maybe even in a year.\n Book Organisation and Tone The book is divided into three parts. The first part is the theoretical explanation of what the system will be like. In the second part, it takes a deeper dive into how to create one. In the third and final part, the author talks from his perspective and how he uses it and what is the ideal time to review the system and some other general tips and tricks.\nThe first part can be challenging to go through. Because it is a lot of theory, and I always felt I should get my hands dirty and start implementing what I am reading. But I went through it all and finally got to the part(second part) where I could do something with the information and implement the knowledge learnt.\nThe book is very relatable. When you set out to create your own GTD system, the author gives so many mental cues that help you clear your mind and dump all the information in it into your system. These mental prompts range from single-word categories to situations that a person might encounter and should put into the system.\n Any ‚Äúwould, could, or should‚Äù commitment held only in the psyche creates irrational and unresolvable pressure, 24-7.\n Overall Impression I would prescribe the book to anyone who feels that they are losing their mind over all the stuff they have to do. I was overwhelmed by the tasks I had to do and was not getting to any of them. There were many occurrences where I would forget about doing something only to remember it at the eleventh hour. I was keeping all in my head. Many tasks that were due someday, but I missed deadlines. In other instances, I would get these ideas about things at work or in personal life, really cool ideas, at that moment I would note it down in a note-keeping app as Google Keep or Apple Notes, but there was so little context in them that over time when I looked at these one-liner notes I wouldn\u0026rsquo;t remember what it was all about. These note-keeping apps became dumpsters of ideas whose context was decomposed over time.\nAll this became really frustrating, and that\u0026rsquo;s when I came across this book. I will recommend it to anyone and everyone who wishes to get more done in the same amount of time. The book has helped me set up a system that I can trust to close all my open loops, deliver all the promises made to myself and others.\n The sense of anxiety and guilt doesn‚Äôt come from having too much to do; it‚Äôs the automatic result of breaking agreements with yourself.\n The author helps anyone reading and obeying the book to build a system to be productive. So if you give time to make the system as you read the book, then you will have something to use at the end of the book. Don‚Äôt try to finish the book cover to cover it is more like an instruction manual to follow along. Since I was following along, it took me some time to finish the book. Also, I was watching other videos to implement my system in the tool of my choice, Notion.\n There is no reason to ever have the same thought twice, unless you like having that thought.\n For me personally, the system has helped immensely in my work and personal task keeping and making sure that I don‚Äôt lose out on something that I need to do. I have felt an enormous boost in my productivity and throughput.\nNow even for unloading the ideas in my moment of creative flashes, I use my GTD system built into Notion. In the spur of the moment, I input the thought from my phone. And later, when I get to my computer, I add more context to the idea that I had. And then decide on what needs to be done with the idea.\nThe book also highlights the significance of checklists. Generally, we tend to remember and try to obtain things to do from our brains. It could be stressful, so it is a good idea to make checklists. I have started making checklists for repeatable stuff now. Like I organise a meetup every month, and this involves many small tasks. Now I have created a checklist with the help of my co-organiser, and this relieves the pressure of managing a meetup. Now we can follow a predefined algorithm/checklist.\n Capability and willingness to instantly make a checklist, accessible and used when needed, is a core component of high-performance self-management.\n My favourite part of the book is implementing the system. I would like to get through part two again and try to get those mental hints to dump anything I still am holding in my head to reduce the cognitive baggage. I want my system to become my second brain.\nThe author has used a lot of epigrams in the middle of the book. I think if those are removed, the book size will reduce considerably. Sometimes they feel out of place. Nothing against the quotes themselves, but I felt it was overdone.\nAnyone who likes to improve their own life by doing things efficiently would like the book. People who are productivity nerds or who are always looking for ways to do tasks more effectively would like the book.\nConclusion Before reading this book, when I didn‚Äôt have any system for managing tasks, I used to wander off and waste a lot of time on social media. Now I don‚Äôt do it because I always have an action that I need to do next and doing that task and checking it off the list is my source of dopamine now.\nI won\u0026rsquo;t say this is a perfect system of getting things done. But it is one of the effective ones, and I believe having one is beneficial than going with the flow. This book and the system created from it will help you chart your own flow.\nReferences and Links  Goodreads page for the book. My highlights from the book. What the hell is Notion? https://www.notion.so/ My GTD system, in Notion, was built with inspiration from this video. In getting started with Notion, this playlist helped me.  ",
    "ref": "/post/book-review-getting-things-done/"
  },{
    "title": "Framework for managing random scripts and binaries",
    "date": "",
    "description": "This is an explanation of the framework that I have created to manage scripts and binaries.",
    "body": "I always had a conundrum about how to manage the scripts and binaries downloaded randomly from the internet. One way is to put them in the global PATH directory like /usr/local/bin, but I am sceptical about it.\nThere are a couple of things I wanted to solve. How do you update these scripts and binaries? How to do it consistently across all my machines? How to make it easier to have my setup available on any new Linux machine(or even container) I setup? How to do it without sudo?\nManaging random scripts Global scripts I have a Github repository where I have all my scripts and configurations. In the past, I use to copy these scripts into another directory which is in PATH viz. ~/.local/bin. This directory is added to PATH in Fedora by default.\nNow the problem with this copying approach was that if I had to make any corrections to the script because I found a bug. I had to make those changes in two places, one in the Github repository directory and other in the ~/.local/bin directory. As a human would, I use to forget making changes to the Github repository and this use to go out of sync pretty quickly.\nThat is when I thought of creating symlinks. Now all the files stay in the Github repository directory and then this installer script makes a symlink which is stored in ~/.local/bin. Now when I make changes to the script, those changes are in fact, happening in the repository. And whenever I have time I git commit those changes and git push them to Github.\nFor making changes to the scripts I just have to do this:\nvi $(which start-minikube) Local scripts There are scripts that you want to have only locally. Like syncing code to a remote server. These are the scripts you don\u0026rsquo;t want to share with the whole world. For this problem, I have added .scripts directory to the PATH variable in ~/.bashrc. Now anywhere I can create .scripts directory, it is automatically in PATH.\n$ cat ~/.bashrc | grep scripts export PATH=$PATH:.scripts Now with this foundational setup in place, whenever I have a project-specific script, I create a .scripts directory at the root of the project and put that bespoke script in that directory. So it is available for me from the root of the project directory.\nSince I am creating such directories which are unrelated to the project and should not be committed. I have a global entry for ignoring .scripts directory.\n# ignore the local scripts directory .scripts You can find my ~/.bashrc file here.\nProject Specific Scripts There are specific scripts that you want to use only for a project. Now you might wonder why not contribute them to the project itself. There could be scripts to copy code to a remote server; you don\u0026rsquo;t want to contribute such scripts to the project because they are particular to your workflow. In such a case, I have something called a project-specific .scripts directory in the root of the project. Read more about it in detail in this another blog post of mine here.\nManaging random binaries Now many projects ship their binaries off of their Github releases but don\u0026rsquo;t have a package made for an operating system. Now packaging an application for every operating system could be a daunting task for the maintainers of the project, not everyone is willing to take that on their plate(I am grateful those maintainers already working on the open source project I don\u0026rsquo;t expect them to do more work).\nTo manage binaries of such projects, you need your own mechanism of downloading the new versions and replacing older ones. For this again, I rely on my ~/.local/bin directory. Here I put such downloaded binaries. And for the tools that I need regularly, I have created scripts for them. These scripts are managed as explained in the Global scripts section.\nFor example, I use helm regularly, and I need it to be updated as new releases come. So I have a script with following code snippet:\ncurl -LO https://get.helm.sh/helm-\u0026#34;${version}\u0026#34;-linux-amd64.tar.gz tar -xvzf helm-\u0026#34;${version}\u0026#34;-linux-amd64.tar.gz mv linux-amd64/helm ~/.local/bin/ Now in the above code snippet, I require the user to provide the version. But some tools have consistent naming off of the Github releases so for them I don\u0026rsquo;t require the user to provide the version and finding version is automated as well. For example, the easiest way to find the latest version is using the Github API:\nfunction get_latest_release() { version=$(curl --silent \u0026#34;https://api.github.com/repos/$1/releases/latest\u0026#34; | grep \u0026#39;\u0026#34;tag_name\u0026#34;:\u0026#39; | sed -E \u0026#39;s/.*\u0026#34;([^\u0026#34;]+)\u0026#34;.*/\\1/\u0026#39;) } And use above function like this in my scripts:\nget_latest_release bash/minikube So if you compare the UX for both of them this is how it looks like. One that requires version:\n$ update-helm v3.2.4 + cd /tmp + curl -LO https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz % Total % Received % Xferd Average Speed Time Time Time Current Dload Upload Total Spent Left Speed 100 12.3M 100 12.3M 0 0 3054k 0 0:00:04 0:00:04 --:--:-- 3055k + tar -xvzf helm-v3.2.4-linux-amd64.tar.gz linux-amd64/ linux-amd64/helm linux-amd64/README.md linux-amd64/LICENSE + mv linux-amd64/helm /home/suraj/.local/bin/ One that does not require version:\n$ update-minikube Downloading minikube v1.12.1 Downloaded successfully in ~/.local/bin/ Setting up a new machine With all that framework in place, whenever I setup a new machine all I need to do is run following commands:\ngit clone https://github.com/surajssd/dotfiles cd dotfiles make install-all That\u0026rsquo;s it!\nWith this simplicity I get all the goodness of the scripts and since this repository also places my custom ~/.bashrc I get all the aliases I use to be productive on shell.\nConclusion You can also setup a similar workflow for your scripts and binaries. This will provide you immense boost in productivity. Also if you have some better way to do same sripts and binaries management please share with me here or on Twitter. Happy Hacking!\n",
    "ref": "/post/framework-for-scripts-and-binaries/"
  },{
    "title": "Opinion: Contemporary world vis-√†-vis 1984 by George Orwell",
    "date": "",
    "description": "The Unsettling Similarity of Timeless Dystopian Novel.",
    "body": "The book 1984 was written by Geroge Orwell in 1949 as an attempt to demonstrate how democraries can also fall into the trap of totalitarianism. The story in the book showcases a dystopian world in the year 1984, where there are only three countries¬†in the world, and all of them are in a constant power struggle. All three countries have a totalitarian, oligarchic government of their own. But the story in the book is from a country called Oceania, which is ruled by a party called Ingsoc or English Socialism. This blog is about the similarities of the world in the book 1984 and today. There is no exact present-day equivalent of Ingsoc except, to certain extent, the Communist Party of China.\nSource of image.\nFreedom of Expression, What?  ‚ÄòDon‚Äôt you see that the whole aim of Newspeak is to narrow the range of thought? In the end we shall make thoughtcrime literally impossible, because there will be no words in which to express it.\n The book states that the Party would expunge history and had complete control over the media viz. print, television, radio. Citizens had no freedom of speech, even thinking against the Party, i.e. thoughtcrime was forbidden and punished by death. Citizens were observed using a television screen all the time, so there is constant surveillance.\n They could spy upon you night and day, but if you kept your head you could still outwit them. With all their cleverness they had never mastered the secret of finding out what another human being was thinking. Perhaps that was less true when you were actually in their hands.\n I think in the present day, the rule that comes closest in the aforementioned aspects is the Chinese government. Even though the Party(Ingsoc) had more diabolical methods of restraining citizens and Chinese ways might be more subtle. The Indian government also has similarity with the control of media ‚Äî news, movies, national press ‚Äî to some extent. However, India being a democracy, still has many rights and freedoms(especially freedom of expression, otherwise this blog could get me in trouble) which Indians relish, which is not the case in either China or Oceania. On the aspect of surveillance, most countries are doing it now, and many of them are quite similar to Oceania.\nDistortion of History  And if all others accepted the lie which the Party imposed‚Äîif all records told the same tale‚Äîthen the lie passed into history and became truth. ‚ÄòWho controls the past,‚Äô ran the Party slogan, ‚Äòcontrols the future: who controls the present controls the past.‚Äô\n The Party in Oceania, Ingsoc, also misconstrued facts and history in favour of the Party. They had a Ministry of Truth for handling such forgery. Books, newspaper articles and every written accord were altered to reflect the new reality.\n The past not only changed, but changed continuously.\n The Chinese government has done similar things to purge the account of 1989 Tiananmen Square protests, now no youngsters in China know about the history. The Indian administration has enacted such acts in the past, where they deliberately changed the history to downplay the contributions of the first PM of India Jawaharlal Nehru and defame him.\n And when memory failed and written records were falsified ‚Äì when that happened, the claim of the Party to have improved the conditions of human life had got to be accepted, because there did not exist, and never again could exist, any standard against which it could be tested.\n Deliberate hiding of history has happened in the UK as well. The British education system does not teach their school kids the horrors of colonialism and their hegemony over the world. How Victorian England reigned terror and brought atrocities on the native population in their colonies.\nEfface Science  In the end the Party would announce that two and two made five, and you would have to believe it.\n The Party was working on this new language called¬†Newspeak, where they were dismantling down the English language to its bare minimum. In this, they also removed the word Science. And they altered the scientific facts, relegating science to be just a tool for propaganda. For example, they attributed all the past inventions as the inventions of the Party. The only employment of science was in warfare. The familiarity of science became so esoteric that ordinary people had no concern with it whatsoever.\nIn India also the current government has tried to twist scientific discoveries and inventions to create misleading correlations between modern-day devices and the old mythological stories.\nFoment Nationalism to Hide Failures  It was not desirable that the proles should have strong political feelings. All that was required of them was a primitive patriotism which could be appealed to whenever it was necessary to make them accept longer working-hours or shorter rations. And even when they became discontented, as they sometimes did, their discontent led nowhere, because being without general ideas, they could only focus it on petty specific grievances. The larger evils invariably escaped their notice.\n Governments have been using jingoism as a tool to cover their failure and misdeeds; this helps governments in distracting their subjects to other less critical issues. Authorities need a scapegoat where they can divert the hatred of citizens to some other dupe. So they can abscond from their scandals for the time being.\nArousing nationalism is a trick from the old book used repeatedly in history. Nazis used this tool and made Jews as their patsy. Due to the ongoing coronavirus situation, many Chinese have lost their jobs and in fear of an uprising from the citizens the government is being hostile on Indian, Taiwanese, Japanese borders and in South China sea. Also, Chinese government is trying to bedeck its image in the eyes of its citizens by brandishing how barbaric democracies are. Like in the USA, in the light of recent protests of black lives matter, there have been some incidents of violence, and Chinese government propaganda is trying to show that only the violent parts of the whole situation.\nThe Indian government has used similar tactics like doing airstrikes on the Pakistani area to instigate patriotism and gain votes in 2019 elections. It has helped them to put a veil on their inadequacy to thrive the Indian economy, which is plummeting at its highest rate than ever.\n induced hysteria, which was desirable because it could be transformed into war-fever and leader-worship.\n Final Thoughts  the weaker the opposition, the tighter the despotism.\n All of the above factors help the propaganda machine to function effectively. When it does not work on specific individuals, use force to vaporise (get rid of) them.\n Freedom is the freedom to say that two plus two make four. If that is granted, all else follows.\n Definition of Orwellian Wikipedia:  \u0026ldquo;Orwellian\u0026rdquo; is an adjective describing a situation, idea, or societal condition that George Orwell identified as being destructive to the welfare of a free and open society. It denotes an attitude and a brutal policy of draconian control by propaganda, surveillance, disinformation, denial of truth (doublethink), and manipulation of the past, including the \u0026ldquo;unperson\u0026rdquo;‚Äîa person whose past existence is expunged from the public record and memory, practiced by modern repressive governments.\n Ted-Ed:  If they\u0026rsquo;re talking about the deceptive and manipulative use of language, they\u0026rsquo;re on the right track. If they\u0026rsquo;re talking about mass surveillance and intrusive government, they\u0026rsquo;re describing something authoritarian but not necessarily Orwellian. And if they use it as an all-purpose word for any ideas they dislike, it\u0026rsquo;s possible their statements are more Orwellian than whatever it is they\u0026rsquo;re criticizing.\n   References When I read a book I try to find complementary content related to the book in the form or videos on YouTube, blogs or Wikipedia articles about it. This helps me understand the book and the concepts it includes. Especially when the book has a deep impact on the human society. 1984 is book of that kind, the accounts in the book still hold true today to a large extent. At the end of this blog you will find the list of all the videos and other articles that helped me get a good grasp of the book 1984.\nVideos  Crash Course videos     and    Playlist  1984 - George Orwell from Course Hero\n",
    "ref": "/post/opinion-contemporary-world-1984-by-george-orwell/"
  },{
    "title": "Book Review of Einstein: His Life and Universe",
    "date": "",
    "description": "Authored by Walter Isaacson",
    "body": "I recently finished this book Einstein: His Life and Universe by Walter Isaacson. And here are my thoughts on the book.\nIt\u0026rsquo;s a book that brings the image of Einstein to life.\nAlthough the book is a biography, it makes a reasonable effort in explaining the physics behind his theories of relativity, photoelectric effect and quantum physics. Physics in the book can be intimidating to someone coming from the non-Scientific background. Since it is in the early chapters, one might feel a compulsion to abandon the book. Still, I would urge you to persevere, and the story flows like any novel after that.\nThere are many epigrams stated by Einstein that are worth quoting, and I found them fascinating. His observations of extremism in Germany are very much visible in the present world and in India.\n \u0026ldquo;A foolish faith in authority is the worst enemy of truth\u0026rdquo;\n This statement made a lasting mark on me. This book has definitely made me non-conformist and sceptic of the authority of any kind. Where Einstein later contemplated:\n \u0026ldquo;To punish me for my contempt of authority, Fate has made me an authority myself.\u0026rdquo;\n With so many people associated with Einstein, it is sometimes hard to picture someone against their names. So someone starting the book should go to the end of the book and look at the pictures of people. This will help in making associations easier.\nThe book explains the thought process of Einstein. How he postulated things first and then reached mathematical proof afterwards.\n Einstein\u0026rsquo;s great strength as a theorist was that he had a keener ability than other scientists to come up with what he called \u0026ldquo;the general postulates and principles which serve as the starting point.\u0026rdquo;\n The immense curiosity that Einstein garnered even about the mundane things of everyday life and his non-conformist attitude in questioning long-held beliefs even if they were postulated by venerated people is impressive.\nIn my education about Physics and Chemistry, I have only studied the theories and mathematical formulae postulated by the scientists mentioned in the book. It was a delight to read that they were contemporaries of Einstein and got to know some back story of their own.\nI have highlighted few notes from the book which you can find on the goodreads page.\n",
    "ref": "/post/book-review-einstein/"
  },{
    "title": "Watch Container Traffic Without Exec",
    "date": "",
    "description": "How to watch the traffic of a container or a pod without execing into the pod/contaienr?",
    "body": "Introduction For the reasons of security, many container deployments nowadays run their workloads in a scratch based image. This form of implementation helps reduce the attack surface since there is no shell to gain access to, especially if someone were to break out of the application.\nBut for the developers or operators of such applications, it is hard to debug. Since they lack essential tools or even bash for that matter, but the application\u0026rsquo;s debugging ability should not dictate its production deployment and compromise its security posture.\nThis blog shows one of the many debugging requirements of any connected application, i.e. network snooping or network debugging. I won\u0026rsquo;t emphasise the importance of network debugging in the world of microservices, where containers lead the deployment model.\nNow imagine a scenario where you have an application, statically built based on scratch image and running inside a container. How do you watch the traffic of such an app? You don\u0026rsquo;t have any terminal in that container. You are not allowed to make changes to the environment of the application because this is a production setup. This barebone container image deployment is the exact scenario that this post helps in debugging.\nContainer Scenario For the demo, I have a running application which is doing some networking.\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 241bf66c36c6 fedora \u0026#34;bash\u0026#34; 14 minutes ago Up 14 minutes upbeat_heisenberg The IP address of this application is 172.17.0.2 found using following command:\n$ docker inspect upbeat_heisenberg | grep IPAddress | tail -1 \u0026#34;IPAddress\u0026#34;: \u0026#34;172.17.0.2\u0026#34;, And on the server-side I have the following simple HTTP server running on the host machine:\n$ python3 -m http.server Serving HTTP on 0.0.0.0 port 8000 (http://0.0.0.0:8000/) ... 172.17.0.2 - - [06/Jun/2020 18:53:18] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - [REDACTED] The IP address of the server is 172.17.0.1 found using the following command:\n$ ip address show docker0 | grep \u0026#39;inet \u0026#39; inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0 Debugging Now I cannot exec into the container, but I want to see what traffic sent by the application inside the container. Let\u0026rsquo;s find out the PID of the process running inside the container:\n$ docker inspect --format \u0026#39;{{.State.Pid}}\u0026#39; 241bf66c36c6 1371226 Alright, now we know that the process running inside has PID 1371226. Now lets us try to enter into this process\u0026rsquo;s network namespace and use the good old tcpdump.\n$ sudo nsenter -t 1371226 -n tcpdump -A -s 0 \u0026#39;(((ip[2:2] - ((ip[0]\u0026amp;0xf)\u0026lt;\u0026lt;2)) - ((tcp[12]\u0026amp;0xf0)\u0026gt;\u0026gt;2)) != 0)\u0026#39; dropped privs to tcpdump tcpdump: verbose output suppressed, use -v or -vv for full protocol decode listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 19:21:26.685275 IP thinkpad.36852 \u0026gt; _gateway.irdmi: Flags [P.], seq 2980991125:2980991212, ack 2001355994, win 502, options [nop,nop,TS val 3794513222 ecr 3920804277], length 87 E.....@.@..............@..P.wJD.....X...... .+.F....GET /file.txt HTTP/1.1 Host: 172.17.0.1:8000 User-Agent: curl/7.66.0 Accept: */* 19:21:26.686787 IP thinkpad.56338 \u0026gt; XiaoQiang.domain: 8019+ PTR? 1.0.17.172.in-addr.arpa. (41) E..E..@.@..+...........5.1...S...........1.0.17.172.in-addr.arpa..... 19:21:26.686958 IP _gateway.irdmi \u0026gt; thinkpad.36852: Flags [P.], seq 1:186, ack 87, win 509, options [nop,nop,TS val 3920804279 ecr 3794513222], length 185 E.....@.@.9/.........@..wJD...P.....Y...... .....+.FHTTP/1.0 200 OK Server: SimpleHTTP/0.6 Python/3.7.7 Date: Sat, 06 Jun 2020 13:51:26 GMT Content-type: text/plain Content-Length: 12 Last-Modified: Sat, 06 Jun 2020 13:22:34 GMT 19:21:26.687101 IP _gateway.irdmi \u0026gt; thinkpad.36852: Flags [P.], seq 186:198, ack 87, win 509, options [nop,nop,TS val 3920804279 ecr 3794513224], length 12 E..@..@.@.9..........@..wJE...P.....XX..... .....+.HI am Server [REDACTED] There you go, the output shows that the target server 172.17.0.1:8000 is responding over HTTP protocol I am Server.\nWith Kubernetes The same can be achieved with Kubernetes as well. Follow these steps:\ndocker ps | grep \u0026lt;pod-initial\u0026gt; ID= // take docker id by hand PID=`docker inspect --format '{{.State.Pid}}' $ID` sudo nsenter -t $PID -n tcpdump References and Credits  Thanks to my colleague Mauricio V√°squez Bernal, from whom I learnt this trick. Tweet about these steps in brief details https://twitter.com/surajd_/status/1152218150932365312. The complex tcpdump command used to decode the HTTP traffic: https://sites.google.com/site/jimmyxu101/testing/use-tcpdump-to-monitor-http-traffic.  ",
    "ref": "/post/snoop-on-pod-traffic/"
  },{
    "title": "Enabling Seccomp on your Prometheus Operator and related Pods",
    "date": "",
    "description": "This post shows how you can enable seccomp on all the Pods that are deployed with Prometheus Operator",
    "body": "Seccomp helps us limit the system calls the process inside container can make. And PodSecurityPolicy is the way to enable it on pods in Kubernetes.\nPrometheus Operator Prometheus Operator makes it really easy to monitor your Kubernetes cluster. To deploy this behemoth, helm chart is the easiest way to do it.\nAlmost all the pods that run as a part of Prometheus Operator viz. Prometheus Operator, Prometheus, Alertmanager, Grafana, Kube State Metrics don‚Äôt need to run with elevated privileges except Node Exporter. In your Kubernetes cluster if you are using PodSecurityPolicy to make sure that your cluster is secure, then you would want your Prometheus Operator pods to run securely as well. And the good news is, Prometheus Operator chart ships PodSecurityPolicy for each sub-component. We will look at how to enable seccomp for all the sub-components.\nSince these components have their own PSPs, to enable seccomp on the pods you only need to add specific annotations in metadata.annotations. These annotations can help you to select and set the seccomp profiles that is applied to the Pod which is mutated by that PSP. More information on seccomp with PSP is in the Kubernetes docs.\nIn examples below I assume that you are running your workloads on Docker, hence the annotation value is docker/default. If you are running your workloads on other runtimes then use the generic runtime/default policy as mentioned in the docs. There are ways to provide custom seccomp profiles, but it is out of scope of this post.\nKube State Metrics Using Kube State Metrics is the best way to monitor your Kubernetes cluster state. Kube State Metrics chart supports custom PSP annotations which is little different from other components.\nIn the Prometheus Operator helm chart values file add following snippet to enable seccomp to the Kube State Metrics Pods:\nkube-state-metrics: podSecurityPolicy: annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;docker/default\u0026#39; seccomp.security.alpha.kubernetes.io/defaultProfileName: \u0026#39;docker/default\u0026#39; Once this is added the resultant PSP will have following annotations:\napiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: prometheus-operator-kube-state-metrics annotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: docker/default seccomp.security.alpha.kubernetes.io/defaultProfileName: docker/default Now the Kube State Metrics pod that is mutated by the above PSP will have a seccomp profile that is shipped with Docker.\nPrometheus Operator For workloads Prometheus, Prometheus Operator, Alertmanager here are steps to enable seccomp on those pods. Since Grafana chart already ships pods with seccomp enabled so we don\u0026rsquo;t need any special provisions.\nIn the Prometheus Operator helm chart values file add following snippet:\nglobal: rbac: pspAnnotations: seccomp.security.alpha.kubernetes.io/allowedProfileNames: \u0026#39;docker/default\u0026#39; seccomp.security.alpha.kubernetes.io/defaultProfileName: \u0026#39;docker/default\u0026#39; This will add above annotations to the PSP metadata.annotations of the aforementioned workloads.\nVerify if seccomp is enabled on a pod To verify if the seccomp is enabled on a pod, you kubectl exec into the pod and run following command:\ncat /proc/self/status | grep Seccomp: If the output is Seccomp:\t2 then seccomp is enabled. If it is Seccomp:\t0 then seccomp is disabled.\nReference Read more in depth about Kubernetes and Seccomp in this blog post.\n",
    "ref": "/post/seccomp-in-kube-state-metrics/"
  },{
    "title": "Capabilities on executables",
    "date": "",
    "description": "Note on Linux Kernel capabilities",
    "body": "File capabilities allow users to execute programs with higher privileges. Best example is network utility ping.\nA ping binary has capabilities CAP_NET_ADMIN and CAP_NET_RAW. A normal user doesn\u0026rsquo;t have CAP_NET_ADMIN privilege, since the executable file ping has that capability you can run it.\n$ getcap `which ping` /usr/bin/ping = cap_net_admin,cap_net_raw+p Which normally works as follows:\n$ ping -c 1 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.1.1.1: icmp_seq=1 ttl=55 time=36.9 ms --- 1.1.1.1 ping statistics --- 1 packets transmitted, 1 received, 0% packet loss, time 0ms rtt min/avg/max/mdev = 36.885/36.885/36.885/0.000 ms If you copy file as a normal user the binary loses its privilege and the command ceases to work:\n$ cp `which ping` /tmp/ping $ /tmp/ping -c 1 1.1.1.1 ping: socket: Operation not permitted References Linux Capabilities: making them work\n",
    "ref": "/post/linux-cap-note/"
  },{
    "title": "Root user inside container is root on the host",
    "date": "",
    "description": "The easiest way to prove that root inside the container is also root on the host",
    "body": "Here are simple steps that you can follow to prove that the root user inside container is also root on the host. And how to mitigate this.\nRoot in container, root on host I have a host with docker daemon running on it. I start a normal container on it with sleep process as PID1. See in the following output that the container clever_lalande started with sleep process.\n$ docker run -d --rm alpine sleep 9999 6c541cf8f7b315783d2315eebc2f7dddd1f7b26f427e182f8597b10f2746ab0b $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6c541cf8f7b3 alpine \u0026#34;sleep 9999\u0026#34; 12 seconds ago Up 11 seconds clever_lalande Now let\u0026rsquo;s find out the process sleep on the host. Here in the following output you can see that the process sleep is running as user root.\n$ ps aufx | grep sleep USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 4826 0.3 0.0 1552 4 ? Ss 07:34 0:00 \\_ sleep 9999 core 4864 0.0 0.0 6864 964 pts/0 S+ 07:34 0:00 \\_ grep --colour=auto sleep Also the sleep process is root inside the container.\n$ docker exec -it clever_lalande id uid=0(root) gid=0(root) Non-root inside the container, non-root on host The user I am logged into this machine is called core with user id 500.\n$ whoami core $ id uid=500(core) gid=500(core) groups=500(core),10(wheel),233(docker),248(systemd-journal),250(portage),251(rkt) context=system_u:system_r:kernel_t:s0 Let\u0026rsquo;s start the container in the same way but with additional flag --user. Here I started a new container and forced it to run under the security context of the user core. Here I specify the UID same as the UID on the host. The documentation of the flag says that:\n -u, \u0026ndash;user=\u0026quot;\u0026quot;.\nSets the username or UID used and optionally the groupname or GID for the specified command.\n Here is the container that is started with sleep process named wonderful_proskuriakova.\n$ docker run -d --rm --user ${UID}:${UID} alpine sleep 9999 1cdc11a449e4e62a9557a4d7b586aa320f5512f2746f4a8e1cac7b9e6d2e1225 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 1cdc11a449e4 alpine \u0026#34;sleep 9999\u0026#34; 25 seconds ago Up 25 seconds wonderful_proskuriakova If I try to find the same process on the host here you can clearly see that the process is not running as root but as user core.\n$ ps aufx | grep sleep USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND core 4607 0.0 0.0 1552 4 ? Ss 07:30 0:00 \\_ sleep 9999 core 4648 0.0 0.0 6864 900 pts/0 S+ 07:30 0:00 \\_ grep --colour=auto sleep Also inside the container I am running as UID 500.\n$ docker exec -it wonderful_proskuriakova id uid=500 gid=500 This is just one way to mitigate the user to be non-root. You should also drop all the capabilities and whitelist the capabilities that are absolutely needed. Also provide a seccomp profile that is locked down which only allows the syscalls that are needed by your application. There are many times where you want to run as root inside the container in such situations you should use user namespaces, which is what we are looking in the next section.\nRoot inside container, non-root on host Now I have docker daemon which is started with docker user namespace enabled. Look at the flag --userns-remap=default being used to start the docker daemon with user namespace. To know more about enabling user namespace, follow docs here.\n$ systemctl status docker-userns ‚óè docker-userns.service - Docker Application Container Engine Loaded: loaded (/etc/systemd/system/docker-userns.service; disabled; vendor preset: disabled) Active: active (running) since Tue 2019-06-25 07:07:37 UTC; 1h 11min ago Docs: http://docs.docker.com Main PID: 4011 (dockerd) Tasks: 10 Memory: 61.8M CGroup: /system.slice/docker-userns.service ‚îî‚îÄ4011 /run/torcx/bin/dockerd --host=fd:// --host=tcp://127.0.0.1:2376 --containerd=/var/run/docker/libcontainerd/docker-containerd.sock --userns-remap=default --pidfile /var/run/docker-userns.pid --selinux-enabled=true ... Now start the container like we did for the first time without any --user flag. In the following output you can see that the container started with name sad_pasteur.\n$ docker run --rm -d alpine sleep 9999 05290a7088b3e7c0e4e80cbb3a63c0d63a49627b8d31ec9f75f44b9a57b717f4 $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 05290a7088b3 alpine \u0026#34;sleep 9999\u0026#34; 2 seconds ago Up 1 second sad_pasteur Now if we see the sleep process on host, the process has started with different user 100000 on host.\n$ ps aufx | grep sleep USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND 100000 5467 0.2 0.0 1552 4 ? Ss 08:16 0:00 \\_ sleep 9999 core 5511 0.0 0.0 6864 988 pts/0 S+ 08:16 0:00 \\_ grep --colour=auto sleep But inside the container the user is still root.\n$ docker exec -it sad_pasteur id uid=0(root) gid=0(root) This is because of the user namespace enabled on the docker daemon that we see user 100000 on host. This mapping of the user id on host and inside the container can be found in the following files:\n$ cat /etc/subuid dockremap:100000:65536 $ cat /etc/subgid dockremap:100000:65536 References  Docker Lab: User Namespaces Isolate containers with a user namespace  ",
    "ref": "/post/root-in-container-root-on-host/"
  },{
    "title": "Project specific scripts",
    "date": "",
    "description": "Instead of polluting your PATH the easier way to put project specific scripts",
    "body": "There are always scripts that you write to automate some mundane tasks. And then you put that script in a directory that is in your PATH. But what this does is that it pollutes your system global PATH and shows up in places you wouldn\u0026rsquo;t want it to be in.\nI was struggling with this issue for a while and struggling to get a proper solution. But there is a very simple and clever trick to solve this problem. You extend your PATH to have a relative path .scripts in it. Like following (this is a snippet from my .bashrc).\nexport PATH=$PATH:.scripts And also add this directory to your global gitignore file. This should be done so that when you put this .scripts directory in your project the git should ignore it and not look at it as the change you would want to push to the git repository. Here is a snippet from my gitignore file.\n$ grep scripts ~/.gitignore 17:# ignore the local scripts directory 18:.scripts Now on what you can do is put the scripts you want to be available in a particular project into a directory called .scripts. Like for example:\n$ ll -a drwxrwxr-x@ - surajd 17 Jun 14:38 .scripts/ drwxrwxr-x@ - surajd 4 Jun 15:51 .terraform/ .rw-rw-r--@ 162 surajd 11 Jun 14:03 locals.tf $ ll .scripts/ .rwxrwxr-x@ 141 surajd 14 Jun 12:00 redeploy.sh* Now you can see that whenever I am in this directory redeploy.sh is available for me to run. When I press tab key the autocomplete shows up the results.\n$ red\u0026lt;tab\u0026gt; red redeploy.sh Now I just create those scripts directories and my global PATH is not polluted and it makes me happy.\nI have penned down the other aspects of scripts management in my post called: Framework for managing random scripts and binaries.\n",
    "ref": "/post/project-specific-scripts/"
  },{
    "title": "Copying files to container the generic way",
    "date": "",
    "description": "No docker cp needed to copy files from host to your container",
    "body": "This blog shows you how you can copy stuff from your host machine to the running container without the docker cp command that we usually use.\nSteps in text Here I have a script on the host, which looks following:\n#!/bin/bash  tput bold echo \u0026#34;OS Information:\u0026#34; tput sgr0 echo cat /etc/os-release After running which looks like following:\n$ ls script.sh $ ./script.sh OS Information: NAME=\u0026#34;Flatcar Linux by Kinvolk\u0026#34; ID=flatcar ID_LIKE=coreos VERSION=2079.6.0 VERSION_ID=2079.6.0 BUILD_ID=2019-06-18-0855 PRETTY_NAME=\u0026#34;Flatcar Linux by Kinvolk 2079.6.0 (Rhyolite)\u0026#34; ANSI_COLOR=\u0026#34;38;5;75\u0026#34; HOME_URL=\u0026#34;https://flatcar-linux.org/\u0026#34; BUG_REPORT_URL=\u0026#34;https://issues.flatcar-linux.org\u0026#34; FLATCAR_BOARD=\u0026#34;amd64-usr\u0026#34; And here is the running container in another tab to which I want to copy the file.\n$ docker run -it fedora bash [root@0d6d865626ff /]# Note: All the console with shell prompt [root@0d6d865626ff /]# means that the command is run inside the container.\nI can always run the docker cp command like following to copy file:\n$ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 0d6d865626ff fedora \u0026#34;bash\u0026#34; About a minute ago Up About a minute hardcore_roentgen $ docker cp script.sh hardcore_roentgen:/ Now if I check it from the container:\n[root@0d6d865626ff /]# ls / | grep script script.sh Let\u0026rsquo;s clean up the file and use another easier and container runtime agnostic method to do the same.\n[root@0d6d865626ff /]# rm script.sh rm: remove regular file \u0026#39;script.sh\u0026#39;? y [root@0d6d865626ff /]# ls / bin boot dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr var To figure out the file system root of the container process let\u0026rsquo;s start a sleep process.\n[root@0d6d865626ff /]# sleep 3000 Now the sleep process will help us identify the PID of this process on the host.\n$ ps aufx | grep sleep USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 1668 0.0 0.0 5252 692 pts/0 S+ 16:47 0:00 \\_ sleep 3000 Since we figured out the PID of the process to be 1668, now let\u0026rsquo;s copy the file to the root of the container by running following command. Down here I have used the PID that I got output of above, replace it with the PID you get for your process.\n$ sudo cp -v ./script.sh /proc/1668/root/ \u0026#39;./script.sh\u0026#39; -\u0026gt; \u0026#39;/proc/1668/root/script.sh\u0026#39; Now we can stop the sleep process we started earlier in the container see in the root that the file has been copied successfully.\n[root@0d6d865626ff /]# sleep 3000 ^C [root@0d6d865626ff /]# ls / | grep script script.sh  Video of above steps    I learnt this trick while pair coding/learning things about container security with my colleague Alban, thanks Alban!.\n",
    "ref": "/post/copy-to-container-without-docker-cp/"
  },{
    "title": "Writing your own Seccomp profiles for Docker",
    "date": "",
    "description": "Understanding the seccomp profile json format",
    "body": "What is Seccomp?  A large number of system calls are exposed to every userland process with many of them going unused for the entire lifetime of the process. A certain subset of userland applications benefit by having a reduced set of available system calls. The resulting set reduces the total kernel surface exposed to the application. System call filtering is meant for use with those applications. Seccomp filtering provides a means for a process to specify a filter for incoming system calls.\n source: Kernel Docs\nSeccomp with Docker Seccomp profile is attached with docker container by default. But understanding the profile can be hard if you are new to it.\nHere is the snippet of syscalls allowed from the default profile:\n{ \u0026#34;names\u0026#34;: [ \u0026#34;bpf\u0026#34;, \u0026#34;clone\u0026#34;, \u0026#34;fanotify_init\u0026#34;, \u0026#34;lookup_dcookie\u0026#34;, \u0026#34;mount\u0026#34;, \u0026#34;name_to_handle_at\u0026#34;, \u0026#34;perf_event_open\u0026#34;, \u0026#34;quotactl\u0026#34;, \u0026#34;setdomainname\u0026#34;, \u0026#34;sethostname\u0026#34;, \u0026#34;setns\u0026#34;, \u0026#34;syslog\u0026#34;, \u0026#34;umount\u0026#34;, \u0026#34;umount2\u0026#34;, \u0026#34;unshare\u0026#34; ], \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;includes\u0026#34;: { \u0026#34;caps\u0026#34;: [ \u0026#34;CAP_SYS_ADMIN\u0026#34; ] }, \u0026#34;excludes\u0026#34;: {} }, Here the syscalls mentioned in the names list are allowed for container only if the container starting has the capability CAP_SYS_ADMIN included when starting it, using the flag --cap-add=SYS_ADMIN.\nExperiment I have done my own experiment where I am tying the chmod syscall to the capability CAP_WAKE_ALARM (There is no serious thinking put behind tying chmod to this capability CAP_WAKE_ALARM, I chose it because this capability did not seem to be doing much important hence I picked it up).\nTo try this out I have created a config of my own by changing the default config which looks like following diff:\n--- default.json 2019-06-10 14:23:11.688170627 +0530 +++ chmod-wake-alarm.json 2019-06-10 14:19:25.706274172 +0530 @@ -62,7 +62,6 @@  \u0026#34;capget\u0026#34;, \u0026#34;capset\u0026#34;, \u0026#34;chdir\u0026#34;, - \u0026#34;chmod\u0026#34;,  \u0026#34;chown\u0026#34;, \u0026#34;chown32\u0026#34;, \u0026#34;clock_getres\u0026#34;, @@ -94,8 +93,6 @@  \u0026#34;fallocate\u0026#34;, \u0026#34;fanotify_mark\u0026#34;, \u0026#34;fchdir\u0026#34;, - \u0026#34;fchmod\u0026#34;, - \u0026#34;fchmodat\u0026#34;,  \u0026#34;fchown\u0026#34;, \u0026#34;fchown32\u0026#34;, \u0026#34;fchownat\u0026#34;, @@ -381,6 +378,22 @@  }, { \u0026#34;names\u0026#34;: [ + \u0026#34;chmod\u0026#34;, + \u0026#34;fchmod\u0026#34;, + \u0026#34;fchmodat\u0026#34; + ], + \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, + \u0026#34;args\u0026#34;: [], + \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, + \u0026#34;includes\u0026#34;: { + \u0026#34;caps\u0026#34;: [ + \u0026#34;CAP_WAKE_ALARM\u0026#34; + ] + }, + \u0026#34;excludes\u0026#34;: {} + }, + { + \u0026#34;names\u0026#34;: [  \u0026#34;personality\u0026#34; ], \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, @@ -791,4 +804,4 @@  \u0026#34;excludes\u0026#34;: {} } ] -} \\ No newline at end of file +} To understand what has changed I first removed all the references to the chmod and then added following snippet to tie the chmod to CAP_WAKE_ALARM.\n{ \u0026#34;names\u0026#34;: [ \u0026#34;chmod\u0026#34;, \u0026#34;fchmod\u0026#34;, \u0026#34;fchmodat\u0026#34; ], \u0026#34;action\u0026#34;: \u0026#34;SCMP_ACT_ALLOW\u0026#34;, \u0026#34;args\u0026#34;: [], \u0026#34;comment\u0026#34;: \u0026#34;\u0026#34;, \u0026#34;includes\u0026#34;: { \u0026#34;caps\u0026#34;: [ \u0026#34;CAP_WAKE_ALARM\u0026#34; ] }, \u0026#34;excludes\u0026#34;: {} }, Now let\u0026rsquo;s try this if it works:\n$ docker container run --rm -it --security-opt seccomp=chmod-wake-alarm.json alpine sh / # touch foo.sh / # chmod +x foo.sh chmod: foo.sh: Operation not permitted If you see with the newly created profile the container did not allow chmod to run.\n$ docker container run --cap-add=WAKE_ALARM --rm -it --security-opt seccomp=chmod-wake-alarm.json alpine sh / # touch foo.sh / # chmod +x foo.sh But in the above command I explicitly permitted this container to run with capability CAP_WAKE_ALARM, and now the container allows chmod. In this way you can create your own profile and tie it up with any capability you want.\nReference  Docker Seccomp Configs used in this experiment Capabilities man page What is seccomp? Seccomp docker labs  ",
    "ref": "/post/docker-seccomp-manual/"
  },{
    "title": "Suraj Deshmukh's talks at conferences",
    "date": "",
    "description": "List of all the talks presented by me",
    "body": " Hardening Kubernetes by Securing Pods - Rootconf 2019    State of Kubernetes Meetups - DevOpsDays India 2017    Making Kubernetes Simple For Developers - Rootconf 2017    Taking docker-compose to Production - Gophercon 2017 Lightening talk Watch from 55m59s\n  ",
    "ref": "/post/surajd-talks-links/"
  },{
    "title": "Kubernetes Bangalore March 2019 Event Report",
    "date": "",
    "description": "Event Report for Kubernetes Bangalore Meetup",
    "body": "The Kubernetes Bangalore Meetup was organized at Arvind Internet on Feb 16th 2019. The agenda for the meetup was to teach Kubernetes to the beginners.\nMeetup agenda can be found here.\nThe moments from Meetup:\nWe go online in sometime here https://t.co/FkwgOx0Tm4\n\u0026mdash; Kubernetes Bangalore (@k8sBLR) March 16, 2019  .@pmishra1598 kick started the Meetup by explaining what #Kubernetes is! Currently clarifying what a pod is. pic.twitter.com/Ny7bN9c62x\n\u0026mdash; Kubernetes Bangalore (@k8sBLR) March 16, 2019  Huge turnout at today\u0026#39;s meetup it\u0026#39;s on üî•üî• pic.twitter.com/YYbMBoumWw\n\u0026mdash; Kubernetes Bangalore (@k8sBLR) March 16, 2019  Deploying your containers on kubernetes by @hrishike8. #kubernetes pic.twitter.com/JgLaNGVgRu\n\u0026mdash; Kubernetes Bangalore (@k8sBLR) March 16, 2019  Create ingress routing session by Deepak Kumar. The live video stream is up at: https://t.co/FkwgOxiuKE #kubernetes #networking pic.twitter.com/qoxjzbdldg\n\u0026mdash; Kubernetes Bangalore (@k8sBLR) March 16, 2019  @dolftax taking us through Kubernetes networking. Watch him live at: https://t.co/FkwgOxiuKE#kubernetes #networking pic.twitter.com/1XXLWxNYTy\n\u0026mdash; Kubernetes Bangalore (@k8sBLR) March 16, 2019  Here is the link to meetup recording:\n  Photos from the meetup can be found here.\nSee you in the next meetup.\nAlso this event report was auto-generated using the project blog-generator, all kinds of contribution are most welcome.\n",
    "ref": "/post/k8s-blr-event-report-2019-03-16/"
  },{
    "title": "Make static configs available for apiserver in minikube",
    "date": "",
    "description": "Dealing with apiserver in minikube can be tricky",
    "body": "If you want to provide extra flags to the kube-apiserver that runs inside minikube how do you do it? You can use the minikube\u0026rsquo;s --extra-config flag with apiserver.\u0026lt;apiserver flag\u0026gt;=\u0026lt;value\u0026gt;, for e.g. if you want to enable RBAC authorization mode you do it as follows:\n--extra-config=apiserver.authorization-mode=RBAC So this is a no brainer when doing it for flags whose value can be given right away, like the one above. But what if you want to provide value which is a file path. Because you will have to make that file available for apiserver. And this apiserver runs as a static pod inside minikube. How do you make the file available to that process inside pod inside minikube?\nThe solution is little tricky and not very straight forward. The api-server pod mounts minikube\u0026rsquo;s /var/lib/minikube/certs/ directory in the pod at location /var/lib/minikube/certs/. Make the file available at this location. When enabling that flag for apiserver provide file location of this directory.\nTo make this step easier I have filed an issue in minikube upstream kubernetes/minikube/3559.\nFollow this tutorial on how to do this. In this tutorial I want to make the EncryptionConfiguration file available for apiserver to enable encryption of secret data at rest. This is the first step to the tasks from kubernetes docs as mentioned here.\n Start minikube normally To make the file needed available in the machine start minikube normally. For that run following command:\nminikube start \\ --vm-driver kvm2 \\ --kubernetes-version v1.13.2 \\ --cpus 3 --memory 3000 \\ --extra-config=apiserver.authorization-mode=RBAC \\ --v 10 You can make required changes to the above commmand lke change the --vm-driver or --cpus or --memory, as per your needs.\nMake file available inside minikube Run following command to go into machine\nminikube ssh Once inside machine become root by running sudo -i. And then create the config file needed that will be passed to the apiserver. For my needs I wanted to create a EncryptionConfiguration.\nRun following command to make the config file available.\necho \u0026#34; kind: EncryptionConfiguration apiVersion: apiserver.config.k8s.io/v1 resources: - resources: - secrets providers: - aescbc: keys: - name: key1 secret: fPLrjJNkbuLmh2aqOsCR5sZV+/Wqhi8CdMrgceaKR3E= - identity: {} \u0026#34; | tee /var/lib/minikube/certs/encryptionconfig.yaml See the location of the file it is in /var/lib/minikube/certs. In above command you can change it to the config you would want to make available for apiserver.\nRestart minikube Exit out of the minikube vm and get to your host machine and run following:\nminikube stop minikube start \\ --vm-driver kvm2 \\ --kubernetes-version v1.13.2 \\ --cpus 3 --memory 3000 \\ --extra-config=apiserver.authorization-mode=RBAC \\ --extra-config=apiserver.encryption-provider-config=/var/lib/minikube/certs/encryptionconfig.yaml \\ --v 10 Again make changes to the apiserver flag and file name if needed according to your needs. Now you should have apiserver started without problems.\n",
    "ref": "/post/apiserver-in-minikube-static-configs/"
  },{
    "title": "Recreate Kubernetes CVE-2017-1002101",
    "date": "",
    "description": "Subpath Volume Mount could give you access to node",
    "body": "A volume mount CVE was discovered in Kubernetes 1.9 and older which allowed access to node file system using emptyDir volume mount using subpath. The official description goes as follows:\n In Kubernetes versions 1.3.x, 1.4.x, 1.5.x, 1.6.x and prior to versions 1.7.14, 1.8.9 and 1.9.4 containers using subpath volume mounts with any volume type (including non-privileged pods, subject to file permissions) can access files/directories outside of the volume, including the host\u0026rsquo;s filesystem.\n Source: https://nvd.nist.gov/vuln/detail/CVE-2017-1002101\nTo understand the problem and re-create the CVE follow the steps in following video:\nTo create an older Kubernetes setup run following. You need to start older cluster on minikube(version of minikube does not matter) and need older kubectl for compatiblility reasons.\ncurl -LO https://storage.googleapis.com/kubernetes-release/release/v1.9.0/bin/linux/amd64/kubectl chmod +x ./kubectl minikube start \\ --vm-driver kvm2 \\ --kubernetes-version v1.9.0 \\ --cpus 3 --memory 3000 \\ --extra-config=apiserver.authorization-mode=RBAC Verify the cluster version:\n$ ./kubectl get nodes NAME STATUS ROLES AGE VERSION minikube Ready master 9m v1.9.0 This is the pod manifest used in the video:\napiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: testpod name: testpod spec: initContainers: - name: init image: busybox command: [\u0026#34;ln\u0026#34;, \u0026#34;-s\u0026#34;, \u0026#34;/\u0026#34;, \u0026#34;/mnt/my-volume/data1\u0026#34;] volumeMounts: - name: my-volume mountPath: /mnt/my-volume containers: - image: busybox imagePullPolicy: IfNotPresent name: testpod command: [\u0026#34;sleep\u0026#34;, \u0026#34;999999\u0026#34;] volumeMounts: - name: my-volume mountPath: /mnt/my-volume - name: my-volume mountPath: /mnt/data subPath: data1 volumes: - name: my-volume emptyDir: medium: Memory dnsPolicy: ClusterFirst With above setup I could re-create the CVE successfully.\nWhen tried on the v1.13.2 cluster the pod goes into Pending phase with status being CreateContainerConfigError and also lists the message as failed to prepare subPath for volumeMount \u0026quot;my-volume\u0026quot; of container \u0026quot;testpod\u0026quot;.\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES testpod 0/1 CreateContainerConfigError 0 24m 10.38.0.2 w1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; ",
    "ref": "/post/cve-2017-1002101-subpath-volume-mount-recreate/"
  },{
    "title": "Cobra and Persistentflags gotchas",
    "date": "",
    "description": "How wrong usage of persistent flags can burn you",
    "body": "If you are using cobra cmd line library for golang applications and it\u0026rsquo;s PersistentFlags and if you have a use case where you are adding same kind of flag in multiple places. You might burn your fingers in that case, if you keep adding it in multiple sub-commands without giving it a second thought. To understand what is really happening and why it is happening follow along.\nAll the code referenced here can be found here https://github.com/surajssd/cobrademo.\nThe cmd line tool built with cobra has following structure. The main tool is called cobrademo. And the sub-commands are alpha and num. Sub-command num has one more sub-command called one.\ncobrademo ‚îú‚îÄ‚îÄ alpha ‚îî‚îÄ‚îÄ num ‚îî‚îÄ‚îÄ one Now I want a persistent flag --config to be availabe under sub-command one and alpha both. So I created a func that allows me to add this flag under any command, which looked like following:\nfunc addConfig(cmd *cobra.Command) { // add config flag \tcmd.PersistentFlags().String( \u0026#34;config\u0026#34;, os.ExpandEnv(\u0026#34;$HOME/.config\u0026#34;), \u0026#34;Path to config file\u0026#34;) viper.BindPFlag(\u0026#34;config\u0026#34;, cmd.PersistentFlags().Lookup(\u0026#34;config\u0026#34;)) } Above code is here.\nNow this is called from one.go and alpha.go to add the flag under those sub-command.\nNow the command structure for alpha sub-command looks like following:\n$ go run main.go alpha -h All the alphabet related commands Usage: cobrademo alpha [flags] Flags: --config string Path to config file (default \u0026#34;/home/hummer/.config\u0026#34;) -h, --help help for alpha For sub-command one it looks like following:\n$ go run main.go num -h All the numeric related commands Usage: cobrademo num [command] Available Commands: one first subcommand in numerics Flags: -h, --help help for num Use \u0026#34;cobrademo num [command] --help\u0026#34; for more information about a command. $ go run main.go num one -h first subcommand in numerics Usage: cobrademo num one [flags] Flags: --config string Path to config file (default \u0026#34;/home/hummer/.config\u0026#34;) -h, --help help for one But if you look at the functionality it does not work as expected.\n$ go run main.go num one --config=foobar inside one, config value: foobar $ go run main.go alpha --config=foobar inside alpha, config value: /home/hummer/.config If you see the output of both the commands it is different. While it should have been same i.e. foobar. What made it work in case of sub-command one and it did not work in case of sub-command alpha?\nNow we are registering a persistent flag twice once for sub-command one and again for alpha. And these calls happen from the init func of those files. If you look at the order of the evaluation of those init functions then it happens in alphabetical order.\n$ tree cmd/ cmd/ ‚îú‚îÄ‚îÄ alpha.go ‚îú‚îÄ‚îÄ num.go ‚îú‚îÄ‚îÄ one.go ‚îî‚îÄ‚îÄ root.go Hence the init func of alpha is called first and the flag config is registered there first and again it is registered for one. So the final flag is just registered for one. Hence the functionality works correctly for one and not for alpha.\nSo the right way to work with persistent flags is to register them only once. If any particular sub-command tree needs that flag then only register at it\u0026rsquo;s root. In our case most of the sub-commands will need it, so the right way to use it is to add it to the rootCmd.\nIn above code I removed the function addConfig and all it\u0026rsquo;s references(see the changes here). And added following code snippet to the init func of root.go.\n// add config flag \trootCmd.PersistentFlags().String( \u0026#34;config\u0026#34;, os.ExpandEnv(\u0026#34;$HOME/.config\u0026#34;), \u0026#34;Path to config file\u0026#34;) viper.BindPFlag(\u0026#34;config\u0026#34;, rootCmd.PersistentFlags().Lookup(\u0026#34;config\u0026#34;)) And now after running the code again with above changes it works absolutely fine:\n$ go run main.go num one --config=foobar inside one, config value: foobar $ go run main.go alpha --config=foobar inside alpha, config value: foobar There are other cobra gotchas that exist but then that is for another post.\n",
    "ref": "/post/cobra-persistent-flag/"
  },{
    "title": "Old laptop setup reference",
    "date": "",
    "description": "Links and things to do while setting up older Dell Inspiron 1525",
    "body": "I have this old PC Dell Inspiron 1525 with 2GB RAM and 32 bit dual core processor and I wanted to install fedora on it, but I cam accross few problems which I am documenting for further reference.\nWifi device not detected The wifi drivers are not loaded by default, so followed this blog, basically do following:\nexport FIRMWARE_INSTALL_DIR=\u0026quot;/lib/firmware\u0026quot; wget http://mirror2.openwrt.org/sources/broadcom-wl-5.100.138.tar.bz2 tar xjf broadcom-wl-5.100.138.tar.bz2 cd broadcom-wl-5.100.138/linux/ sudo b43-fwcutter -w /lib/firmware wl_apsta.o Slow boot problem Since the boot process is very slow so add this line to /etc/default/grub\nGRUB_CMDLINE_LINUX_DEFAULT=\u0026quot;quiet splash video=SVIDEO-1:d\u0026quot; Update grub using\ngrub2-mkconfig -o /boot/grub2/grub.cfg Continual reporting of issues Disabled SELinux and it stopped reporting those issues. For which just change the line from SELinux=enforcing to SELinux=disabled in /etc/sysconfig/selinux. Need to figure out why those errors were reported in a loop.\nRef:\n Update Grub Boot very slow because of drm_kms_helper errors Firmware file \u0026ldquo;b43/ucode15.fw\u0026rdquo; not found How to Disable SELinux Temporarily or Permanently in RHEL/CentOS 7/6  ",
    "ref": "/post/old-laptop-setup/"
  },{
    "title": "Add new Node to k8s cluster with Bootstrap token",
    "date": "",
    "description": "Use this technique to add new node to the cluster without providing any certificates and without having to restart the kube-apiserver",
    "body": " NOTE: There is an updated version of this blog here.\n Few days back I wrote a blog about adding new node to the cluster using the static token file. The problem with that approach is that you need to restart kube-apiserver providing it the path to the token file. Here we will see how to use the bootstrap token, which is very dynamic in nature and can be controlled by using Kubernetes resources like secrets.\nSo if you are following Kubernetes the Hard Way to set up the cluster here are the changes you should do to adapt it to run with bootstrap token.\nMaster Node changes kube-apiserver Add this flag --enable-bootstrap-token-auth=true to your kube-apiserver service file. In the end your service file should look like following:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\\\ --advertise-address=${INTERNAL_IP} \\\\ --allow-privileged=true \\\\ --apiserver-count=3 \\\\ --audit-log-maxage=30 \\\\ --audit-log-maxbackup=3 \\\\ --audit-log-maxsize=100 \\\\ --audit-log-path=/var/log/audit.log \\\\ --authorization-mode=Node,RBAC \\\\ --bind-address=0.0.0.0 \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\\\ --enable-bootstrap-token-auth=true \\\\ --enable-swagger-ui=true \\\\ --etcd-cafile=/var/lib/kubernetes/ca.pem \\\\ --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\\\ --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\\\ --etcd-servers=https://192.168.50.10:2379 \\\\ --event-ttl=1h \\\\ --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\\\ --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\\\ --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\\\ --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\\\ --kubelet-https=true \\\\ --runtime-config=api/all \\\\ --service-account-key-file=/var/lib/kubernetes/service-account.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --service-node-port-range=30000-32767 \\\\ --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\\\ --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF kube-controller-manager Add following flag --controllers=*,bootstrapsigner,tokencleaner to the controller manager service file. So service file should look like following:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kube-controller-manager.service [Unit] Description=Kubernetes Controller Manager Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-controller-manager \\\\ --address=0.0.0.0 \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --cluster-name=kubernetes \\\\ --cluster-signing-cert-file=/var/lib/kubernetes/ca.pem \\\\ --cluster-signing-key-file=/var/lib/kubernetes/ca-key.pem \\\\ --controllers=*,bootstrapsigner,tokencleaner \\\\ --kubeconfig=/var/lib/kubernetes/kube-controller-manager.kubeconfig \\\\ --leader-elect=true \\\\ --root-ca-file=/var/lib/kubernetes/ca.pem \\\\ --service-account-private-key-file=/var/lib/kubernetes/service-account-key.pem \\\\ --service-cluster-ip-range=10.32.0.0/24 \\\\ --use-service-account-credentials=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Bootstrap secret The bootstrap token that we are using to setup is 07401b.f395accd246ae52d(You can generate one of yourself). This token has two parts 07401b which is public id and private part f395accd246ae52d a secret. Read more about the token, what is allowed and what it should look like here. And about the bootstrap token secret format here.\nCreate following secret which has certain requirements. The name of the secret should of the format bootstrap-token-\u0026lt;token public id\u0026gt; and should be available in kube-system namespace.\ncat \u0026lt;\u0026lt;EOF | kubectl apply --kubeconfig admin.kubeconfig -f - apiVersion: v1 kind: Secret metadata: # Name MUST be of form \u0026quot;bootstrap-token-\u0026lt;token id\u0026gt;\u0026quot; name: bootstrap-token-07401b namespace: kube-system # Type MUST be 'bootstrap.kubernetes.io/token' type: bootstrap.kubernetes.io/token stringData: # Human readable description. Optional. description: \u0026quot;Created for Kubernetes the Hard Way\u0026quot; # Token ID and secret. Required. token-id: 07401b token-secret: f395accd246ae52d # Allowed usages. usage-bootstrap-authentication: \u0026quot;true\u0026quot; usage-bootstrap-signing: \u0026quot;true\u0026quot; EOF RBAC policies to enable bootstrapping The user authenticated by that token belongs to the group system:bootstrappers, that is why following permissions are given to that group.\nkubectl create clusterrolebinding kubelet-bootstrap \\  --clusterrole=system:node-bootstrapper \\  --group=system:bootstrappers kubectl create clusterrolebinding node-autoapprove-bootstrap \\  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \\  --group=system:bootstrappers kubectl create clusterrolebinding node-autoapprove-certificate-rotation \\  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \\  --group=system:nodes Worker node changes Kubelet Add this flag --bootstrap-kubeconfig, it is a path to kubeconfig which we will generate shortly, it contains bootstrap token and information to talk to the kube-apiserver. And also you don\u0026rsquo;t need to provide --kubeconfig but provide a path to it, this will be auto-generated and saved at that path.\nYour kubelet service file should look like following:\ncat \u0026lt;\u0026lt;EOF | sudo tee /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --bootstrap-kubeconfig=/home/vagrant/bootstrap.kubeconfig \\\\ --config=/var/lib/kubelet/kubelet-config.yaml \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\\\ --image-pull-progress-deadline=2m \\\\ --kubeconfig=/home/vagrant/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Now changes in kubelet configuration file\ncat \u0026lt;\u0026lt;EOF | sudo tee /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \u0026quot;/home/vagrant/ca.pem\u0026quot; authorization: mode: Webhook clusterDomain: \u0026quot;cluster.local\u0026quot; clusterDNS: - \u0026quot;10.32.0.10\u0026quot; podCIDR: \u0026quot;${POD_CIDR}\u0026quot; rotateCertificates: true runtimeRequestTimeout: \u0026quot;15m\u0026quot; serverTLSBootstrap: true EOF Provide appropriate path to clientCAFile. And add two more fields rotateCertificates: true \u0026amp; serverTLSBootstrap: true, this will enable cert rotation.\nCreate bootstrap kubeconfig # I have used the ip address of my kube kube-apiserver use yours kubectl config set-cluster kthwkinvolk \\  --certificate-authority=ca.pem \\  --embed-certs=true \\  --server=https://192.168.50.10:6443 \\  --kubeconfig=/home/vagrant/bootstrap.kubeconfig # this token is above generated kubectl config set-credentials kubelet-bootstrap \\  --token=07401b.f395accd246ae52d \\  --kubeconfig=/home/vagrant/bootstrap.kubeconfig kubectl config set-context default \\  --cluster=kthwkinvolk \\  --user=kubelet-bootstrap \\  --kubeconfig=/home/vagrant/bootstrap.kubeconfig kubectl config use-context default \\  --kubeconfig=/home/vagrant/bootstrap.kubeconfig This is the bootstrap config file which we referred earlier in kubelet service file.\nNow once you start kubelet service it will use the bootstrap token in the initial request and fetch the certificates.\nTrying it all in one go Here is the link to the gist where you can start a master and a node using Vagrant and create cluster using above setup.\nHope that helps. Happy Hacking.\n",
    "ref": "/post/add-new-k8s-node-bootstrap-token/"
  },{
    "title": "PodSecurityPolicy on existing Kubernetes clusters",
    "date": "",
    "description": "Burnt by enabling PSPs on existing Kubernetes and wondering why everything still works",
    "body": "I enabled PodSecurityPolicy on a minikube cluster by appending PodSecurityPolicy to the apiserver flag in minikube like this:\n--extra-config=apiserver.enable-admission-plugins=Initializers,NamespaceLifecycle,\\  LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,\\  NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,\\  ResourceQuota,PodSecurityPolicy Ideally when you have PSP enabled and if you don\u0026rsquo;t define any PSP and authorize it with right RBAC no pod will start in the cluster. But what I saw was that there were some pods still running in kube-system namespace.\n$ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-g2t8v 1/1 Running 4 5d11h etcd-minikube 1/1 Running 2 5d11h heapster-bn5xp 1/1 Running 2 5d11h influxdb-grafana-qzpv4 2/2 Running 4 5d11h kube-addon-manager-minikube 1/1 Running 2 5d11h kube-controller-manager-minikube 1/1 Running 1 4d20h kube-scheduler-minikube 1/1 Running 2 5d11h kubernetes-dashboard-5bb6f7c8c6-9d564 1/1 Running 8 5d11h storage-provisioner 1/1 Running 7 5d11h Which got me thinking what is wrong with the way PSPs work. So if you look closely only two pods are scheduled by a deployment.\n$ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE coredns 1 1 1 1 5d11h kubernetes-dashboard 1 1 1 0 5d11h These pods were already scheduled and worked fine before, Kubernetes was just trying to restart them. Now when I deleted them\n$ kubectl delete pod kubernetes-dashboard-5bb6f7c8c6-9d564 pod \u0026#34;kubernetes-dashboard-5bb6f7c8c6-9d564\u0026#34; deleted $ kubectl delete pod coredns-576cbf47c7-g2t8v pod \u0026#34;coredns-576cbf47c7-g2t8v\u0026#34; deleted Those pods won\u0026rsquo;t come up anymore\n$ kubectl get deploy NAME DESIRED CURRENT UP-TO-DATE AVAILABLE AGE coredns 1 0 0 0 5d11h kubernetes-dashboard 1 0 0 0 5d11h And the reason they won\u0026rsquo;t come up.\n$ kubectl get deploy coredns -o jsonpath=\u0026#39;{.status.conditions[-1:].message}\u0026#39; pods \u0026#34;coredns-576cbf47c7-\u0026#34; is forbidden: no providers available to validate pod request So to fix this I will have to define a PSP and then bind it to the service account that these controllers use.\nUsing PSP; if you enable it on a running cluster and if it had some workloads that were running already they won\u0026rsquo;t be affected. But once a pod is deleted and is up for re-scheduling again it will fail with something like above.\nSo to enable it on the running cluster here is what the Kubernetes documentation says:\n Since the pod security policy API (policy/v1beta1/podsecuritypolicy) is enabled independently of the admission controller, for existing clusters it is recommended that policies are added and authorized before enabling the admission controller.\n ",
    "ref": "/post/psp-on-existing-cluster/"
  },{
    "title": "Road to CKA",
    "date": "",
    "description": "My experience with CKA exam preparation",
    "body": "I passed CKA exam with 92% marks on 19th October 2018.\nA lot of folks are curious about how to prepare and what resources to follow. Here is my list of things to do and list of resources that might help you on successful CKA exam.\nThe duration of exam is three hours, which is enough time if you do good practice. The exam is pretty straight forward and tests your Kubernetes hands-on knowledge, so whatever you read please try to do it on a real cluster.\nYou have access to Kubernetes docs during the exam so make sure you go through Kubernetes documentation thoroughly. Once you are familiar with the documentation you will know where to look at if you need to refer something.\nNote: I have not read any book on Kubernetes or followed any professional training. Other people do recommend reading books which might be helpful but again do follow those books/courses with more preference to hands-on practice.\nYour Kubernetes cluster For me the go-to place for a cluster has always been either Katacoda\u0026rsquo;s Kubernetes Playground or my local machine with minikube installed on it. This sufficed most of my use cases but if you want to have a multi-node setup with networking and everything kubeadm is a quick way to setup a new cluster.\nThings you must do hands-on   Kubernetes the hard way by Kelsey Hightower. This tutorial setup is on GCP, but if you want you can also do it locally by setting up machines using Vagrant or something similar. I personally always did this setup on my local machine using Vagrant(The explanation about my setup can go in another post though).\n  Kubernetes Tasks from docs, just don\u0026rsquo;t blindly do the tasks. Most tasks have link to the concepts of what is being done. Follow those links and read up.\n  Things you must read  Kubernetes Concepts Kubernetes Reference Basics of systemd, like starting, restarting and enabling the service. And how to read the systemd service file.  Other resources   You should focus on imperative style of using Kubernetes; for that thoroughly read up and practice following posts:\n Overview of kubectl Managing Resources Managing Kubernetes Objects Using Imperative Commands kubectl Cheat Sheet    This github repo by Walid Shaari is a really good source of finding right resources to read according to the syllabus distribution.\n  Learn to generate resouces on the fly with kubectl create .... This will help you in not having to write entire kubernetes resouce yaml file by hand, which can be tedious and erroneous task. Whenever you are running those commands and are confused about it\u0026rsquo;s usage kubectl create -h gives you examples of various usages of the command.\n  You don\u0026rsquo;t need to remember the location of systemd service files, the trick I use to find the location of service file is just try to see the status of service by running systemctl status \u0026lt;service name\u0026gt; and in there you will see the location to service file that is loaded.\n  During exam   Make yourself familiar with tmux, this helps if you like having split screen while working.\n  Having a decent sized monitor for exam can help if you have problem with small font size and having to fit everything in one small laptop window, but then this will also imply that you will need external webcam.\n  The exam is supported on Chrome only and if you are used to using the shortcut on terminal to delete word Ctrl + W, which also happens to close the chrome window tab then you might want to remove that shortcut. Here is what I did on my system to disable the shortcut during the exam.\n  The first thing to do after exam starts, enable kubectl auto-completion in exam terminal by running source \u0026lt;(kubectl completion bash).\n   With all that said, I will try to keep this document alive by adding things if I remember any, otherwise always feel free to reach out to me on twitter @surajd_. Also join the Kubernauts community where people are really helpful with your queries.\n",
    "ref": "/post/road-to-cka/"
  },{
    "title": "How to disable your Chrome Ctrl + W keybinding",
    "date": "",
    "description": "Learn how to disable the shortcut Ctrl + W altogether on your GNOME",
    "body": "I am about to attempt the CKA exam and it has a browser based terminal. And I am used to this terminal shortcut Ctrl + W which deletes a word. But the same shortcut in browser can close a tab. Since this exam is combination of both I am afraid I might close my exam tab while deleting a word in terminal. Now the only solution to this is disabling the shortcut in chrome. But turns out that the shortcut is hard coded in chrome.\nI use Fedora Linux with GNOME as my desktop environment. So the alternative to this problem is you just add this keybinding as a no operation in GNOME. So I am going to add this as a no-op till the exam and remove this once I am done with the exam. So here are the steps to do it.\nSteps to disable \u0026ldquo;Ctrl + W\u0026rdquo; Open Keyboard in your Settings, you can just type in the GNOME search.\nOnce you open Keyboard you can see bunch of shortcuts listed there.\nGoto the bottom of it and click on the plus button.\nNow you can add a custom shortcut here, Name it something so that you remember that you want to remove it later and in Command put some no-op thing. After that click on Set Shortcut to provide key binding.\nType key combination of Ctrl + W here\nYour custom shortcut should look like this, and click the Add button.\nThat\u0026rsquo;s how you will have your Ctrl + W disabled.\n",
    "ref": "/post/disable-ctrl-w/"
  },{
    "title": "Add new Node to k8s cluster with cert rotation",
    "date": "",
    "description": "Use this technique to add node to the cluster without providing any certificates",
    "body": "The setup here is created by following Kubernetes the Hard Way by Kelsey Hightower. So if you are following along in this then do all the setup till the step Bootstrapping the Kubernetes Worker Nodes. In this just don\u0026rsquo;t start the kubelet, start other services like containerd and kube-proxy.\nmaster node Following the docs of TLS Bootstrapping, let\u0026rsquo;s first create the token authentication file. Create a file with following content:\n$ cat tokenfile 02b50b05283e98dd0fd71db496ef01e8,kubelet-bootstrap,10001,\u0026#34;system:bootstrappers\u0026#34; You should create the token which is as random as possible by running following command:\nhead -c 16 /dev/urandom | od -An -t x | tr -d \u0026#39; \u0026#39; Now we need to tell the kube apiserver about this file. So add following flag to the kube-apiserver service file, with the path to above token file.\n--token-auth-file=/home/vagrant/tokenfile So my kube-apiserver service file looks like following:\n$ cat /etc/systemd/system/kube-apiserver.service [Unit] Description=Kubernetes API Server Documentation=https://github.com/kubernetes/kubernetes [Service] ExecStart=/usr/local/bin/kube-apiserver \\  --advertise-address=192.168.50.10 \\  --allow-privileged=true \\  --apiserver-count=3 \\  --audit-log-maxage=30 \\  --audit-log-maxbackup=3 \\  --audit-log-maxsize=100 \\  --audit-log-path=/var/log/audit.log \\  --authorization-mode=Node,RBAC \\  --bind-address=0.0.0.0 \\  --client-ca-file=/var/lib/kubernetes/ca.pem \\  --enable-admission-plugins=Initializers,NamespaceLifecycle,NodeRestriction,LimitRanger,ServiceAccount,DefaultStorageClass,ResourceQuota \\  --enable-swagger-ui=true \\  --etcd-cafile=/var/lib/kubernetes/ca.pem \\  --etcd-certfile=/var/lib/kubernetes/kubernetes.pem \\  --etcd-keyfile=/var/lib/kubernetes/kubernetes-key.pem \\  --etcd-servers=https://192.168.50.10:2379 \\  --event-ttl=1h \\  --experimental-encryption-provider-config=/var/lib/kubernetes/encryption-config.yaml \\  --kubelet-certificate-authority=/var/lib/kubernetes/ca.pem \\  --kubelet-client-certificate=/var/lib/kubernetes/kubernetes.pem \\  --kubelet-client-key=/var/lib/kubernetes/kubernetes-key.pem \\  --kubelet-https=true \\  --runtime-config=api/all \\  --service-account-key-file=/var/lib/kubernetes/service-account.pem \\  --service-cluster-ip-range=10.32.0.0/24 \\  --service-node-port-range=30000-32767 \\  --tls-cert-file=/var/lib/kubernetes/kubernetes.pem \\  --tls-private-key-file=/var/lib/kubernetes/kubernetes-key.pem \\  --token-auth-file=/home/vagrant/tokenfile \\  --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target Once this is done, just run following commands to restart the apiserver:\nsudo systemctl daemon-reload sudo systemctl restart kube-apiserver sudo systemctl status kube-apiserver Now that api-server has restart we need to give permissions so that our worker node can ask for certs automatically, for that run following commands:\nkubectl create clusterrolebinding kubelet-bootstrap \\  --clusterrole=system:node-bootstrapper \\  --user=kubelet-bootstrap kubectl create clusterrolebinding node-client-auto-approve-csr \\  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient \\  --group=system:node-bootstrappers kubectl create clusterrolebinding node-client-auto-renew-crt \\  --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient \\  --group=system:nodes worker node First create a bootstrap kubeconfig file that will be used by kubelet. Run following commands to create it.\n# I have used the ip address of my api-server use yours kubectl config set-cluster kthw \\ --certificate-authority=ca.pem \\ --embed-certs=true \\ --server=https://192.168.50.10:6443 \\ --kubeconfig=bootstrap.kubeconfig # this token is above generated kubectl config set-credentials kubelet-bootstrap \\ --token=02b50b05283e98dd0fd71db496ef01e8 \\ --kubeconfig=bootstrap.kubeconfig kubectl config set-context default \\ --cluster=kthw \\ --user kubelet-bootstrap \\ --kubeconfig=bootstrap.kubeconfig kubectl config use-context default \\ --kubeconfig=bootstrap.kubeconfig Now that we have this file, add following flags to the kubelet systemd service file.\n--bootstrap-kubeconfig=/home/vagrant/bootstrap.kubeconfig --kubeconfig=/home/vagrant/kubeconfig --rotate-certificates=true --rotate-server-certificates=true Provide path to the bootstrap.kubeconfig you have generated just before. And even if you don\u0026rsquo;t have kubeconfig still provide some path where kubelet has permission to write. kubelet will create this file for you.\nSo my kubelet, systemd file looks like following:\n$ cat /etc/systemd/system/kubelet.service [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/kubernetes/kubernetes After=containerd.service Requires=containerd.service [Service] ExecStart=/usr/local/bin/kubelet \\  --bootstrap-kubeconfig=/home/vagrant/bootstrap.kubeconfig \\  --config=/var/lib/kubelet/kubelet-config.yaml \\  --container-runtime=remote \\  --container-runtime-endpoint=unix:///var/run/containerd/containerd.sock \\  --image-pull-progress-deadline=2m \\  --kubeconfig=/home/vagrant/kubeconfig \\  --network-plugin=cni \\  --register-node=true \\  --rotate-certificates=true \\  --rotate-server-certificates=true \\  --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target And KubeletConfiguration looks like this:\n$ cat /var/lib/kubelet/kubelet-config.yaml kind: KubeletConfiguration apiVersion: kubelet.config.k8s.io/v1beta1 authentication: anonymous: enabled: false webhook: enabled: true x509: clientCAFile: \u0026#34;/var/lib/kubernetes/ca.pem\u0026#34; authorization: mode: Webhook clusterDomain: \u0026#34;cluster.local\u0026#34; clusterDNS: - \u0026#34;10.32.0.10\u0026#34; podCIDR: \u0026#34;10.200.0.0/24\u0026#34; resolvConf: \u0026#34;/run/systemd/resolve/resolv.conf\u0026#34; runtimeRequestTimeout: \u0026#34;15m\u0026#34; Once this is done, just run following commands to restart the kubelet:\nsudo systemctl daemon-reload sudo systemctl restart kubelet sudo systemctl status kubelet master node List the request that node has made:\n$ kubectl get csr NAME AGE REQUESTOR CONDITION node-csr-WXnon3AEhdxgZ1FZ2reqKRQmWS-pwP3x263YbwUAH9k 11m kubelet-bootstrap Pending Approve that request:\n$ kubectl certificate approve node-csr-WXnon3AEhdxgZ1FZ2reqKRQmWS-pwP3x263YbwUAH9k certificatesigningrequest.certificates.k8s.io/node-csr-WXnon3AEhdxgZ1FZ2reqKRQmWS-pwP3x263YbwUAH9k approved Now you can see that the node has been created and you can list by running:\n$ kubectl get nodes NAME STATUS ROLES AGE VERSION node Ready \u0026lt;none\u0026gt; 27s v1.12.0 ",
    "ref": "/post/add-new-k8s-node-cert-rotate/"
  },{
    "title": "Adding new worker to existing Kubernetes cluster",
    "date": "",
    "description": "Step by step guide to add new node",
    "body": "To setup a multi-node Kubernetes cluster just run this script and you will have a cluster with 3 masters and 3 workers.\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME worker-0 Ready \u0026lt;none\u0026gt; 1h v1.11.2 192.168.199.20 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-1 Ready \u0026lt;none\u0026gt; 1h v1.11.2 192.168.199.21 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-2 Ready \u0026lt;none\u0026gt; 1h v1.11.2 192.168.199.22 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 Now to add a new node to this cluster you will need to bring up a VM, for this just use following Vagrantfile.\n$ cat Vagrantfile # -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.define \u0026#34;ubuntu\u0026#34; do |ubuntu| ubuntu.vm.box = \u0026#34;ubuntu/bionic64\u0026#34; config.vm.hostname = \u0026#34;ubuntu\u0026#34; ubuntu.vm.network \u0026#34;private_network\u0026#34;, ip: \u0026#34;192.168.199.23\u0026#34; end config.vm.provider \u0026#34;virtualbox\u0026#34; do |virtualbox, override| virtualbox.memory = 3000 virtualbox.cpus = 3 end end Note: This machine I have given IP address which is in the same subnet as other workers.\nBring up this machine using:\nvagrant up Now you can setup the node in one shot by running this script.\nDownload and install binaries Let\u0026rsquo;s go through this script and let me try to explain what each step does.\nYou can skip the part where we are downloading tools, if you all the required binaries available, viz. kubelet, kube-proxy, kubectl, cfssl, cni, runc, etc.\nOnce you all the tools available install them following this section:\n# install those tools mkdir -p \\  /etc/containers \\  /etc/cni/net.d \\  /etc/crio \\  /opt/cni/bin \\  /usr/local/libexec/crio \\  /var/lib/kubelet \\  /var/lib/kube-proxy \\  /var/lib/kubernetes \\  /var/run/kubernetes tar -xvf cni-plugins-amd64-v0.6.0.tgz -C /opt/cni/bin/ cp runc /usr/local/bin/ cp {crio,kube-proxy,kubelet,kubectl} /usr/local/bin/ cp {conmon,pause} /usr/local/libexec/crio/ cp {crio.conf,seccomp.json} /etc/crio/ cp policy.json /etc/containers/ curl -sSL \\  -O \u0026#34;https://pkg.cfssl.org/${cfssl_version}/cfssl_linux-amd64\u0026#34; \\  -O \u0026#34;https://pkg.cfssl.org/${cfssl_version}/cfssljson_linux-amd64\u0026#34; chmod +x cfssl_linux-amd64 cfssljson_linux-amd64 mv -v cfssl_linux-amd64 /usr/local/bin/cfssl mv -v cfssljson_linux-amd64 /usr/local/bin/cfssljson Generating configurations Now that all the required tools are installed. Let\u0026rsquo;s create the configuration that we need to make sure these tools work fine. But before that you will need the ca.pem, ca-key.pem \u0026amp; ca-config.json used for createing configs for master nodes.\nConfiguration shared by all workers Some of the configuration are same as other nodes which can be copied from other nodes. This includes following:\n /etc/cni/net.d/99-loopback.conf /var/lib/kube-proxy/kubeconfig /etc/systemd/system/kube-proxy.service  Following steps here help you re-create them if you can\u0026rsquo;t access those from other nodes, follow them from here:\n# generate the 99-loopback.conf common for all the workers, can be copied cat \u0026gt; 99-loopback.conf \u0026lt;\u0026lt;EOF { \u0026quot;cniVersion\u0026quot;: \u0026quot;0.3.1\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;loopback\u0026quot; } EOF # generate the kube-proxy cert common for all the workers, can be copied cat \u0026gt; kube-proxy-csr.json \u0026lt;\u0026lt;EOF { \u0026quot;CN\u0026quot;: \u0026quot;system:kube-proxy\u0026quot;, \u0026quot;key\u0026quot;: { \u0026quot;algo\u0026quot;: \u0026quot;rsa\u0026quot;, \u0026quot;size\u0026quot;: 2048 }, \u0026quot;names\u0026quot;: [ { \u0026quot;C\u0026quot;: \u0026quot;US\u0026quot;, \u0026quot;L\u0026quot;: \u0026quot;Portland\u0026quot;, \u0026quot;O\u0026quot;: \u0026quot;system:node-proxier\u0026quot;, \u0026quot;OU\u0026quot;: \u0026quot;Kubernetes The Hard Way\u0026quot;, \u0026quot;ST\u0026quot;: \u0026quot;Oregon\u0026quot; } ] } EOF cfssl gencert \\ -ca=${capem} \\ -ca-key=${cakeypem} \\ -config=${caconfig} \\ -profile=kubernetes \\ kube-proxy-csr.json | cfssljson -bare kube-proxy kubectl config set-cluster kubernetes-the-hard-way \\ --certificate-authority=${capem} \\ --embed-certs=true \\ --server=https://192.168.199.10:6443 \\ --kubeconfig=\u0026quot;kube-proxy.kubeconfig\u0026quot; kubectl config set-credentials kube-proxy \\ --client-certificate=\u0026quot;kube-proxy.pem\u0026quot; \\ --client-key=\u0026quot;kube-proxy-key.pem\u0026quot; \\ --embed-certs=true \\ --kubeconfig=\u0026quot;kube-proxy.kubeconfig\u0026quot; kubectl config set-context default \\ --cluster=kubernetes-the-hard-way \\ --user=kube-proxy \\ --kubeconfig=\u0026quot;kube-proxy.kubeconfig\u0026quot; kubectl config use-context default --kubeconfig=\u0026quot;kube-proxy.kubeconfig\u0026quot; # generate the kube-proxy.service common for all the workers, can be copied cat \u0026gt; kube-proxy.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Kube Proxy Documentation=https://github.com/GoogleCloudPlatform/kubernetes [Service] ExecStart=/usr/local/bin/kube-proxy \\\\ --cluster-cidr=10.200.0.0/16 \\\\ --kubeconfig=/var/lib/kube-proxy/kubeconfig \\\\ --proxy-mode=iptables \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF Configuration specific to workers  The cri-o daemon\u0026rsquo;s systemd service file.  # generate the service file for the crio daemon, specific to node cat \u0026gt; ${hostname}-crio.service \u0026lt;\u0026lt;EOF [Unit] Description=CRI-O daemon Documentation=https://github.com/kubernetes-incubator/cri-o [Service] ExecStart=/usr/local/bin/crio --stream-address ${ipaddr} --runtime /usr/local/bin/runc --registry docker.io Restart=always RestartSec=10s [Install] WantedBy=multi-user.target EOF Here IP address the daemon should bing to is to be specified and the only change.\n Generating csr file:  # generate the worker certs, specific to node cat \u0026gt; ${hostname}-csr.json \u0026lt;\u0026lt;EOF { \u0026#34;CN\u0026#34;: \u0026#34;system:node:${hostname}\u0026#34;, \u0026#34;key\u0026#34;: { \u0026#34;algo\u0026#34;: \u0026#34;rsa\u0026#34;, \u0026#34;size\u0026#34;: 2048 }, \u0026#34;names\u0026#34;: [ { \u0026#34;C\u0026#34;: \u0026#34;US\u0026#34;, \u0026#34;L\u0026#34;: \u0026#34;Portland\u0026#34;, \u0026#34;O\u0026#34;: \u0026#34;system:nodes\u0026#34;, \u0026#34;OU\u0026#34;: \u0026#34;Kubernetes The Hard Way\u0026#34;, \u0026#34;ST\u0026#34;: \u0026#34;Oregon\u0026#34; } ] } EOF Make sure that the CN field in csr is of the format system:node:\u0026lt;hostname\u0026gt;, because this is used to register the name of the worker with master. This is the only changed field from other worker nodes.\n Generate certs and kubeconfig:  cfssl gencert \\  -ca=${capem} \\  -ca-key=${cakeypem} \\  -config=${caconfig} \\  -hostname=\u0026#34;${hostname},${ipaddr}\u0026#34; \\  -profile=kubernetes \\  \u0026#34;${hostname}-csr.json\u0026#34; | cfssljson -bare \u0026#34;${hostname}\u0026#34; # generate kubeconfig specific to the node kubectl config set-cluster kubernetes-the-hard-way \\  --certificate-authority=${capem} \\  --embed-certs=true \\  --server=https://192.168.199.40:6443 \\  --kubeconfig=\u0026#34;${hostname}.kubeconfig\u0026#34; kubectl config set-credentials system:node:${hostname} \\  --client-certificate=\u0026#34;${hostname}.pem\u0026#34; \\  --client-key=\u0026#34;${hostname}-key.pem\u0026#34; \\  --embed-certs=true \\  --kubeconfig=\u0026#34;${hostname}.kubeconfig\u0026#34; kubectl config set-context default \\  --cluster=kubernetes-the-hard-way \\  --user=system:node:${hostname} \\  --kubeconfig=\u0026#34;${hostname}.kubeconfig\u0026#34; kubectl config use-context default --kubeconfig=\u0026#34;${hostname}.kubeconfig\u0026#34;  Now create kubelet systemd service file:  cat \u0026gt; ${hostname}-kubelet.service \u0026lt;\u0026lt;EOF [Unit] Description=Kubernetes Kubelet Documentation=https://github.com/GoogleCloudPlatform/kubernetes After=crio.service Requires=crio.service [Service] ExecStart=/usr/local/bin/kubelet \\\\ --anonymous-auth=false \\\\ --authorization-mode=Webhook \\\\ --client-ca-file=/var/lib/kubernetes/ca.pem \\\\ --allow-privileged=true \\\\ --cluster-dns=10.32.0.10 \\\\ --cluster-domain=cluster.local \\\\ --container-runtime=remote \\\\ --container-runtime-endpoint=unix:///var/run/crio/crio.sock \\\\ --image-pull-progress-deadline=2m \\\\ --image-service-endpoint=unix:///var/run/crio/crio.sock \\\\ --kubeconfig=/var/lib/kubelet/kubeconfig \\\\ --network-plugin=cni \\\\ --register-node=true \\\\ --runtime-request-timeout=10m \\\\ --tls-cert-file=/var/lib/kubelet/${hostname}.pem \\\\ --tls-private-key-file=/var/lib/kubelet/${hostname}-key.pem \\\\ --node-ip=${ipaddr} \\\\ --v=2 Restart=on-failure RestartSec=5 [Install] WantedBy=multi-user.target EOF In above service file the things that changes are two flags: --tls-cert-file and --tls-private-key-file. And another flag value that is different from other nodes is --node-ip. Rest everything is same with other nodes. So just change the path to point to the right cert file and key file.\nInstall configurations Once all those configs are generated, copy all those to the appropriate location.\n# install above generated config cp 99-loopback.conf /etc/cni/net.d cp kube-proxy.kubeconfig /var/lib/kube-proxy/kubeconfig cp kube-proxy.service /etc/systemd/system/ cp \u0026#34;${hostname}-crio.service\u0026#34; /etc/systemd/system/crio.service cp ${capem} /var/lib/kubernetes/ cp \u0026#34;${hostname}.pem\u0026#34; \u0026#34;${hostname}-key.pem\u0026#34; /var/lib/kubelet cp \u0026#34;${hostname}.kubeconfig\u0026#34; /var/lib/kubelet/kubeconfig cp \u0026#34;${hostname}-kubelet.service\u0026#34; /etc/systemd/system/kubelet.service Start processes Now that all binaries and configs are in place, just restart the processes:\nsystemctl daemon-reload systemctl enable crio kubelet kube-proxy systemctl start crio kubelet kube-proxy Check nodes $ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME ubuntu Ready \u0026lt;none\u0026gt; 5m v1.11.2 192.168.199.23 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-34-generic cri-o://1.11.2 worker-0 Ready \u0026lt;none\u0026gt; 40m v1.11.2 192.168.199.20 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-34-generic cri-o://1.11.2 worker-1 Ready \u0026lt;none\u0026gt; 39m v1.11.2 192.168.199.21 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-34-generic cri-o://1.11.2 worker-2 Ready \u0026lt;none\u0026gt; 38m v1.11.2 192.168.199.22 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-34-generic cri-o://1.11.2 Node has been added successfully.\n",
    "ref": "/post/add-new-k8s-node-manually/"
  },{
    "title": "Single node Kubernetes Cluster on Fedora with SELinux enabled",
    "date": "",
    "description": "Kubeadm to install Single Node K8S with SELinux",
    "body": "Start a single node fedora machine, using whatever method but I have used this Vagrantfile to do it:\n# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(\u0026#34;2\u0026#34;) do |config| config.vm.define \u0026#34;fedora\u0026#34; do |fedora| fedora.vm.box = \u0026#34;fedora/28-cloud-base\u0026#34; config.vm.hostname = \u0026#34;fedora\u0026#34; end config.vm.provider \u0026#34;virtualbox\u0026#34; do |virtualbox, override| virtualbox.memory = 4096 virtualbox.cpus = 4 end config.vm.provision \u0026#34;shell\u0026#34;, privileged: false, inline: \u0026lt;\u0026lt;-SHELL  echo \u0026#39;127.0.0.1 localhost\u0026#39; | cat - /etc/hosts \u0026gt; temp \u0026amp;\u0026amp; sudo mv temp /etc/hosts SHELL end Now start it and ssh into it:\nvagrant up vagrant ssh Once inside the machine, become root user and run this script:\nsudo -i curl https://raw.githubusercontent.com/surajssd/scripts/master/shell/k8s-install-single-node/install.sh | sh And you should have a running Kubernetes cluster.\nUnderstanding steps Install and start docker:\nyum install -y docker systemctl enable docker \u0026amp;\u0026amp; systemctl start docker Install kubelet and start it:\necho \u0026#34; [kubernetes] name=Kubernetes baseurl=https://packages.cloud.google.com/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=1 repo_gpgcheck=1 gpgkey=https://packages.cloud.google.com/yum/doc/yum-key.gpg https://packages.cloud.google.com/yum/doc/rpm-package-key.gpg exclude=kube* \u0026#34; | tee /etc/yum.repos.d/kubernetes.repo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes systemctl enable kubelet \u0026amp;\u0026amp; systemctl start kubelet Set SELinux contexts:\n# for kubernetes files mkdir -p /etc/kubernetes/ chcon -R -t svirt_sandbox_file_t /etc/kubernetes/ # for etcd files mkdir -p /var/lib/etcd chcon -R -t svirt_sandbox_file_t /var/lib/etcd Start kubeadm:\nkubeadm config images pull kubeadm init Set the kubectl context:\nmkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/config Install network, this step can be varied depending on which networking provider you want to install, here I have installed weave net. For other providers see here.\nkubectl apply -f \u0026#34;https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d \u0026#39;\\n\u0026#39;)\u0026#34; Also use master node as worker node.\nkubectl taint nodes --all node-role.kubernetes.io/master- Finally list nodes or wait until node is ready.\nkubectl get nodes Debugging the setup  You can see logs of kubelet by running journalctl -f -u kubelet You can also see if there are any failing control plain containers by running docker ps -a and then check the logs of failed containers.  References  Install Kubeadm Start Kubeadm cluster Installation script  ",
    "ref": "/post/single-node-k8s-fedora-selinux/"
  },{
    "title": "HostPath volumes and it's problems",
    "date": "",
    "description": "Kubernetes HostPath volume good way to nuke your Kubernetes Nodes",
    "body": "This post will demonstrate how Kubernetes HostPath volumes can help you get access to the Kubernetes nodes. Atleast you can play with the filesystem of the node on which you pod is scheduled on. You can get access to other containers running on the host, certificates of the kubelet, etc.\nI have a 3-master and 3-node cluster and setup using this script, running in a Vagrant environment.\nAll the nodes are in ready state:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME worker-0 Ready \u0026lt;none\u0026gt; 24m v1.11.2 192.168.199.20 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-1 Ready \u0026lt;none\u0026gt; 23m v1.11.2 192.168.199.21 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-2 Ready \u0026lt;none\u0026gt; 21m v1.11.2 192.168.199.22 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 The deployment looks like this:\n$ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: run: web name: web spec: replicas: 1 selector: matchLabels: run: web template: metadata: labels: run: web spec: containers: - image: centos/httpd name: web volumeMounts: - mountPath: /web name: test-volume volumes: - name: test-volume hostPath: path: / Above you can see we are mounting / of the host inside pod at /web. This is our gateway to host\u0026rsquo;s file system. Let\u0026rsquo;s deploy this:\n$ kubectl apply -f deployment.yaml deployment.apps/web created And now that pod has started:\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE web-66cdf67bbc-44zhj 1/1 Running 0 4m 10.38.0.1 worker-2 \u0026lt;none\u0026gt; Getting inside the pod and checking out the mounted directory:\n$ kubectl exec -it web-66cdf67bbc-44zhj bash [root@web-66cdf67bbc-44zhj /]# cd /web [root@web-66cdf67bbc-44zhj web]# ls bin boot dev etc home initrd.img initrd.img.old lib lib64 lost+found media mnt opt proc root run sbin snap srv sys tmp usr vagrant var vmlinuz vmlinuz.old Now we can either chroot into this and see the output of ps.\n[root@web-66cdf67bbc-44zhj ~]# chroot /web # ps aufx USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMAND root 2 0.0 0.0 0 0 ? S 05:15 0:00 [kthreadd] root 4 0.0 0.0 0 0 ? I\u0026lt; 05:15 0:00 \\_ [kworker/0:0H] root 6 0.0 0.0 0 0 ? I\u0026lt; 05:15 0:00 \\_ [mm_percpu_wq] root 7 0.0 0.0 0 0 ? S 05:15 0:00 \\_ [ksoftirqd/0] root 8 0.0 0.0 0 0 ? I 05:15 0:00 \\_ [rcu_sched] root 9 0.0 0.0 0 0 ? I 05:15 0:00 \\_ [rcu_bh] root 10 0.0 0.0 0 0 ? S 05:15 0:00 \\_ [migration/0] root 11 0.0 0.0 0 0 ? S 05:15 0:00 \\_ [watchdog/0] root 12 0.0 0.0 0 0 ? S 05:15 0:00 \\_ [cpuhp/0] Or you can just delete the entire root, a.k.a. nuking the node.\n# rm -rf --no-preserve-root / Now if you look at the nodes:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME worker-0 Ready \u0026lt;none\u0026gt; 30m v1.11.2 192.168.199.20 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-1 Ready \u0026lt;none\u0026gt; 29m v1.11.2 192.168.199.21 \u0026lt;none\u0026gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-2 NotReady \u0026lt;none\u0026gt; 27m v1.11.2 192.168.199.22 \u0026lt;none\u0026gt; Unknown 4.15.0-33-generic cri-o://Unknown The last node worker-2 is in NotReady state. We have successfully made one node unusable. Now that one node is gone your pod will be scheduled on another node, where you can do similar stuff.\n$ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE web-66cdf67bbc-44zhj 0/1 Unknown 0 20m 10.38.0.1 worker-2 \u0026lt;none\u0026gt; web-66cdf67bbc-b22xn 1/1 Running 0 8m 10.32.0.2 worker-0 \u0026lt;none\u0026gt; As you can see above the pod is re-scheduled on node worker-0. And we can do same set of steps to make worker-0 unusable.\nDeleting the deployment to cleanup.\n$ kubectl delete deployment web deployment.extensions \u0026#34;web\u0026#34; deleted Stopping this attack using PodSecurityPolicy Now as a cluster admin how can you prevent this from happening? You can create something called as PodSecurityPolicy. This let\u0026rsquo;s you define what kind of pods be created. Or what permissions pod can request. Enable admission controller for this, read about it here.\nHere is an example PodSecurityPolicy:\n$ cat podsecuritypolicy.yaml apiVersion: policy/v1beta1 kind: PodSecurityPolicy metadata: name: example spec: seLinux: rule: RunAsAny supplementalGroups: rule: RunAsAny runAsUser: rule: RunAsAny fsGroup: rule: RunAsAny volumes: - \u0026#39;*\u0026#39; privileged: false # Don\u0026#39;t allow privileged pods! allowedHostPaths: - pathPrefix: /foo readOnly: true In above example, we are restricting access to hostPath a pod can request. Here the path that is allowed is /foo and has readOnly access to the underlying file system.\nCreate PodSecurityPolicy using above file:\n$ kubectl apply -f podsecuritypolicy.yaml podsecuritypolicy.policy/example created To enable this policy we need to create few more objects, a Role and RoleBinding.\nRole:\n$ cat role.yaml kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata: name: authorize-hostpath rules: - apiGroups: [\u0026#39;policy\u0026#39;] resources: [\u0026#39;podsecuritypolicies\u0026#39;] verbs: [\u0026#39;use\u0026#39;] resourceNames: - example This Role will allow usage of policy that we created above.\nCreate Role:\n$ kubectl apply -f role.yaml role.rbac.authorization.k8s.io/authorize-hostpath created RoleBinding:\n$ cat rolebinding.yaml kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata: name: auth-hostpath roleRef: kind: Role name: authorize-hostpath apiGroup: rbac.authorization.k8s.io subjects: # Authorize all service accounts in a namespace: - kind: Group apiGroup: rbac.authorization.k8s.io name: system:serviceaccounts This RoleBinding will bind the Role above and all the ServiceAccounts in current namespace.\nCreate RoleBinding:\n$ kubectl create -f rolebinding.yaml rolebinding.rbac.authorization.k8s.io/auth-hostpath created Now that we have required permissions in place, try to re-create the deployment:\n$ kubectl apply -f deployment.yaml deployment.apps/web created The pod is not created and in events you can see an error as Error creating: pods \u0026quot;web-66cdf67bbc-\u0026quot; is forbidden: unable to validate against any pod security policy: [spec.volumes[0].hostPath.pathPrefix: Invalid value: \u0026quot;/\u0026quot;: is not allowed to be used]:\n$ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE ... 6s 17s 12 web-66cdf67bbc.15530e83fd4f8592 ReplicaSet Warning FailedCreate replicaset-controller Error creating: pods \u0026#34;web-66cdf67bbc-\u0026#34; is forbidden: unable to validate against any pod security policy: [spec.volumes[0].hostPath.pathPrefix: Invalid value: \u0026#34;/\u0026#34;: is not allowed to be used] This error is due to the fact that we have allowed hostPath to be only under /foo and in the original file it is set to /.\nNow change in deployment.yaml file at path deployment.spec.template.spec.volumes[0].hostPath.path from / to /foo and apply again:\n$ kubectl apply -f deployment.yaml deployment.apps/web configured You can see another error Error creating: pods \u0026quot;web-85cb548b47-\u0026quot; is forbidden: unable to validate against any pod security policy: [spec.containers[0].volumeMounts[0].readOnly: Invalid value: false: must be read-only]:\n$ kubectl get events LAST SEEN FIRST SEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE ... 5s 15s 12 web-85cb548b47.15530eb0c27c6122 ReplicaSet Warning FailedCreate replicaset-controller Error creating: pods \u0026#34;web-85cb548b47-\u0026#34; is forbidden: unable to validate against any pod security policy: [spec.containers[0].volumeMounts[0].readOnly: Invalid value: false: must be read-only] This is because in the volumeMount\u0026rsquo;s readOnly we have used in container has no value defined, which means it defaults to false and in PodSecurityPolicy we have defaulted the hostPath to be readOnly.\nSo change deployment.spec.template.spec.containers[0].volumeMounts[0].readOnly to true. And manifest should look like following:\n$ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata: labels: run: web name: web spec: replicas: 1 selector: matchLabels: run: web template: metadata: labels: run: web spec: containers: - image: centos/httpd name: web volumeMounts: - mountPath: /web name: test-volume readOnly: true volumes: - name: test-volume hostPath: path: /foo Now if you re-deploy the app:\n$ kubectl apply -f deployment.yaml deployment.apps/web configured The pod is scheduled and created:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE web-7b77459d6b-kbqm8 1/1 Running 0 7s Now if you exec into this pod and try to do something, you can see nothing happens:\n$ kubectl exec -it web-7b77459d6b-kbqm8 bash [root@web-7b77459d6b-kbqm8 /]# cd /web/ [root@web-7b77459d6b-kbqm8 web]# ls [root@web-7b77459d6b-kbqm8 web]# touch file.txt touch: cannot touch \u0026#39;file.txt\u0026#39;: Read-only file system So this is really good feature you can use to stop someone from nuking your cluster.\nStopping this attack using SELinux Above setup of the Kubernetes cluster had a Ubuntu based machines, now I have a Kubernetes cluster that is setup on Fedora which supports SELinux.\nYou can setup this cluster following steps in this post.\nNote: This is a simple cluster setup without PodSecurityPolicy.\nOnce you have the cluster running:\n$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME fedora Ready master 23m v1.11.2 10.0.2.15 \u0026lt;none\u0026gt; Fedora 28 (Cloud Edition) 4.16.3-301.fc28.x86_64 docker://1.13.1 Lets follow the same set of steps of creating the deployment:\n$ kubectl apply -f deployment.yaml deployment.apps/web created The pod is created:\n$ kubectl get pods NAME READY STATUS RESTARTS AGE web-66cdf67bbc-t9n7m 1/1 Running 0 28s Getting into the machine:\n$ kubectl exec -it web-66cdf67bbc-t9n7m bash [root@web-66cdf67bbc-t9n7m /]# [root@web-66cdf67bbc-t9n7m /]# cd /web/ [root@web-66cdf67bbc-t9n7m web]# ls bin boot dev etc home lib lib64 lost+found media mnt opt proc root run sbin srv sys tmp usr vagrant var [root@web-66cdf67bbc-t9n7m web]# touch file.txt touch: cannot touch \u0026#39;file.txt\u0026#39;: Permission denied As you can see if you go to the root of host which is mounted at /web and try to create new file, it fails. This event will be logged into the SELinux audit logs. On host if you run following you will see logs:\n# ausearch -m avc ---- time-\u0026gt;Tue Sep 11 06:41:54 2018 type=AVC msg=audit(1536648114.522:985): avc: denied { write } for pid=15775 comm=\u0026#34;touch\u0026#34; name=\u0026#34;/\u0026#34; dev=\u0026#34;sda1\u0026#34; ino=2 scontext=system_u:system_r:container_t:s0:c230,c784 tcontext=system_u:object_r:root_t:s0 tclass=dir permissive=0 Here simple SELinux has stopped the container from writing into places it shouldn\u0026rsquo;t. Compared to PodSecurityPolicy (which is a beta feature in k8s 1.11), SELinux can help you right away if you are using older Kubernetes cluster and are on CentOS or RHEL.\nReferences  Multi node Kubernetes setup used here Pod Security Policies Securing a Cluster hostPath volumes Enabling Admission controller SELinux denials Single node cluster with SELinux used here  ",
    "ref": "/post/k8s-hostpat-nuke-nodes/"
  },{
    "title": "HTTPS during development using 'mkcert'",
    "date": "",
    "description": "Use https even during your development",
    "body": "It\u0026rsquo;s always a hassle creating certificates and lot of technical jargons involved. This can be simplified, using mkcert. Install by following one of the steps mentioned in the docs.\nOnce installed just run:\n$ mkcert -install Created a new local CA at \u0026#34;/home/hummer/.local/share/mkcert\u0026#34; üí• [sudo] password for hummer: The local CA is now installed in the system trust store! ‚ö° The local CA is now installed in the Firefox and/or Chrome/Chromium trust store (requires browser restart)! ü¶ä This has installed the local CA. Now all you need to do is create a new certificate.\n$ mkcert 127.0.0.1 localhost Using the local CA at \u0026#34;/home/hummer/.local/share/mkcert\u0026#34; ‚ú® Created a new certificate valid for the following names üìú - \u0026#34;127.0.0.1\u0026#34; - \u0026#34;localhost\u0026#34; The certificate is at \u0026#34;./127.0.0.1+1.pem\u0026#34; and the key at \u0026#34;./127.0.0.1+1-key.pem\u0026#34; ‚úÖ Now is the time to test it, so to test it I am running a Python\u0026rsquo;s SimpleHTTPServer using following code(by default if you run python -m SimpleHTTPServer it runs on HTTP).\n$ cat simple-https-server.py import BaseHTTPServer, SimpleHTTPServer import ssl httpd = BaseHTTPServer.HTTPServer((\u0026#39;localhost\u0026#39;, 4443), SimpleHTTPServer.SimpleHTTPRequestHandler) httpd.socket = ssl.wrap_socket (httpd.socket, certfile=\u0026#39;./127.0.0.1+1.pem\u0026#39;, keyfile=\u0026#39;127.0.0.1+1-key.pem\u0026#39;, server_side=True) httpd.serve_forever() This code is taken from here with just modification to the certfile and keyfile file names.\nNow just run this file as:\n$ python2 simple-https-server.py 127.0.0.1 - - [14/Aug/2018 11:02:29] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 127.0.0.1 - - [14/Aug/2018 11:02:29] code 404, message File not found 127.0.0.1 - - [14/Aug/2018 11:02:29] \u0026#34;GET /favicon.ico HTTP/1.1\u0026#34; 404 - 127.0.0.1 - - [14/Aug/2018 11:03:55] \u0026#34;GET / HTTP/1.1\u0026#34; 200 - 127.0.0.1 - - [14/Aug/2018 11:03:58] \u0026#34;GET /simple-https-server.py HTTP/1.1\u0026#34; 200 - Now if you have browser running already restart it and goto https://localhost:4443/. And voila your local HTTPS server is running.\nSimilarly you can create more certificates with wildcard domain and use those certificates with your applications.\nHuge üëç to the developer of mkcert.\n",
    "ref": "/post/mkcert-using-python-http-server/"
  },{
    "title": "Golang struct tags gotchas",
    "date": "",
    "description": "Struct tags can give you problems you didn't see coming ;-)",
    "body": "In golang while using struct tag, the spaces make a lot of difference. For example look at the following code.\ntype PodStatus struct { Status string `json: \u0026#34;,status\u0026#34;` } If you run go vet on this piece of code you will get following error:\n$ go vet types.go # command-line-arguments ./types.go:28: struct field tag `json: \u0026#34;,status\u0026#34;` not compatible with reflect.StructTag.Get: bad syntax for struct tag value Now this does not tell us what is wrong with the struct tag, json: \u0026quot;,status\u0026quot;. The problem with this struct tag is that the extra space can be interpreted as delimiter so provide key-value pair without space.\nSo if the struct changes from:\n`json: \u0026#34;,status\u0026#34;` to\n`json:\u0026#34;,status\u0026#34;` So the change is just the space after json:, now we don\u0026rsquo;t see the error.\nMore information about the struct tags can be found in this elaborated blog post named: Tags in Golang.\n",
    "ref": "/post/golang-struct-tags-space/"
  },{
    "title": "Access etcd in OpenShift origin",
    "date": "",
    "description": "Access the etcd in OpenShift started by oc cluster up",
    "body": "How do you access the etcd that is being used by the OpenShift started by oc cluster up or using minishift.\nIf you are using minishift then get docker environment access of the minishift VM by running following commands.\neval $(minishift docker-env) \u0026amp;\u0026amp; eval $(minishift oc-env) Exec into the container named origin that runs OpenShift and all the needed services.\n$ docker exec -it origin bash First install the etcdctl needed to talk to etcd.\n[root@surajd origin]$ yum -y install etcd Get into the directory where all the certs and keys are available.\n[root@surajd origin]$ cd /var/lib/origin/openshift.local.config/master Now run following to connect to the etcd.\n[root@surajd master]$ export ETCDCTL_API=3 [root@surajd master]$ etcdctl --cacert ./ca.crt --cert ./master.etcd-client.crt \\  --key ./master.etcd-client.key \\  --endpoints=[https://127.0.0.1:4001] \\  get --prefix --keys-only=true / /kubernetes.io/apiextensions.k8s.io/customresourcedefinitions/openshiftwebconsoleconfigs.webconsole.operator.openshift.io /kubernetes.io/apiservices/v1. /kubernetes.io/apiservices/v1.apps /kubernetes.io/apiservices/v1.apps.openshift.io ... Now you can try to read about a specific object by looking at a specific key.\n[root@surajd master]$ etcdctl --cacert ./ca.crt --cert ./master.etcd-client.crt --key ./master.etcd-client.key --endpoints=[https://127.0.0.1:4001] get --prefix /openshift.io/users/developer /openshift.io/users/developer k8s user.openshift.io/v1Userb G developer\u0026#34;*$a749b0bf-79ee-11e8-87db-507b9d785c9e2zanypassword:developer\u0026#34; This is a rpc binary data. You can use tools like protoc to decode it. There is some discussion about decoding this data.\n",
    "ref": "/post/accessing-oc-cluster-up-etcd/"
  },{
    "title": "Change namespaces in Kubernetes",
    "date": "",
    "description": "Easy way to change namespace in Kubernetes",
    "body": "There is no easy way to change namespace in Kubernetes using kubectl command line utility. But here are some commands that you can alias in your bashrc file so that it\u0026rsquo;s just a single command that you can use to change the namespace in the Kubernetes cluster.\nChange namespace Let\u0026rsquo;s see step by step what goes in to change the namespace. So the first step is to find the context.\n A context element in a kubeconfig file is used to group access parameters under a convenient name. Each context has three parameters: cluster, namespace, and user. By default, the kubectl command-line tool uses parameters from the current context to communicate with the cluster. Read more.\n $ kubectl config current-context minikube Now I have a namespace which I have created and want to switch to it.\n$ kubectl create ns mywebapp namespace \u0026#34;mywebapp\u0026#34; created Now we have two needed information to change the current default namespace, context and namespace name.\n$ kubectl config set-context minikube --namespace mywebapp Context \u0026#34;minikube\u0026#34; modified. Notice that we have put the context here as minikube and namespace name as we have created, mywebapp.\nSo this can be put in one command and used conveniently. Just copy following bash function into your bashrc file and you will have change-ns as a command available.\nfunction change-ns() { namespace=$1 if [ -z $namespace ]; then echo \u0026#34;Please provide the namespace name: \u0026#39;change-ns mywebapp\u0026#39;\u0026#34; return 1 fi kubectl config set-context $(kubectl config current-context) --namespace $namespace } Verify change of namespace How do you verify that the namespace is changed? How do you find what is the current namespace? Run following command:\nkubectl get sa default -o jsonpath=\u0026#39;{.metadata.namespace}\u0026#39; Let\u0026rsquo;s deconstruct that. Every namespace that is created has a ServiceAccount created by default with name default (Read more).\nFor any object/artifact in Kubernetes there is a field called namespace inside it\u0026rsquo;s metadata field. Which tells you about what namespace a particular object is part of. If you don\u0026rsquo;t set it while creating, Kubernetes will set it to the current default namespace. So this is a source of knowing what current namespace is.\nLook at following pod object definition, the namespace for pod storage-provisioner is kube-system.\n$ kubectl get pod storage-provisioner -o yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: 2018-06-10T09:56:24Z name: storage-provisioner namespace: kube-system ... Now you can use the flag -o with jsonpath type of input to tell what specific object you would like to fetch information about, in our case we want metadata.namespace.\nThe above command can also be converted to one command, just add following snippet to bashrc and you will have a command current-ns.\nfunction current-ns() { kubectl get sa default -o jsonpath=\u0026#39;{.metadata.namespace}\u0026#39; echo } Hope that helps, you with being productive with Kubernetes. Happy hacking!\n",
    "ref": "/post/changing-k8s-ns/"
  },{
    "title": "Prometheus with existing application on OpenShift",
    "date": "",
    "description": "Setting up Prometheus with any application",
    "body": "This post is very specific to OpenShift and how you can have an application exposing prometheus metrics to be scraped by a prometheus running in the same cluster.\nRequirements Setting up cluster I have done it using the oc cluster up, read about how to do this here. You could also setup a local OpenShift cluster by running minishift, read about setting up minishift here.\nDownloading Kedge The configurations defined for setting up this cluster is written in a format that is understood by a tool called Kedge. This makes configuration easier to understand and edit. So for using this setup download Kedge and put it in your path as explained here.\nFollowing the setup Make sure you have a running OpenShift cluster\noc new-project monitor Now if your metrics exporting service if it is backed by https then set this flag otherwise the default is http.\nexport APP_SCHEME=https Give your application name\nexport APP_NAME=wit This one is important, here I have put in the link to the cluster which is exposing metrics, you can put yours.\nexport APP_URL=api.prod-preview.openshift.io Download the latest Prometheus Kedge file prometheus.yml. Or you can also download the prometheus.yml which was created as of this writing.\nkedge apply -f prometheus.yml Finally visit this URL to start seeing your prometheus dashboard.\necho http://$(oc get routes | grep prometheus | awk \u0026#39;{print $2}\u0026#39;) How it looks? $ oc new-project monitor Already on project \u0026quot;monitor\u0026quot; on server \u0026quot;https://192.168.122.1:8443\u0026quot;. You can add applications to this project with the 'new-app' command. For example, try: oc new-app centos/ruby-22-centos7~https://github.com/openshift/ruby-ex.git to build a new example application in Ruby. $ export APP_SCHEME=https $ export APP_NAME=wit $ export APP_URL=api.prod-preview.openshift.io $ kedge apply -f prometheus.yml persistentvolumeclaim \u0026quot;prometheus-storage\u0026quot; created service \u0026quot;prometheus\u0026quot; created route \u0026quot;prometheus\u0026quot; created configmap \u0026quot;prometheus-config\u0026quot; created deployment \u0026quot;prometheus\u0026quot; created $ echo http://$(oc get routes | grep prometheus | awk '{print $2}') http://prometheus-monitor.192.168.122.1.nip.io Happy monitoring!\n",
    "ref": "/post/using-prometheus/"
  },{
    "title": "Notes on talk - Advanced testing in golang by Mitchell Hashimoto",
    "date": "",
    "description": "This talk has really great takeaways which are worth considering while writing your tests",
    "body": "Test Fixtures  \u0026ldquo;go test\u0026rdquo; sets pwd as package directory  Test Helpers  should never return an error they should access to the *testing.T object call t.Helper() in the beginning (works only for go1.9+) for things reqiuiring clean up return closures  Configurability  Unconfigurable behavior is often a point of difficulty for tests. e.g. ports, timeouts, paths. Over-parameterize structs to allow tests to fine-tune their behavior It\u0026rsquo;s ok to make these configs unexported so only tests can set them.  Slides Video ",
    "ref": "/post/advanced-golang-testing-mitchellh-talk-notes/"
  },{
    "title": "Methods that satisfy interfaces in golang",
    "date": "",
    "description": "What receiver type methods satisfy which interface, can be understood here.",
    "body": "Pointer receiver For a struct User with a method Work with pointer receiver.\ntype User struct { Name string Period int } func (u *User) Work() { fmt.Println(u.Name, \u0026#34;has worked for\u0026#34;, u.Period, \u0026#34;hrs.\u0026#34;) } func main() { uval := User{\u0026#34;UserVal\u0026#34;, 5} uval.Work() pval := \u0026amp;User{\u0026#34;UserPtr\u0026#34;, 6} pval.Work() } See on go playground.\noutput:\nUserVal has worked for 5 hrs. UserPtr has worked for 6 hrs. If we call this method on value type object uval it works, and obviously it works with pointer type object pval.\nValue receiver Now we change the method receiver from pointer to value.\ntype User struct { Name string Period int } func (u User) Work() { fmt.Println(u.Name, \u0026#34;has worked for\u0026#34;, u.Period, \u0026#34;hrs.\u0026#34;) } func main() { uval := User{\u0026#34;UserVal\u0026#34;, 5} uval.Work() pval := \u0026amp;User{\u0026#34;UserPtr\u0026#34;, 6} pval.Work() } See on go playground.\noutput:\nUserVal has worked for 5 hrs. UserPtr has worked for 6 hrs. So this also worked on both value type object uval and with pointer type object pval.\nInterface and pointer receiver Lets try to add interface in the mix and see what happens, so adding interface Worker to pointer receiver method:\ntype User struct { Name string Period int } type Worker interface { Work() } func (u *User) Work() { fmt.Println(u.Name, \u0026#34;has worked for\u0026#34;, u.Period, \u0026#34;hrs.\u0026#34;) } func main() { uval := User{\u0026#34;UserVal\u0026#34;, 5} DoWork(uval) pval := \u0026amp;User{\u0026#34;UserPtr\u0026#34;, 6} DoWork(pval) } func DoWork(w Worker) { w.Work() } See on go playground.\noutput:\n# command-line-arguments tmp/main.go:20:8: cannot use uval (type User) as type Worker in argument to DoWork: User does not implement Worker (Work method has pointer receiver) So pointer type object pval implements interface Worker, but value type object uval does not. Since the error clearly says\nUser does not implement Worker (Work method has pointer receiver) To understand why this above code fails, you need to understand the concept of method sets. Golang spec defines Method sets as:\n The method set of any other type T consists of all methods declared with receiver type T.\n and\n The method set of the corresponding pointer type *T is the set of all methods declared with receiver *T or T (that is, it also contains the method set of T).\n finally\n The method set of a type determines the interfaces that the type implements and the methods that can be called using a receiver of that type.\n In above example the method with pointer receiver is not in method set of value type object uval.\nInterface and value receiver Now lets try the same inteface but this time with value receiver:\ntype User struct { Name string Period int } type Worker interface { Work() } func (u User) Work() { fmt.Println(u.Name, \u0026#34;has worked for\u0026#34;, u.Period, \u0026#34;hrs.\u0026#34;) } func main() { uval := User{\u0026#34;UserVal\u0026#34;, 5} DoWork(uval) pval := \u0026amp;User{\u0026#34;UserPtr\u0026#34;, 6} DoWork(pval) } func DoWork(w Worker) { w.Work() } See on go playground.\noutput:\nUserVal has worked for 5 hrs. UserPtr has worked for 6 hrs. Meanwhile the method with value receiver worked for both type of objects pointer type object pval and value type object uval.\nSo here is a table which charts this behavior:\n   scenario object value object pointer     pointer method yes yes   value method yes yes   interface pointer method no yes   interface value method yes yes    More on the case of why code compilation fails or compiler complains about value type object uval not implementing the interface Worker.\nReferences  Method sets in golang spec Methods, Interfaces and Embedded Types in Go  ",
    "ref": "/post/golang-methods-interfaces/"
  },{
    "title": "vscode Shortcuts",
    "date": "",
    "description": "Shortcuts for vscode and some notes",
    "body": "This post has shortcuts that are generic and golang specific as well. This post will edited from time to time.\nShortcuts  Toggle side bar  Ctrl + B\n Project explorer in side bar  Ctrl + Shift + E\n Project wide search in side bar  Ctrl + Shift + F\n Source control in side bar  Ctrl + Shift + G\n Copy entire line  Ctrl + C (without any selection)\n Delete entire line  Ctrl + Shift + K\n Toggle terminal  Ctrl + ~ \n Toggle problems/errors in current project  Ctrl + Shift + M\n Select entire line  Ctrl + I keep pressing I and then next line gets selected\n Move line or selected block up/down  Alt + up OR Alt + down\n Insert line below  Ctrl + Enter\n Insert line below  Ctrl + Shift + Enter\n Multiple cursors at random place  Alt + Mouse click\n Multiple line cursor  Shift + Alt + up/down\n Get cursor on multiple occurrences of string  Ctrl + Shift + L\n Comment line or selected block  Ctrl + /\n Reformat code  Ctrl + Shift + I\n Fold code block  Ctrl + Shift + [\n Unfold code block  Ctrl + Shift + ]\n Fold all sections  Ctrl + K and Ctrl + 0\n Unfold all sections  Ctrl + K and Ctrl + J\n Navigate through all errors in file  F8\n Open command pallette  F1 OR Ctrl + Shift + P\n Fuzzy search \u0026amp; open a file  Ctrl + P\n Zen mode  Ctrl + K and Z\n Trigger Intellisense  Ctrl + Space\n Go back to last where you were in file  Ctrl + Alt + -\n Go forward  Ctrl + Shift + -\n Vertically split editor  Ctrl + \\\n Vertically split editor  From project bar just select file and press Ctrl + Enter\n Switch among splits  Ctrl + 1 OR Ctrl + 2\n Navigate by symbols in a file  Ctrl + Shift + O\n Goto line number  Ctrl + G now enter line number\n Undo last cursor operation  Ctrl + U\nTricks  Use vscode as diff tool  code --diff \u0026lt;file1\u0026gt; \u0026lt;file2\u0026gt;  Code Snippets  This is small code templates or snippets which can help getting started boiler plate code.\nFor e.g. in golang file just type fmain and TAB you should see recommendation for main function. To see all snippets and what their short syntax is search for insert snippet in command palette. You can also add custom snippets.\nReference  Linux Shortcuts cheatsheet VSCode: 10 Most Useful Tips And Tricks. This video also shows how to configure task runner and then setup a custom key binding. Best of Visual Studio Code - Tips and Tricks  ",
    "ref": "/post/vscode-shortcuts/"
  },{
    "title": "Using private container registries from minikube",
    "date": "",
    "description": "A guide to how would you download image from private container registry in minikube",
    "body": "I am doing Kubernetes native development using minikube. And for doing that I had to download a Container image that is available in internally hosted private container registry.\nOn the configuration side of doing that you will need to create Kubernetes Secret of type docker-registry. And now refer that secret you just created in your Pod manifest under pod.spec.imagePullSecrets. For more info follow the tutorial in Kubernetes docs on Pull an Image from a Private Registry.\nBut this did not help me in pulling image from the private registry, I was getting error as follows:\n$ kubectl get events -w LASTSEEN FIRSTSEEN COUNT NAME KIND SUBOBJECT TYPE REASON SOURCE MESSAGE ... 2017-10-06 18:46:11 +0530 IST 2017-10-06 18:40:17 +0530 IST 30 private-reg Pod Warning FailedSync kubelet, minikube Error syncing pod 2017-10-06 18:46:23 +0530 IST 2017-10-06 18:40:17 +0530 IST 31 private-reg Pod Warning FailedSync kubelet, minikube Error syncing pod 2017-10-06 18:46:23 +0530 IST 2017-10-06 18:40:18 +0530 IST 24 private-reg Pod spec.containers{private-reg-container} Normal BackOff kubelet, minikube Back-off pulling image \u0026#34;my-cool-registry.com/surajd-images/busybox:1.26.2\u0026#34; ... Here private-reg is the pod name and my-cool-registry.com/surajd-images/busybox:1.26.2 is the image name.\nThe registry uses an identity certificate that is issued by company\u0026rsquo;s internal root CA. Your minikube VM needs to trust this root CA in order to work properly with the internal registry. This is where I realized that I had to download the CA cert.\nSo download the cert and dump it inside the VM, running following command:\n$ certificatefile=\u0026lt;your-ca-crt-file\u0026gt; $ registryserver=\u0026lt;your-registry-server\u0026gt; $ cat $certificatefile | minikube ssh \u0026#34;sudo mkdir -p /etc/docker/certs.d/$registryserver\u0026amp;\u0026amp; sudo tee /etc/docker/certs.d/$registryserver/ca.crt\u0026#34; ^C Note: You will have to press Ctrl + C, it should have done writing!\nNow you are fully ready to pull the image from this private registry. Happy Hacking!\n",
    "ref": "/post/private-registry-from-minikube/"
  },{
    "title": "Static Pods using Kubelet on Fedora",
    "date": "",
    "description": "Extension to the Kelsey Hightower's tutorial on 'Standalone Kubelet'",
    "body": "I wanted to try out Standalone Kubelet Tutorial of Kelsey Hightower by myself but I could not follow it as it is, because it was firstly on GCE and secondly it uses CoreOS, but since I am very familiar to Fedora I thought of following that tutorial on it. To get a quick setup of a fresh Fedora machine use Vagrant. I have used Vagrantfile available here.\nThis blog is only replacement of section Install the Standalone Kubelet in tutorial.\nInstalling packages Since the tutorial uses CoreOS VM it already has a Kubelet binary available, on Fedora you can get one using dnf. In tutorial Kelsey has put in his custom systemd service file, we will also make some changes to the default kubelet\u0026rsquo;s service file packaged in Fedora.\nsudo dnf -y install kubernetes-node sudo systemctl enable docker --now sudo systemctl enable kubelet --now Since docker is a dependency of kubelet it is also installed, all we need to do is start Docker manually alongwith Kubelet.\nVerify if kubelet and docker are running\nsudo systemctl status docker kubelet Editing kubelet systemd service file Apply following changes to /etc/kubernetes/config\ndiff --git a/config b/config index 8c0a284..cfccbee 100644 --- a/config +++ b/config @@ -16,7 +16,7 @@ KUBE_LOGTOSTDERR=\u0026#34;--logtostderr=true\u0026#34;  KUBE_LOG_LEVEL=\u0026#34;--v=0\u0026#34; # Should this cluster be allowed to run privileged docker containers -KUBE_ALLOW_PRIV=\u0026#34;--allow-privileged=false\u0026#34; +KUBE_ALLOW_PRIV=\u0026#34;--allow-privileged=true\u0026#34;  # How the controller-manager, scheduler, and proxy find the apiserver KUBE_MASTER=\u0026#34;--master=http://127.0.0.1:8080\u0026#34; By default in Fedora the Kubelet service won\u0026rsquo;t be running privileged pods. Setting flag --allow-privileged to true will allow you to do that. In Kelsey\u0026rsquo;s tutorial you can find it here.\nApply following changes to /etc/kubernetes/kubelet\ndiff --git a/kubelet b/kubelet index cfd5686..07c11ab 100644 --- a/kubelet +++ b/kubelet @@ -14,4 +14,4 @@ KUBELET_HOSTNAME=\u0026#34;--hostname-override=127.0.0.1\u0026#34;  KUBELET_API_SERVER=\u0026#34;--api-servers=http://127.0.0.1:8080\u0026#34; # Add your own! -KUBELET_ARGS=\u0026#34;--cgroup-driver=systemd\u0026#34; +KUBELET_ARGS=\u0026#34;--cgroup-driver=systemd --pod-manifest-path=/etc/kubernetes/manifests\u0026#34; To Kubelet we have added one more flag called --pod-manifest-path which is explained in docs as:\n Path to the directory containing pod manifest files to run, or the path to a single pod manifest file.\n Above line can be found in tutorial in here.\nMake directory for pod manifests sudo mkdir /etc/kubernetes/manifests Restart kubelet service sudo systemctl restart kubelet Now you can follow the rest of the tutorial as it is from section Static Pods.\nReferences:\n Static Pods Standalone Kubelet Tutorial - Kelsey Hightower  ",
    "ref": "/post/static-pods/"
  },{
    "title": "Sharing Vagrant Box offline",
    "date": "",
    "description": "Share Vagrant boxes like you shareed Operating System ISOs",
    "body": "If you have some box that was downloaded on your machine from Atlas but now you wanna share it on other machines and you don\u0026rsquo;t have internet to download it, how do you share it?\nYou will need to export the box from machine that has it downloaded already. So on machine with boxes:\n$ vagrant box list centos/7 (libvirt, 1610.01) centos/7 (libvirt, 1704.01) fedora/25-cloud-base (libvirt, 20161122) fedora/26-cloud-base (libvirt, 20170705) I wanted to share fedora/26-cloud-base box to another machine. Run the following command:\n$ vagrant box repackage fedora/26-cloud-base libvirt 20170705 If you deconstruct the above command you will find that the input to above command is all seen in output of command vagrant box list, where fedora/26-cloud-base is the box name \u0026amp; libvirt is the provider \u0026amp; 20170705 is the version.\nOr find more help running:\n$ vagrant box repackage -h Usage: vagrant box repackage \u0026lt;name\u0026gt; \u0026lt;provider\u0026gt; \u0026lt;version\u0026gt; Above command creates the file named package.box.\nNow transfer this box file to another machine and there you will have to import this box. Over there run command:\n$ vagrant box add fedora/26-cloud-base package.box ==\u0026gt; box: Box file was not detected as metadata. Adding it directly... ==\u0026gt; box: Adding box \u0026#39;fedora/26-cloud-base\u0026#39; (v0) for provider: box: Unpacking necessary files from: file:///home/foobar/package.box ==\u0026gt; box: Successfully added box \u0026#39;fedora/26-cloud-base\u0026#39; (v0) for \u0026#39;libvirt\u0026#39;! Now you can list the boxes and use it on this new machine.\nHere is a list of Vagrantfiles I use for all my local setup.\n",
    "ref": "/post/move-vagrant-boxes/"
  },{
    "title": "Add new Maps to Counter Strike",
    "date": "",
    "description": "Steps to add new maps to CS",
    "body": "If you are playing Counter Strike on Steam on linux you might be wondering how do I install new maps in the game. So here are steps to install maps.\nAll the game maps for Counter Strike - Condition Zero are stored in following path:\n~/.local/share/Steam/steamapps/common/Half-Life/czero/maps Format of the map files is .bsp.\nSo download the map files from over internet and then put in this location and now you should be able to see the maps in game.\nHappy Gaming, fire in the hole :-) !!\n",
    "ref": "/post/add-new-map-to-cs/"
  },{
    "title": "List of Kubernetes Bangalore meetup event report",
    "date": "",
    "description": "List of all the Kubernetes and OpenShift meetup event reports",
    "body": "List of all the event reports from Kubernetes Bangalore meetup.\n April 2017 May 2017 June 2017 July 2017 Kubernetes 2nd Birthday Celebration August 2017 September 2017 October 2017 November 2017  ",
    "ref": "/post/list-all-k8s-meetups/"
  },{
    "title": "Kubernetes Learning resources",
    "date": "",
    "description": "Places you can find learning material for Kubernetes",
    "body": "Following is the list of all the places you can learn Kubernetes from:\n Scalable Microservices with Kubernetes - Video tutorial Fundamentals of Containers, Kubernetes, and Red Hat OpenShift - Video tutorial Kubernetes By Example - DIY tutorial Learn Kubernetes using Interactive Browser-Based Scenarios - DIY tutorial in your own web browser Interactive Learning Portal for OpenShift - DIY tutorial in your own web browser Kubernetes docs - Textual DIY docs Kubernetes API reference v1.6 OpenShift docs - Textual DIY docs  ",
    "ref": "/post/k8s-learning-resources/"
  },{
    "title": "Bangalore Kubernetes Meetup July 2017",
    "date": "",
    "description": "Kubernetes Meetup presentation and talks",
    "body": "This edition of meetup was held at Nexus Ventures by folks at OpenEBS on July 8th 2017, which started on a lovely Saturday morning.\nKiran Mova set the floor rolling with his talk on Hyperconverged version of OpenEBS with Kubernetes. Where he talked about containerized storage vs traditional storage, instead of building clustering into OpenEBS how they are leveraging Kubernetes\u0026rsquo;s capabilities to do clustering.\nHe also explained difference between various storage providers viz. Portworx, StorageOS, Rook, GlusterFS, OpenEBS, etc.\nThen was talk by Kamesh Sampath on Istio named A sail in the cloud. Specifically tracing, monitoring, service discovery with Istio.\nHe explained the Istio Service Mesh architecture and all components it has. How side-car pattern is being used to deploy Istio with the existing apps in Kubernetes.\nHe talked and demoed about Canary deployment, tracing and Circuit breaking with Istio. Watch the video recording of talk by Kamesh to learn in detail. Here is his demo content.\nAfter a snacks break, there was a talk by Zeeshan Ahmed about Dance of container image building. The main highlight of this highly interactive session was on best practices to be followed for building container images.\nWhere he explained why it is not a good idea to bake configs and secrets into container image and how one can use use Kubernetes secrets and configMaps to get this configs and secrets in the container on the fly, he then demoed the tool buildah where you don\u0026rsquo;t need a docker daemon to build the container image.\nFollowed by Zeeshan was talk by Saravanakumar where he talked about Source to Image in OpenShift, where he explained what is s2i, how you can use s2i with the running OpenShift cluster by directly providing github url of a project and see it deployed on OpenShift.\nThen he demonstrated how you can build your own builder image for OpenShift s2i. Link to slides.\nThe meetup ended with announcements of Kubernetes Birthday meetup and awesome burgers from Truffles sponsored by OpenEBS. Thanks all speakers, attendees and sponsors for making this meetup a success. Thanks to OpenEBS for being a generous sponsors, and specifically Kiran and Nisanta for organizing logistics and helping on the day of meetup. Thanks to Hemani and Zeeshan for editing this post.\n",
    "ref": "/post/blr-k8s-meetup-july-2017/"
  },{
    "title": "Clean Node setup",
    "date": "",
    "description": "This will help in intalling node without sudo",
    "body": "Make sure you have npm installed.\n$ sudo dnf -y install npm Package npm-1:3.10.10-1.6.10.3.1.fc25.x86_64 is already installed, skipping. Dependencies resolved. Nothing to do. Complete! Taken from this post.\nmkdir \u0026#34;${HOME}/.npm-packages\u0026#34; echo \u0026#39;prefix=${HOME}/.npm-packages\u0026#39; | tee -a ~/.npmrc echo \u0026#39; #====================================== # npm related stuff NPM_PACKAGES=\u0026#34;${HOME}/.npm-packages\u0026#34; PATH=\u0026#34;$NPM_PACKAGES/bin:$PATH\u0026#34; # Unset manpath so we can inherit from /etc/manpath via the `manpath` command unset MANPATH # delete if you already modified MANPATH elsewhere in your config export MANPATH=\u0026#34;$NPM_PACKAGES/share/man:$(manpath)\u0026#34; #====================================== \u0026#39; | tee -a ~/.bashrc Ref:  npm throws error without sudo - Stack Overflow question Install npm packages globally without sudo on macOS and Linux  ",
    "ref": "/post/node-no-sudo/"
  },{
    "title": "Bangalore Kubernetes Meetup May 2017",
    "date": "",
    "description": "Kubernetes and OpenShift 101 hands-on workshop",
    "body": "\u0026ldquo;One does not simply deploy containers to production\u0026rdquo;\nWith the rising craze around the container community in Bangalore and relative lack in awareness around different container technologies like Kubernetes and OpenShift, an effort was made in imparting knowledge in this direction.\nSo, this time around newbies were targeted for the Kubernetes Meetup.\nWith the above objective, it was decided to have a Kubernetes 101 workshop at Red Hat Bangalore office on May 21, 2017 to familiarize people with concepts of Kubernetes and OpenShift and their usage and relevance as container orchestration tools for managing application deployments.\nThe schedule for the Meetup was as follows:\n Kubernetes 101 - Knowing the terms in Kubernetes Kubernetes 101 - Hands-on workshop OpenShift 101 - What OpenShift adds to Kubernetes? OpenShift 101 - Hands-on workshop  As per the schedule, we opened the floor with a talk by Hemani Katyal on Kubernetes Basics(slides), wherein she explained the need for Kubernetes, what it is, it\u0026rsquo;s architecture, why Kubernetes would be better container orchestrator and the constructs of Kubernetes like pods, volumes, labels, replications controllers, services, etc.\nIt was good to see people interested in concepts of service and the way it load balances traffic, how the labels construct is helpful in bringing in flexibility with deployments, why Kubernetes is better than other container orchestrators.\nFollowing up with the excitement, Suraj Deshmukh, Abhishek Singh, Zeeshan Ahmed and Shubham Minglani happily volunteered to conduct the hands-on workshop. The workshop exercises were taken from Kubernetes section of katacoda.com.\nIt was pleasure to have an excited and responsive audience. Questions like kube-adm functionality on CentOS, installing Kubernetes in an isolated environment without internet, how ingress works and the likes of the same were asked during the hands-on session.\nPost a quick coffee break, Abhishek Singh briefed the audience about OpenShift basics(slides), wherein he explained what OpenShift is and how it is an addon on top of Kubernetes. He touched upon topics like user management, various build features of OpenShift, container image handling in OpenShift, the way deployment fits in with build part, security, running OpenShift locally using Minishift, to name a few.\nSatiating the hunger not just for knowledge but food as well, we broke for lunch.\nPost which we continued with OpenShift hand-on exercises, which were again from OpenShift section of katacoda.com. Zeeshan Ahmed, Suraj Narwade and Abhishek Singh helped with the workshop exercises. There seemed to be lot of interest around topics like the source to image feature of OpenShift, if OpenShift‚Äôs router allowed UDP traffic, the difference between ingress and routes.\nRed Hat‚Äôs latest announcement regarding the integration of AWS services with OpenShift also generated some curiosity among the audience.\nThe meetup concluded asking for feedback and distributing some swag. In feedback we received, people wanted to see some real world examples, the kind of applications that run on these platforms, and much more which are advanced topics and we would love to take up in normal meetup as opposed to a 101.\nI would like to conclude here by thanking all speakers, volunteers for spreading knowledge, katacoda.com for awesome workshop material and audience for listening patiently and trying out things and making this meetup successful. And last but not the least, I would like to express my deepest gratitude to Red Hat for sponsoring the event and providing us with the venue and Hemani Katyal for making this post an awesome read.\nLinks  Kubernetes DIY hands-on exercises https://www.katacoda.com/courses/kubernetes OpenShift DIY hands-on exercises https://learn.openshift.com/  ",
    "ref": "/post/blr-k8s-meetup-may-2017/"
  },{
    "title": "Enabling local development with Kubernetes",
    "date": "",
    "description": "If you are doing development and want to use kubernetes for it, then here is how you can do it.",
    "body": "I want to show how you can enable Kubernetes in your day to day development workflow. So that you get the feel of production deployment locally from day 1.\nI have a flask application which I am working on. The basic directory structure looks like this:\n$ ll total 24 -rw-rw-r--. 1 foo foo 427 Apr 23 16:23 app.py -rw-rw-r--. 1 foo foo 201 Apr 23 16:55 docker-compose.yml -rw-rw-r--. 1 foo foo 363 Apr 23 16:21 Dockerfile -rwxrwxr-x. 1 foo foo 82 Dec 5 19:41 entrypoint.sh -rw-rw-r--. 1 foo foo 3010 Dec 5 19:41 README.adoc -rw-rw-r--. 1 foo foo 11 Dec 5 19:41 requirements.txt You can find all of these files in this github repo.\nFor having a local cluster I am using minikube. So follow instructions to setup minikube. Once you follow the instructions you will have a vm running a single node kubernetes cluster and a locally available kubectl binary.\nBefore running this application on the minikube cluster we need configurations that kubernetes understands. Since we already have docker-compose file we will generate configs from this file with the help from tool called kompose. Install kompose as per instructions as given on docs.\nGenerating configs:\n$ mkdir configs $ kompose convert -o configs/ WARN Kubernetes provider doesnt support build key - ignoring INFO file \u0026#34;configs/hitcounter-service.yaml\u0026#34; created INFO file \u0026#34;configs/redis-service.yaml\u0026#34; created INFO file \u0026#34;configs/hitcounter-deployment.yaml\u0026#34; created INFO file \u0026#34;configs/redis-deployment.yaml\u0026#34; created Before we deploy the app we need to make some changes in the deployment files, that were converted from docker-compose service having build construct in them. In our case only python app hitcounter is built is being built from Dockerfile.\nWe will edit file hitcounter-deployment.yaml in configs directory, to not pull image but read image from the local docker storage. Add a field after image called imagePullPolicy: IfNotPresent. Make changes as shown in following diff:\n$ git diff diff --git a/configs/hitcounter-deployment.yaml b/configs/hitcounter-deployment.yaml index 7b1116d..0ef35b3 100644 --- a/configs/hitcounter-deployment.yaml +++ b/configs/hitcounter-deployment.yaml @@ -17,6 +17,7 @@ spec:  - name: REDIS_HOST value: redis image: hitcounter + imagePullPolicy: IfNotPresent  name: hitcounter ports: - containerPort: 5000 Now we are ready with the configs, but we need to build container image for our app. So here you will need to have docker-compose installed on your machine. For that follow docs here.\nBuild image in the minikube\neval $(minikube docker-env) docker-compose build Once the build is complete, we are good to the deployment in kubernetes.\n$ kubectl create -f configs/ deployment \u0026#34;hitcounter\u0026#34; created service \u0026#34;hitcounter\u0026#34; created deployment \u0026#34;redis\u0026#34; created service \u0026#34;redis\u0026#34; created To verify that the app is running, find out the exposed IP Address as follows:\n$ kubectl get svc NAME CLUSTER-IP EXTERNAL-IP PORT(S) AGE hitcounter 10.0.0.244 \u0026lt;pending\u0026gt; 5000:30476/TCP 6s kubernetes 10.0.0.1 \u0026lt;none\u0026gt; 443/TCP 3d redis 10.0.0.21 \u0026lt;none\u0026gt; 6379/TCP 6s Now hit the externally exposed port 30476 of service hitcounter as:\n$ curl $(minikube ip):30476 Now everytime you make changes to code do the following:\ndocker-compose build kubectl scale deployment hitcounter --replicas=0 kubectl scale deployment hitcounter --replicas=1 Above we are removing all containers with old image and asking it to use the new image. For OpenShift we can do oc deploy hitcounter --latest and it will trigger the deployment but I could not find anything similar with kubernetes.\nFAQ   - Why do I need to make changes in the kompose generated configs?\nBecause by the default the config that kompose generates will not set imagePullPolicy and hence Kubernetes assumes its value to be Always. So if you don\u0026rsquo;t make changes and try to deploy then Kubernetes will try to find the image from docker hub. Which it won\u0026rsquo;t find and then that deployment will fail. So we need to tell Kubernetes to look for the image in local docker storage.\n  - Can I use the same configs in the production servers as well?\nYes you can use it just remove the change we did in the imagePullPolicy: IfNotPresent. The change is done to enable you to use the locally built images without having to push the image to any container registry.\n  - How do I get images when I am deploying in production level cluster?\nMake sure your cluster can pull images from some private container registry. And then set up a build pipeline from your code repo to build container on every change of it\u0026rsquo;s stable branch.\n  - I have build defined in my docker-compose service why do I need to mention image name?\nWith docker-compose this is okay. But kompose cannot make up a name on it\u0026rsquo;s own and create deployment. The issue is tracked in kompose. But for now with build also provide the image name you would expect.\n  - I get error running docker commands with minikube?\nIf you face problem accessing the docker daemon running inside the minikube VM like one of this\n$ eval $(minikube docker-env) $ docker ps could not read CA certificate \u0026#34;/etc/docker/ca.pem\u0026#34;: open /etc/docker/ca.pem: no such file or directory This could be because there is a mismatch in docker client and docker daemon version, so to solve this issue just copy the docker client from the minikube VM to the local machine.\nEnter in the VM\nminikube ssh Copy the binary to host machine\nscp $(which docker) foo@192.168.122.1:/home/foo/ Now put the binary in PATH.\n  If you have any other questions please ask it, I would like to add those here in FAQ section.\n",
    "ref": "/post/enabling-local-development-with-k8s/"
  },{
    "title": "Quick PV for local Kubernetes cluster",
    "date": "",
    "description": "A hostPath based local PV creation process for using via PVC",
    "body": "I do lot of Kubernetes related work either on minikube or local OpenShift cluster setup in a VM. Often I need to create a PersistentVolumeClaim a.k.a. pvc. But to use pvc you have to have a PersistentVolume or pv defined.\nEnter into the machine running k8s If using minikube you can do\nminikube ssh Create a local directory for storage mkdir /tmp/pv0001 chmod 777 /tmp/pv0001 If you are on a machine that has SELinux enabled do the following\nsudo chcon -R -t svirt_sandbox_file_t /tmp/pv0001 Creating pv Create file with following content\n$ cat pv.yaml apiVersion: \u0026quot;v1\u0026quot; kind: \u0026quot;PersistentVolume\u0026quot; metadata: name: \u0026quot;pv0001\u0026quot; spec: capacity: storage: \u0026quot;5Gi\u0026quot; accessModes: - \u0026quot;ReadWriteOnce\u0026quot; persistentVolumeReclaimPolicy: Recycle hostPath: path: /tmp/pv0001 Get to the terminal from where you can run kubectl commands.\nkubectl create -f pv.yaml If you are doing it for OpenShift cluster then run following command with privileged access.\noc create -f pv.yaml There you have a pv now you can create pvc\u0026rsquo;s to use it.\n",
    "ref": "/post/quick-pv-for-local-k8s/"
  },{
    "title": "Bangalore Kubernetes Meetup April 2017",
    "date": "",
    "description": "",
    "body": "Like many Saturday mornings, Red Hat Bangalore office was once again abuzz with enthusiasm on 8th of April, for hosting yet another successful chapter of Bangalore Kubernetes Meetup. The Meetup had a good turnaround of about 40 people who gave up on their early morning saturday sleep to attend it despite the sweltering hot season and in line were four awesome talks.\nSuraj Deshmukh set the stage with his opening talk, Kubernetes on CRI-O, wherein he explained different jargons like OCI, CRI, etc., introduced CRI-O and it‚Äôs architecture. Explaining how it glues Kubernetes and OCI compliant runtimes and concluding it with a demo showing how Kubernetes uses CRI-O. Link to slides, video.\nNext in queue, was another wonderful talk by Dipak Pawar about System and integration testing for Kubernetes/OpenShift with arquillian-cube and JBoss Forge, elaborating the usefulness of arquillian-cube‚Äôs integration for Kubernetes and OpenShift coupled with Forge tooling for testing micro-services deployed on OpenShift/Kubernetes. Link to video.\nFollowing a quick breather, was an informative talk by Budhram Gurung on Running OpenShift locally using Minishift, highlighting the inspiration for the project and covering components of Minishift with a demo showing it\u0026rsquo;s usage. Also, leaving behind a note on how one could reach out to the developers and contribute to Minishift. Link to slides, video.\nLast, but not the least, planned talk was by Suraj Narwade on running Docker on ARM/Raspberry PI, wherein he enjoyed sharing his experience of working on his hobby project, and all the problems faced by him in trying to get containers running on Raspberry PI. He shared his discovery showcasing the incompatibility of the container images built on x86_64, to run on arm. Link to slides, video.\nFinally, an unanticipated lightening talk took the audience by storm where a quick 3 minutes demo on Persistent Storage with Kubernetes by Raghavendra Talur turned into a good discussion of 30 minutes. He demo-ed, hyper-converged storage and dynamic provisioning with gluster and answered a lot of queries regarding setting up storage with Kubernetes, finishing on a to be continued note in the successive meetup.\nThe meetup concluded with discussion over snacks and planning for the next chapter of this meetup, which will be a Kubernetes and OpenShift 101 workshop for beginners.\nThanks to all the Speakers for sharing their valuable thoughts and learnings, Baiju for recording all the sessions, Hemani for making this post a good read and Red Hat for the venue.\nThe meetup is also featured on K8sPort, which is community engagement platform for Kubernetes community, courtesy Ryan Quackenbush.\n",
    "ref": "/post/blr-k8s-meetup-april-2017/"
  },{
    "title": "k8s on CRI-O - single node",
    "date": "",
    "description": "How to make kubernetes use CRI-O as container runtime",
    "body": "Here is a single node Kubernetes on CRI-O. This setup is done on Fedora 25.\nInstalling OS dependencies dnf -y install \\  go \\  git \\  btrfs-progs-devel \\  device-mapper-devel \\  glib2-devel \\  glibc-devel \\  glibc-static \\  gpgme-devel \\  libassuan-devel \\  libgpg-error-devel \\  libseccomp-devel \\  libselinux-devel \\  pkgconfig \\  wget \\  etcd \\  iptables Creating go environment cd ~ mkdir -p ~/go export GOPATH=~/go export GOBIN=$GOPATH/bin export PATH=$PATH:$GOBIN echo \u0026#39;GOPATH=~/go\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;GOBIN=$GOPATH/bin\u0026#39; \u0026gt;\u0026gt; ~/.bashrc echo \u0026#39;PATH=$PATH:$GOBIN\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Pull all the code dependencies go get -d k8s.io/kubernetes go get -u github.com/cloudflare/cfssl/cmd/... Install runc go get -d github.com/opencontainers/runc cd $GOPATH/src/github.com/opencontainers/runc git reset --hard v1.0.0-rc3 make BUILDTAGS=\u0026#39;seccomp selinux\u0026#39; \u0026amp;\u0026amp; make install Build cri-o cd go get -d github.com/kubernetes-incubator/cri-o cd $GOPATH/src/github.com/kubernetes-incubator/cri-o make install.tools make \u0026amp;\u0026amp; make install make install.config Set up CNI go get -d github.com/containernetworking/cni cd $GOPATH/src/github.com/containernetworking/cni ./build.sh mkdir -p /opt/cni/bin cp bin/* /opt/cni/bin/ mkdir -p /etc/cni/net.d/ cat \u0026gt; /etc/cni/net.d/10-ocid-bridge.conf \u0026lt;\u0026lt;EOF { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;name\u0026#34;: \u0026#34;ocid-bridge\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;bridge\u0026#34;, \u0026#34;bridge\u0026#34;: \u0026#34;cni0\u0026#34;, \u0026#34;isGateway\u0026#34;: true, \u0026#34;ipMasq\u0026#34;: true, \u0026#34;ipam\u0026#34;: { \u0026#34;type\u0026#34;: \u0026#34;host-local\u0026#34;, \u0026#34;subnet\u0026#34;: \u0026#34;10.88.0.0/16\u0026#34;, \u0026#34;routes\u0026#34;: [ { \u0026#34;dst\u0026#34;: \u0026#34;0.0.0.0/0\u0026#34; } ] } } EOF cat \u0026gt; /etc/cni/net.d/99-loopback.conf \u0026lt;\u0026lt;EOF { \u0026#34;cniVersion\u0026#34;: \u0026#34;0.2.0\u0026#34;, \u0026#34;type\u0026#34;: \u0026#34;loopback\u0026#34; } EOF Create policy.json mkdir -p /etc/containers cat \u0026gt; /etc/containers/policy.json \u0026lt;\u0026lt;EOF { \u0026#34;default\u0026#34;: [ { \u0026#34;type\u0026#34;: \u0026#34;insecureAcceptAnything\u0026#34; } ] } EOF Make SELinux happy mkdir -p /var/lib/containers/ chcon -Rt svirt_sandbox_file_t /var/lib/containers/ Start ocid service export PATH=$PATH:/usr/local/bin/ echo \u0026#39;PATH=$PATH:/usr/local/bin/\u0026#39; \u0026gt;\u0026gt; ~/.bashrc ocid --runtime /usr/local/sbin/runc --log /root/ocid.log --debug --selinux true Start k8s cluster with crio cd $GOPATH/src/k8s.io/kubernetes/ CONTAINER_RUNTIME=remote CONTAINER_RUNTIME_ENDPOINT=\u0026#39;/var/run/ocid.sock --runtime-request-timeout=15m\u0026#39; ./hack/local-up-cluster.sh To use kubectl (in new terminal)\nalias kubectl=$GOPATH/src/k8s.io/kubernetes/cluster/kubectl.sh echo \u0026#39;alias kubectl=$GOPATH/src/k8s.io/kubernetes/cluster/kubectl.sh\u0026#39; \u0026gt;\u0026gt; ~/.bashrc Ref:\n Bangalore Kubernetes Meetup - April 2017 - Slides. runcom\u0026rsquo;s Setup script for Fedora. cri-o project cri-o tutorial Running cri-o on kubernetes cluster CRI-O: A kubernetes runtime - video  ",
    "ref": "/post/using-crio-with-k8s-single-node/"
  },{
    "title": "TODOs for new project",
    "date": "",
    "description": "Things to do to make a project successful",
    "body": "When starting a new open source project, apart from coding following are the things that need to be done.\nDocs  README LICENSE How to install? How to use? Code of conduct guidelines Copyright Issue template like .github in repo.  Developer docs  How to install for developer? Developer docs Contribution guidelines Team meetings info Team communication channels Issue label guidelines Coding standards Code placement doc, or something that explains the way code is ordered Design document  Development  Makefile Unit tests Functional tests Coverage check CI for automatic tests Distro based package  This is just few things that I could think of, please suggest if you know more in comments.\n",
    "ref": "/post/starting-a-new-opensource-project-requirements/"
  },{
    "title": "rpm Notes",
    "date": "",
    "description": "General notes about rpm packaging and references to upstream docs",
    "body": "This post will get you through all the steps needed for doing RPM packaging.\nSetup of the system for building rpms $ dnf -y install fedora-packager fedora-review $ sudo usermod -a -G mock vagrant $ fedora-packager-setup $ kinit surajd@FEDORAPROJECT.ORG My Notes  Start reading from: Fedora packager\u0026rsquo;s guide Some macros come from redhat-rpm-config and fedora-rpm-macros.  $ sudo rpm -ql redhat-rpm-config-45-1.fc25.noarch  To see all macros on the system:  $ rpm --showrc  Koji - fedora build system fedora uses fedpkg for doing builds, while rpmbuild is for CentOS To get general info about the package  $ rpm -qip ./x86_64/namaskar-1-1.fc25.x86_64.rpm OR\n$ less namaskar-1-1.fc25.src.rpm  To see what are the dependencies of the package  $ rpm -qp --requires ./noarch/namaskar-1-1.fc25.noarch.rpm /bin/bash rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 rpmlib(FileDigests) \u0026lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 rpmlib(PayloadIsXz) \u0026lt;= 5.2-1  To see what packages include your package as dependency  $ rpm -qp --provides ./noarch/namaskar-1-1.fc25.noarch.rpm namaskar = 1-1.fc25  After you think the spec is ready and before you push it to build  $ fedpkg --release f25 lint  Doing local mock build  $ fedpkg --release f25 mockbuild  Doing build in fedora build system, koji  $ fedpkg --release f25 scratch-build --target f25 --srpm  Running fedora-review  $ fedpkg --release f25 srpm $ fedora-review -n namaskar  Decompress a package, src.  $ rpm2cpio kompose-0.1.0-0.1.git8227684.el7.src.rpm | cpio -ivdm OR\n$ rpmdev-extract kompose-0.4.0-0.1.git4e3300c.fc27.x86_64.rpm  Various ways to query package: src. To download source as mentioned in the Source0 tag, use:  $ spectool -g kompose.spec  To validate the rpmspec if it is error free  $ rpmlint kompose.spec Ref:  Yum and RPM Tricks https://wiki.centos.org/TipsAndTricks/YumAndRPM Creating RPM Packages with Fedora https://fedoraproject.org/wiki/How_to_create_an_RPM_package Inside the Spec File, Directives For the %files list http://ftp.rpm.org/max-rpm/s1-rpm-inside-files-list-directives.html How to get sponsored into the packager group https://fedoraproject.org/wiki/How_to_get_sponsored_into_the_packager_group Doing local builds help from adb-utils repo Using Mock to test package builds https://fedoraproject.org/wiki/Using_Mock_to_test_package_builds Packaging:RPMMacros https://fedoraproject.org/wiki/Packaging:RPMMacros?rd=Packaging/RPMMacros Tweet about using rpmdev-extract  ",
    "ref": "/post/rpm-notes/"
  },{
    "title": "Git Notes",
    "date": "",
    "description": "Contains notes about using git",
    "body": "Notes about using git.\nTips and tricks  Switch branches $ git checkout \u0026lt;branch\u0026gt;  status $ git status -sb Show status in short format and also give branch info\n show $ git show Shows log message and diff about the commit you are on.\n log $ git log -L 70,100:pkg/transformer/kubernetes/kubernetes.go Get logs on file between line numbers.\n$ git log --graph --abbrev-commit Show graph in logs.\n commit $ git add -p Commit only parts of file. Interactively choose chunks of patch.\n blame To see who wrote the code? For each line of the file what commit edited that line of code will be shown. So now you can use that git commit and pass it to git show \u0026lt;commit\u0026gt; to see all the changes.\n$ git blame path/to/file  cherry-pick To move a commit from one branch to another branch. Situation where: I committed to master when I meant to commit to my feature branch. I need to move my commit!\nGet the commit hash\n$ git show Change to the branch you wanted to add that commit\n$ git checkout \u0026lt;feature-branch\u0026gt; Add it to the branch you are on\n$ git cherry-pick \u0026lt;commit hash\u0026gt; cherry-pick creates an entirely new commit based off the original, and it does not delete the original commit. So you will have to delete it manually. See below how to do it.\nYou can also get conflict during cherry-pick\n$ git cherry-pick 435bedfa Resolve the conflict and then\n$ git cherry-pick --continue  reset Remove the last commit.\n$ git reset --hard HEAD^ HEAD : the commit I\u0026rsquo;m currently sitting on\nHEAD^ : this commit\u0026rsquo;s parent\nHEAD^^: this commit\u0026rsquo;s grandparent and so on\nRemove the changes that were accidentally added and not comitted.\n$ git reset HEAD If you don\u0026rsquo;t want to have the uncommitted changes.\n$ git reset --hard HEAD  rebase  rebase is a command for changing history. Never change history when other people might be using your branch, unless they know you\u0026rsquo;re doing so. Never change history on master. Best practice: only change history for commits that have not yet been pushed.  $ git checkout master $ git pull --ff upstream master $ git checkout \u0026lt;feature-branch\u0026gt; $ git rebase master -i While pushing need to do force push because there is change of history. Local branch and remote branch have diverged.\n$ git push origin \u0026lt;feature-branch\u0026gt; -f In case of conflicts, find the conflicting file.\n$ git status reolve those conflicts and then continue the rebase\n$ git status $ git rebase --continue  Use the same commit message Use the commit message that was generated automatically\ngit merge --no-edit OR\ngit commit --amend --no-edit  Squashing commits  Amending the commit  $ git add missing-file $ git commit --amend  Squashing  Look at last 5 commits. Below command will open the text editor.\n$ git rebase --interactive HEAD~5 Once in editor, you can select which ones to squash into previous one and ones to pick as it is. Now type new commit message to squashed commits.\n Splitting commits $ git rebase --i HEAD~3 Now this will open the commit history in editor. The commit you want to split, change it from pick to edit. Save that file. Git will pause in the rebase process and give us time to create new commits. The too-big commit is already present, so let\u0026rsquo;s pop it off, but keep the changes:\n$ git reset HEAD^ Not using --hard because we want to have the changes we wanted. Make changes as needed. Now add individual file and commit. And continue rebase.\n$ git rebase --continue  Find no. of commits on your branch If you have worked on a branch and have added bunch of commits to it, so how do you find out how many commits you have added?\n$ git rev-list f28adfba5ec4f1b02153e9dcc0298ace118ca9d9..HEAD 5f3693549451429beb7ca17699d83a45f7d8ab49 a50cc4d9fe4d6feb3bb150f9762bad643adbacf0 This is two commits, you can pipe it to wc to find out the number.\n bisect The feature\u0026rsquo;s broken? But it was working fine 2 months ago\u0026hellip; what changes? Bisect will help you find the commit that introduced the problem.\n Need commit where it was working, commit where it\u0026rsquo;s broken and a test to verify that.  $ git bisect start $ git checkout broken-commit $ git bisect bad $ git checkout working-commit $ git bisect good  Auto-correct mis-types in commands $ git config --global help.autocorrect 10  Edit git output colors Set various colors to the git logs and all the git output\n$ git config --global color.ui auto  Git merge from someone else\u0026rsquo;s fork Add their github fork repo as a remote to a clone of your own repo:\n$ git remote add other-guys-repo \u0026lt;url to other guys repo\u0026gt; Get their changes:\n$ git fetch other-guys-repo Checkout the branch where you want to merge:\n$ git checkout my_new_branch Merge their changes in (assuming they did their work on the master branch):\n$ git merge other-guys-repo/master Resolve conflicts, commit the resolutions and voila. Quick Ref: http://stackoverflow.com/a/5606062\n stash Save the changes and clean the tree.\ngit stash Use git stash when you want to record the current state of the working directory and the index, but want to go back to a clean working directory. The command saves your local modifications away and reverts the working directory to match the HEAD commit.\ngit stash --include-untracked git config --global alias.staash \u0026#39;stash --include-untracked\u0026#39; If the --include-untracked option is used, all untracked files are also stashed and then cleaned up with git clean, leaving the working directory in a very clean state.\ngit stash --all git config --global alias.staaash \u0026#39;stash --all\u0026#39; If the --all option is used instead then the ignored files are stashed and cleaned in addition to the untracked files.\n How to pull remote branch from somebody else\u0026rsquo;s PR git remote add coworker git://path/to/coworkers/repo.git git fetch coworker git checkout --track coworker/foo This will setup a local branch foo, tracking the remote branch coworker/foo. So when your coworker has made some changes, you can easily pull them:\ngit checkout foo git pull Quick Ref: http://stackoverflow.com/a/5884825\nOR\nAdd following code snippet to ~/.bashrc, this will way you have alias to pull any PR.\nfunction pr() { id=$1 if [ -z $id ]; then echo \u0026#34;Need Pull request number as argument\u0026#34; return 1 fi git fetch upstream pull/${id}/head:pr_${id} git checkout pr_${id} } Usage\npr \u0026lt;pr_number\u0026gt;  Tips on writing git commits  How to Write a Git Commit Message Conventional Commits 1.0.0-beta.1   Tools   Commitizen - Helps in writing commits\n  Bash Git Prompt\nInstall using following:\ncd \u0026amp;\u0026amp; git clone https://github.com/magicmonty/bash-git-prompt.git .bash-git-prompt echo \u0026#39; #====================================== # bash git prompt source ~/.bash-git-prompt/gitprompt.sh GIT_PROMPT_ONLY_IN_REPO=1 #====================================== \u0026#39; | tee -a ~/.bashrc    Github These are tips about using github.com\n  allow edits from maintainers\nThis will help so that maintainers can push on your branch. On the PR at bottom right corner there is a check box to enable that.\n  Patch from PR\nIf you want a patch/diff of changes in a PR just goto PR and at the end of the url put .patch and you will see formatted patch. e.g. goto PR and now the patch\n  Compare ranges\ngoto https://github.com/\u0026lt;org\u0026gt;/\u0026lt;project\u0026gt;/compare/\u0026lt;old_version\u0026gt;...\u0026lt;new_version\u0026gt;\nOR\ngoto https://github.com/kubernetes-incubator/kompose/compare/v0.3.0...v0.4.0\nCompare things like branches, releases, etc.\n  Compare, patch ranges\ngoto https://github.com/\u0026lt;org\u0026gt;/\u0026lt;project\u0026gt;/compare/\u0026lt;branch\u0026gt;...\u0026lt;branch\u0026gt;.patch\nOR\ngoto https://github.com/kubernetes-incubator/kompose/compare/v0.3.0...v0.4.0.patch\n  Anchors on line numbers\nClick on the line number and shift click on another line later to select a block of code.\n  References and closing issues/PRs\nAlso you can add closes while merging the PR.\n  Code search\nrepo:kubernetes-incubator/kompose is:pr registry in:title registry is the string I am searching in the kubernetes-incubator/kompose repo, which has that string in PR in title.\nOR\nrepo:openshift/origin is:issue ubuntu When doing things on github you will find queries like these automatically generated.\n  Keyboard Shortcuts\n ? for all the shortcuts. Use t to search for files, fuzzy search, you need only file name not the full file path.  Others can be found using ?.\n  Gists as full repos\ngists can act as full repos\n  Embedding the gist\nAdd .pibb at the end of the gist link, you can use it on github pages and other places.\n  Short link to your github profile pic\nhttps://github.com/surajssd.png OR https://github.com/\u0026lt;github_username\u0026gt;.png\n  Short url with github\nGoto git.io and shorten any github url.\n  Blame\nOn any file in github, you can click the blame button and see who made what changes. After clicking on some specific commit, you can see the complete change and from the commit message goto PR for seeing all the discussion.\n  Global list of issues, PRs\nIn the top bar there are buttons for global list of issues and PRs. This can be a good todo list. In issues you can see issues you have created or assigned or mentioned.\n  Writing a very huge comment\n\u0026lt;details\u0026gt; write whatever here that needs to be hidden \u0026lt;/details\u0026gt;    Ref:  Don\u0026rsquo;t be afraid to commit Advanced Git - David Baumgold - video, slides Searching GitHub Tips \u0026amp; Tricks: Gotta Git Them All - GitHub Universe 2016, video. Everything I Wish I Knew When I Started Using GitHub - oscon Portland 2015, video. How to revert a ‚Äúgit rm -r .‚Äù? How to make ‚Äúspoiler‚Äù text in github wiki pages? 20 Tricks with Git and Shell, Spencer Krum - Git Merge 2016 Git Aliases of the Gods! - Git Merge 2017 Commits since a certain commit number  ",
    "ref": "/post/git-notes/"
  },{
    "title": "Intellij Shortcuts",
    "date": "",
    "description": "Shortcuts of intellij and also some short notes",
    "body": "Note: This is a living document and will be updated as I discover new things.\nShortcuts  Ctrl + Shift + A  Find any action in IDE\n Ctrl + Shift + F  Find in Path\n Alt + 1  Open project navigator. You can search here, just start typing here, after the project navigator window is opened.\n Shift + Insert in Project window  Here you can add new file to the project. The filename could be the entire path, so the intermediate directories will be created for you.\n Ctrl + N  Find a struct or class definition by name.\n Ctrl + Shift + N  Find a file by name. If you type file name and number seprated by number it will take you to the line in that file. e.g. main.go:6 This will take you to line 6 in main.go.\n Ctrl + Shift + Alt + N  Find any symbol in the project. Here symbol means mainly type.\n Shift + Shift  Search anything anywhere.\n Ctrl + F4  Close section that is active.\n Ctrl + Tab  Switcher to switch between open items\n Ctrl + Shift + B  Jump to definition.\n Ctrl + B  Find usage of that element in project. Or find declaration of a variable.\n Ctrl + Shift + P  Find type of a variable.\n Ctrl + cursor up/down  Navigate the code window up and down\n Ctrl + D  Duplicate a line or selection.\n Ctrl + Y  Delete a line or selection.\n Ctrl + /  Comment/Uncomment out with line comment, a line or selected block.\n Ctrl + Shift + /  Comment/Uncomment out with block comment, a line or selected block.\n Alt + Shift + cursor up/down  Move line or selection up/down.\n Alt + -\u0026gt; or Alt + \u0026lt;-  Switch tabs of open files.\n Ctrl + E  Show recently openend files.\n Ctrl + Shift + A \u0026lsquo;Scratch files\u0026rsquo;  If you need files that are not part of project but need to test something.\n Ctrl + Shift + F12  Hide all the windows and make editor full screen and press again to go back to having all the windows.\n View + Enter Distraction Free mode  This will only keep editor rest everything goes away.\n Ctrl + Shift + \u0026lt;- or Ctrl + Shift + -\u0026gt;  When in the project navigation window you can resize it.\n Ctrl + W  Select a word, press again and select a line, then select entire line then entire block, then entire outcasting block and so on.\n Shift + Alt + up/down cursor  When a line or block of code is selected and wanna move up and down.\n Ctrl + Shift + V  To see your clipboard history\n Alt + mouse click in location  To have multiple cursors. To get out of this mode just press escape key.\n Shift + F6  This will select all occurrences of the current word, so you can edit all of them at once. After you are done edition press enter.\n Alt + J  When pressed on a word, it finds the next occurrence of the word and puts a cursor there. So I can edit mutiple occurrences of same word simultaneously.\n Ctrl + Alt + I  Select a block of code and get indentation right.\n Ctrl + Alt + L  Reformat the code.\n Alt + Enter  Goto a string and press this key and then you can do language injection. So you can tell intellij to interpret that string as json or something like that. So this helps in adding markup languages as string as a part of code. Also you can inject regular expression and then check if the string is validated against the regular expressions you put in.\n Alt + /  So when a recommendation is opened up and if you press above key combination, the word is completed but if you keep pressing those keys the recommendation rotates through all the recommendation.\n Ctrl + J  Recommend templates, if there are any pre-defined.\n Ctrl + Shift + Enter  Auto complete the current statement.\n Ctrl + F12  Navigate file structure, you can start typing and search for things. And then by pressing the enter key you can jump to the definition.\n Alt + 7  Similar functionality as above only difference is that it opens in a side bar.\n Ctrl + Shift + A \u0026lsquo;annotate\u0026rsquo;  Start annotation to see who made changes on each line. From here you can copy the revision number and then search it in the \u0026lsquo;version control\u0026rsquo; window.\n Alt + F12  Terminal in IDE\n Alt + 9  Version Control window\nNotes and Tips  In code you can write TODOs and FIXME and then IDE can identify them and you can write code later on in there. There is a plugin called as \u0026lsquo;Presentation Assistant\u0026rsquo; which shows what keys did you type In the project window(side bar), select Auto Scroll from Source so when you are moving from one file to another it shows in the project side bar where that file is right now. When navigating in the project side bar, to start editing certain file press function key F4. Press Enter to just view it.  Other receipes Compare branches  Press Ctrl + Shift + A to open the action window, search for branches, open branches window. Also can be done as. Once branch pop-window is open, select with what branch you wanna compare current checked out branch and select compare. It will open up a window with all changes, in top tabs select diff, then select a file you wanna compare. Above steps are also given here.  Ref:  IntelliJ IDEA Tutorial - Shortcuts to Ditch Your Mouse Today 42 IntelliJ IDEA Tips and Tricks  ",
    "ref": "/post/intellij-shortcuts/"
  },{
    "title": "Packaging 'kompose' for centos paas sig",
    "date": "",
    "description": "",
    "body": "Note: This is a living document and will be updated from time to time.\nFollowing are steps to package kompose for CentOS PAAS SIG. CentOS PAAS SIG is a repository of packages where rpms related to OpenShift and eco-system around it are delivered.\nSetup your machine Install packages needed\nsudo yum update -y \u0026amp;\u0026amp; \\ sudo yum install -y epel-release \u0026amp;\u0026amp; \\ sudo yum install -y rpm-build go redhat-rpm-config make koji \\  gcc byobu rpmlint rpmdevtools centos-packager Setup certs\ncentos-cert -u surajd -n Make sure your rpmspec is error free rpmlint kompose.spec Building kompose srpm There are two ways to build srpm either build it locally or the ones that is built in koji for epel.\nBuild rpms locally Before you begin make sure you have setup the local directory structure:\nrm -rf ~/rpmbuild/ mkdir -p ~/rpmbuild/{BUILD,RPMS,SOURCES,SPECS,SRPMS} echo \u0026#39;%_topdir %(echo $HOME)/rpmbuild\u0026#39; \u0026gt; ~/.rpmmacros Update the rpm spec and get source code using it.\nspectool -g kompose.spec Move the source to appropriate location\ncp kompose-* ~/rpmbuild/SOURCES/ Start local build\nrpmbuild -ba kompose.spec --define \u0026#34;dist .el7\u0026#34; Once above exits with status code 0, you can find the RPMs:\n$ ll ~/rpmbuild/RPMS/x86_64/ total 9724 -rw-rw-r--. 1 vagrant vagrant 9956072 May 26 09:37 kompose-0.7.0-0.1.el7.x86_64.rpm SRPMs can be found at:\n$ ll ~/rpmbuild/SRPMS/ total 4828 -rw-rw-r--. 1 vagrant vagrant 4941880 May 26 09:37 kompose-0.7.0-0.1.el7.src.rpm See if dependencies are properly set\n$ rpm -qpR ~/rpmbuild/RPMS/x86_64/kompose-* git libc.so.6()(64bit) libc.so.6(GLIBC_2.2.5)(64bit) libpthread.so.0()(64bit) libpthread.so.0(GLIBC_2.2.5)(64bit) libpthread.so.0(GLIBC_2.3.2)(64bit) rpmlib(CompressedFileNames) \u0026lt;= 3.0.4-1 rpmlib(FileDigests) \u0026lt;= 4.6.0-1 rpmlib(PayloadFilesHavePrefix) \u0026lt;= 4.0-1 rpmlib(PayloadIsXz) \u0026lt;= 5.2-1 Try installing it locally and test it as mentioned in http://suraj.io/post/test-kompose/\nPull sprm For release 0.3.0, I pulled SRPM from:\nwget https://kojipkgs.fedoraproject.org//packages/kompose/0.3.0/0.1.git135165b.el7/src/kompose-0.3.0-0.1.git135165b.el7.src.rpm Build the rpm on cbs from src.rpm CBS is a community build system for SpecialInterestGroup members. It allows to build packages with Koji against CentOS5, CentOS6 and CentOS7.\nTrying a scratch build on CBS Do a scratch build on CBS in paas7-openshift-common-release.\ncbs build --scratch paas7-openshift-common-el7 ~/rpmbuild/SRPMS/kompose-* You can download the rpm built here to test on CentOS machine.\nMaking an actual release cbs build paas7-openshift-common-el7 ~/rpmbuild/SRPMS/kompose-* Once build is done successfully goto build page and download the rpm that is built for x86_64.\nThe page where builds were listed: https://cbs.centos.org/koji/taskinfo?taskID=181452 The page where this particular build happened and where I had download link to rpm: https://cbs.centos.org/koji/buildinfo?buildID=17249\nwget http://cbs.centos.org/kojifiles/packages/kompose/0.7.0/0.1.el7/x86_64/kompose-0.7.0-0.1.el7.x86_64.rpm Try to install this rpm and see if it works on CentOS:\nyum install -y epel-release yum install -y wget jq make wget http://cbs.centos.org/kojifiles/packages/kompose/0.7.0/0.1.el7/x86_64/kompose-0.7.0-0.1.el7.x86_64.rpm yum install -y kompose-0.7.0-0.1.el7.x86_64.rpm git clone https://github.com/kubernetes/kompose/ cd kompose git reset --hard $(kompose version | cut -d \u0026#34;(\u0026#34; -f2 | cut -d \u0026#34;)\u0026#34; -f1) make test-cmd If everything is okay, Tag it into testing, Verify that whatever you built last cbs is the good, the output should be version you wanted, and not the old one.\n$ cbs latest-build paas7-openshift-common-candidate kompose Build Tag Built by ---------------------------------------- -------------------- ---------------- kompose-0.7.0-0.1.el7 paas7-openshift-common-candidate surajd Tag the build output of above command to testing\n$ cbs tag-pkg paas7-openshift-common-testing kompose-0.7.0-0.1.el7 Created task 181472 Watching tasks (this may be safely interrupted)... 181472 tagBuild (noarch): closed 181472 tagBuild (noarch) completed successfully Verify it is in testing\n$ cbs latest-build paas7-openshift-common-testing kompose Build Tag Built by ---------------------------------------- -------------------- ---------------- kompose-0.7.0-0.1.el7 paas7-openshift-common-testing surajd Run whatever tests you want to verify that it\u0026rsquo;s a good build. It takes anywhere from 5 to 30 minutes for the rpm to make it into testing http://buildlogs.centos.org/centos/7/paas/x86_64/openshift-origin/\nyum -y install centos-release-openshift-origin yum -y --enablerepo=centos-openshift-origin-testing install kompose yum install -y epel-release yum install -y jq make git clone https://github.com/kubernetes/kompose/ cd kompose git reset --hard $(kompose version | cut -d \u0026#34;(\u0026#34; -f2 | cut -d \u0026#34;)\u0026#34; -f1) make test-cmd Check if the package is in testing\n$ cbs latest-build paas7-openshift-common-testing kompose Build Tag Built by ---------------------------------------- -------------------- ---------------- kompose-0.7.0-0.1.el7 paas7-openshift-common-testing surajd Tag it into release:\n$ cbs tag-pkg paas7-openshift-common-release kompose-0.7.0-0.1.el7 Created task 181634 Watching tasks (this may be safely interrupted)... 181634 tagBuild (noarch): free 181634 tagBuild (noarch): free -\u0026gt; closed 0 free 0 open 1 done 0 failed 181634 tagBuild (noarch) completed successfully Once it is populated, it will show up in the repos, install it as follows:\nyum install -y centos-release-openshift-origin yum install -y kompose Ref:  Install the SRPM and then Build from the Specfile https://wiki.centos.org/HowTos/RebuildSRPM Set Up an RPM Build Environment under CentOS https://wiki.centos.org/HowTos/SetupRpmBuildEnvironment Kompose build instructions CentOS PaaS SIG https://wiki.centos.org/SpecialInterestGroup/PaaS CentOS SIGs https://wiki.centos.org/SpecialInterestGroup CBS https://wiki.centos.org/HowTos/CommunityBuildSystem Building in CBS RPM help from adb-utils repo  ",
    "ref": "/post/packaging-kompose-for-centos-paas-sig/"
  },{
    "title": "Testing 'fedora' and 'CentOS' kompose package",
    "date": "",
    "description": "",
    "body": "I generally do kompose package testing for fedora and CentOS. So here are the steps I follow.\nFedora For respective fedora version use the tag respectively for e.g. 25 for fedora 25.\nStarting the environment:\ndocker run -it registry.fedoraproject.org/fedora:26 bash Running tests:\n# Inside the container # Pull packages from the testing repository dnf --enablerepo updates-testing -y install kompose # Check the kompose version kompose version # Install the testing dependencies dnf install -y jq make # Pull the git repository to run the functional tests git clone https://github.com/kubernetes/kompose/ cd kompose git reset --hard $(kompose version | cut -d \u0026#34;(\u0026#34; -f2 | cut -d \u0026#34;)\u0026#34; -f1) # Run cmd tests make test-cmd CentOS epel repo Spin the CentOS environment in container.\ndocker run -it centos bash Running tests:\n# Install kompose from \u0026#39;epel-testing\u0026#39; repo yum install -y epel-release yum --enablerepo=epel-testing -y install kompose # Install the testing dependencies yum install -y jq make # Pull the git repository to run the functional tests git clone https://github.com/kubernetes/kompose/ cd kompose git reset --hard $(kompose version | cut -d \u0026#34;(\u0026#34; -f2 | cut -d \u0026#34;)\u0026#34; -f1) # Run cmd tests make test-cmd CentOS paas7-openshift-common-el7 repo Spin the CentOS environment in container.\ndocker run -it centos bash Running tests:\n# For pulling package from testing repo in CentOS PAAS sig yum -y install centos-release-openshift-origin yum -y --enablerepo=centos-openshift-origin-testing install kompose # Install the testing dependencies yum install -y epel-release yum install -y jq make # Pull the git repository to run the functional tests git clone https://github.com/kubernetes/kompose/ cd kompose git reset --hard $(kompose version | cut -d \u0026#34;(\u0026#34; -f2 | cut -d \u0026#34;)\u0026#34; -f1) # Run cmd tests make test-cmd If all tests pass then just give a karma for it on the release page.\nRef:  Original article on Github Getting the latest fedora docker images Fedora docker hub page  ",
    "ref": "/post/test-kompose/"
  },{
    "title": "First post",
    "date": "",
    "description": "",
    "body": "This is the new home for my blog. I use to write on wordpress before. You can find the older blog at https://deshmukhsuraj.wordpress.com. But I wanted to have my own domain and wordpress was charging exorbitantly to connect a custom domain to wordpress, so I just ditched it and started writing here. This is much more convenient.\n",
    "ref": "/post/first-post/"
  },{
    "title": "About me",
    "date": "",
    "description": "",
    "body": " I am open source, systems software enthusiast. I contribute to various container \u0026amp; Kubernetes related technologies like Lokomotive, contour, rook, etc. I work for kinvolk and I am an ex-Red Hatter.  Contact  Github: surajssd Twitter: @surajd_ Email: surajd.service@gmail.com \u0026amp; suraj@kinvolk.com Stackoverflow: surajd  ",
    "ref": "/about/"
  }]
