<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>security on Suraj Deshmukh</title>
    <link>https://suraj.io/tags/security/</link>
    <description>Recent content in security on Suraj Deshmukh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sat, 08 May 2021 12:17:07 +0530</lastBuildDate><atom:link href="https://suraj.io/tags/security/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Access Any Kubernetes Secret</title>
      <link>https://suraj.io/post/2021/05/access-k8s-secrets/</link>
      <pubDate>Sat, 08 May 2021 12:17:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/2021/05/access-k8s-secrets/</guid>
      <description>Photo by Kyle Glenn on Unsplash.
You can gain access to any secret that you want in Kubernetes even if you don&amp;rsquo;t have RBAC permissions to get, list or view that secret. All you need is permission that allows you to do anything on pods and an ability to guess the names of secrets. With these two ingredients, here is how you can access any secret out there.
Nasty User Here is a user called nastyuser who can only do stuff on pod objects.</description>
    </item>
    
    <item>
      <title>Mental models for understanding Kubernetes Pod Security Policy</title>
      <link>https://suraj.io/post/mental-models-for-understanding-kubernetes-pod-security-policy/</link>
      <pubDate>Sat, 16 Jan 2021 13:10:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/mental-models-for-understanding-kubernetes-pod-security-policy/</guid>
      <description>PodSecurityPolicy (PSP) is hard to get right in the first attempt. There has never been a situation when I haven&amp;rsquo;t banged my head to get it working on the cluster. It is a frustrating experience, but it is one of the essential security features of Kubernetes. Some applications have started shipping the PSP configs with their helm charts, but if a helm chart does not ship a PSP config, it must be handcrafted by the cluster-admin to make the application work reliably in the cluster.</description>
    </item>
    
    <item>
      <title>Watch Container Traffic Without Exec</title>
      <link>https://suraj.io/post/snoop-on-pod-traffic/</link>
      <pubDate>Sat, 06 Jun 2020 20:30:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/snoop-on-pod-traffic/</guid>
      <description>Introduction For the reasons of security, many container deployments nowadays run their workloads in a scratch based image. This form of implementation helps reduce the attack surface since there is no shell to gain access to, especially if someone were to break out of the application.
But for the developers or operators of such applications, it is hard to debug. Since they lack essential tools or even bash for that matter, but the application&amp;rsquo;s debugging ability should not dictate its production deployment and compromise its security posture.</description>
    </item>
    
    <item>
      <title>Enabling Seccomp on your Prometheus Operator and related Pods</title>
      <link>https://suraj.io/post/seccomp-in-kube-state-metrics/</link>
      <pubDate>Tue, 14 Apr 2020 11:57:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/seccomp-in-kube-state-metrics/</guid>
      <description>Seccomp helps us limit the system calls the process inside container can make. And PodSecurityPolicy is the way to enable it on pods in Kubernetes.
Prometheus Operator Prometheus Operator makes it really easy to monitor your Kubernetes cluster. To deploy this behemoth, helm chart is the easiest way to do it.
Almost all the pods that run as a part of Prometheus Operator viz. Prometheus Operator, Prometheus, Alertmanager, Grafana, Kube State Metrics donâ€™t need to run with elevated privileges except Node Exporter.</description>
    </item>
    
    <item>
      <title>Capabilities on executables</title>
      <link>https://suraj.io/post/linux-cap-note/</link>
      <pubDate>Tue, 25 Jun 2019 15:15:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/linux-cap-note/</guid>
      <description>File capabilities allow users to execute programs with higher privileges. Best example is network utility ping.
A ping binary has capabilities CAP_NET_ADMIN and CAP_NET_RAW. A normal user doesn&amp;rsquo;t have CAP_NET_ADMIN privilege, since the executable file ping has that capability you can run it.
$ getcap `which ping` /usr/bin/ping = cap_net_admin,cap_net_raw+p Which normally works as follows:
$ ping -c 1 1.1.1.1 PING 1.1.1.1 (1.1.1.1) 56(84) bytes of data. 64 bytes from 1.</description>
    </item>
    
    <item>
      <title>Root user inside container is root on the host</title>
      <link>https://suraj.io/post/root-in-container-root-on-host/</link>
      <pubDate>Tue, 25 Jun 2019 11:57:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/root-in-container-root-on-host/</guid>
      <description>Here are simple steps that you can follow to prove that the root user inside container is also root on the host. And how to mitigate this.
Root in container, root on host I have a host with docker daemon running on it. I start a normal container on it with sleep process as PID1. See in the following output that the container clever_lalande started with sleep process.
$ docker run -d --rm alpine sleep 9999 6c541cf8f7b315783d2315eebc2f7dddd1f7b26f427e182f8597b10f2746ab0b $ docker ps CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES 6c541cf8f7b3 alpine &amp;#34;sleep 9999&amp;#34; 12 seconds ago Up 11 seconds clever_lalande Now let&amp;rsquo;s find out the process sleep on the host.</description>
    </item>
    
    <item>
      <title>Writing your own Seccomp profiles for Docker</title>
      <link>https://suraj.io/post/docker-seccomp-manual/</link>
      <pubDate>Mon, 10 Jun 2019 11:57:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/docker-seccomp-manual/</guid>
      <description>What is Seccomp?  A large number of system calls are exposed to every userland process with many of them going unused for the entire lifetime of the process. A certain subset of userland applications benefit by having a reduced set of available system calls. The resulting set reduces the total kernel surface exposed to the application. System call filtering is meant for use with those applications. Seccomp filtering provides a means for a process to specify a filter for incoming system calls.</description>
    </item>
    
    <item>
      <title>Recreate Kubernetes CVE-2017-1002101</title>
      <link>https://suraj.io/post/cve-2017-1002101-subpath-volume-mount-recreate/</link>
      <pubDate>Mon, 14 Jan 2019 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/cve-2017-1002101-subpath-volume-mount-recreate/</guid>
      <description>A volume mount CVE was discovered in Kubernetes 1.9 and older which allowed access to node file system using emptyDir volume mount using subpath. The official description goes as follows:
 In Kubernetes versions 1.3.x, 1.4.x, 1.5.x, 1.6.x and prior to versions 1.7.14, 1.8.9 and 1.9.4 containers using subpath volume mounts with any volume type (including non-privileged pods, subject to file permissions) can access files/directories outside of the volume, including the host&amp;rsquo;s filesystem.</description>
    </item>
    
    <item>
      <title>Add new Node to k8s cluster with Bootstrap token</title>
      <link>https://suraj.io/post/add-new-k8s-node-bootstrap-token/</link>
      <pubDate>Wed, 24 Oct 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/add-new-k8s-node-bootstrap-token/</guid>
      <description>NOTE: There is an updated version of this blog here.
 Few days back I wrote a blog about adding new node to the cluster using the static token file. The problem with that approach is that you need to restart kube-apiserver providing it the path to the token file. Here we will see how to use the bootstrap token, which is very dynamic in nature and can be controlled by using Kubernetes resources like secrets.</description>
    </item>
    
    <item>
      <title>PodSecurityPolicy on existing Kubernetes clusters</title>
      <link>https://suraj.io/post/psp-on-existing-cluster/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/psp-on-existing-cluster/</guid>
      <description>I enabled PodSecurityPolicy on a minikube cluster by appending PodSecurityPolicy to the apiserver flag in minikube like this:
--extra-config=apiserver.enable-admission-plugins=Initializers,NamespaceLifecycle,\  LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,\  NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,\  ResourceQuota,PodSecurityPolicy Ideally when you have PSP enabled and if you don&amp;rsquo;t define any PSP and authorize it with right RBAC no pod will start in the cluster. But what I saw was that there were some pods still running in kube-system namespace.
$ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-g2t8v 1/1 Running 4 5d11h etcd-minikube 1/1 Running 2 5d11h heapster-bn5xp 1/1 Running 2 5d11h influxdb-grafana-qzpv4 2/2 Running 4 5d11h kube-addon-manager-minikube 1/1 Running 2 5d11h kube-controller-manager-minikube 1/1 Running 1 4d20h kube-scheduler-minikube 1/1 Running 2 5d11h kubernetes-dashboard-5bb6f7c8c6-9d564 1/1 Running 8 5d11h storage-provisioner 1/1 Running 7 5d11h Which got me thinking what is wrong with the way PSPs work.</description>
    </item>
    
    <item>
      <title>Add new Node to k8s cluster with cert rotation</title>
      <link>https://suraj.io/post/add-new-k8s-node-cert-rotate/</link>
      <pubDate>Tue, 16 Oct 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/add-new-k8s-node-cert-rotate/</guid>
      <description>The setup here is created by following Kubernetes the Hard Way by Kelsey Hightower. So if you are following along in this then do all the setup till the step Bootstrapping the Kubernetes Worker Nodes. In this just don&amp;rsquo;t start the kubelet, start other services like containerd and kube-proxy.
master node Following the docs of TLS Bootstrapping, let&amp;rsquo;s first create the token authentication file. Create a file with following content:</description>
    </item>
    
    <item>
      <title>Single node Kubernetes Cluster on Fedora with SELinux enabled</title>
      <link>https://suraj.io/post/single-node-k8s-fedora-selinux/</link>
      <pubDate>Tue, 11 Sep 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/single-node-k8s-fedora-selinux/</guid>
      <description>Start a single node fedora machine, using whatever method but I have used this Vagrantfile to do it:
# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(&amp;#34;2&amp;#34;) do |config| config.vm.define &amp;#34;fedora&amp;#34; do |fedora| fedora.vm.box = &amp;#34;fedora/28-cloud-base&amp;#34; config.vm.hostname = &amp;#34;fedora&amp;#34; end config.vm.provider &amp;#34;virtualbox&amp;#34; do |virtualbox, override| virtualbox.memory = 4096 virtualbox.cpus = 4 end config.vm.provision &amp;#34;shell&amp;#34;, privileged: false, inline: &amp;lt;&amp;lt;-SHELL  echo &amp;#39;127.0.0.1 localhost&amp;#39; | cat - /etc/hosts &amp;gt; temp &amp;amp;&amp;amp; sudo mv temp /etc/hosts SHELL end Now start it and ssh into it:</description>
    </item>
    
    <item>
      <title>HostPath volumes and it&#39;s problems</title>
      <link>https://suraj.io/post/k8s-hostpat-nuke-nodes/</link>
      <pubDate>Mon, 10 Sep 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/k8s-hostpat-nuke-nodes/</guid>
      <description>This post will demonstrate how Kubernetes HostPath volumes can help you get access to the Kubernetes nodes. Atleast you can play with the filesystem of the node on which you pod is scheduled on. You can get access to other containers running on the host, certificates of the kubelet, etc.
I have a 3-master and 3-node cluster and setup using this script, running in a Vagrant environment.
All the nodes are in ready state:</description>
    </item>
    
  </channel>
</rss>
