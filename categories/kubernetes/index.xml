<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kubernetes on Suraj Deshmukh</title>
    <link>https://suraj.io/categories/kubernetes/</link>
    <description>Recent content in kubernetes on Suraj Deshmukh</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Sun, 02 Aug 2020 16:00:51 +0530</lastBuildDate>
    
	<atom:link href="https://suraj.io/categories/kubernetes/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Being Productive with Kubectl</title>
      <link>https://suraj.io/post/being-productive-with-kubectl/</link>
      <pubDate>Sun, 02 Aug 2020 16:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/being-productive-with-kubectl/</guid>
      <description>This blog will showcase my productivity tips with kubectl . This does not venture into any plugins per se. But only using bash aliases to achieve it.
Bash Aliases # k8s alias alias k=kubectl alias kg=&amp;#34;kubectl get&amp;#34; alias kgp=&amp;#34;kubectl get pods&amp;#34; alias kgs=&amp;#34;kubectl get services&amp;#34; alias kge=&amp;#34;kubectl get events&amp;#34; alias kgpvc=&amp;#34;kubectl get pvc&amp;#34; alias kgpv=&amp;#34;kubectl get pv&amp;#34; alias kd=&amp;#34;kubectl describe&amp;#34; alias kl=&amp;#34;kubectl logs -f&amp;#34; alias kc=&amp;#34;kubectl create -f&amp;#34; I have above aliases setup in the ~/.</description>
    </item>
    
    <item>
      <title>How to backup and restore Prometheus?</title>
      <link>https://suraj.io/post/how-to-backup-and-restore-prometheus/</link>
      <pubDate>Fri, 31 Jul 2020 19:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/how-to-backup-and-restore-prometheus/</guid>
      <description>This blog will show you how to take a backup from a running Prometheus and restore it in some other Prometheus instance. You might ask why would you even want to do something like that? Well, sometimes you want the Prometheus metrics because they were collected for some particular purpose and you want to do some analysis later.
Prerequisites/Assumptions This blog assumes that you have a Prometheus running that is deployed using prometheus-operator in monitoring namespace.</description>
    </item>
    
    <item>
      <title>Watch Container Traffic Without Exec</title>
      <link>https://suraj.io/post/snoop-on-pod-traffic/</link>
      <pubDate>Sat, 06 Jun 2020 20:30:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/snoop-on-pod-traffic/</guid>
      <description>Introduction For the reasons of security, many container deployments nowadays run their workloads in a scratch based image. This form of implementation helps reduce the attack surface since there is no shell to gain access to, especially if someone were to break out of the application.
But for the developers or operators of such applications, it is hard to debug. Since they lack essential tools or even bash for that matter, but the application&amp;rsquo;s debugging ability should not dictate its production deployment and compromise its security posture.</description>
    </item>
    
    <item>
      <title>Enabling Seccomp on your Prometheus Operator and related Pods</title>
      <link>https://suraj.io/post/seccomp-in-kube-state-metrics/</link>
      <pubDate>Tue, 14 Apr 2020 11:57:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/seccomp-in-kube-state-metrics/</guid>
      <description>Seccomp helps us limit the system calls the process inside container can make. And PodSecurityPolicy is the way to enable it on pods in Kubernetes.
Prometheus Operator Prometheus Operator makes it really easy to monitor your Kubernetes cluster. To deploy this behemoth, helm chart is the easiest way to do it.
Almost all the pods that run as a part of Prometheus Operator viz. Prometheus Operator, Prometheus, Alertmanager, Grafana, Kube State Metrics donâ€™t need to run with elevated privileges except Node Exporter.</description>
    </item>
    
    <item>
      <title>Make static configs available for apiserver in minikube</title>
      <link>https://suraj.io/post/apiserver-in-minikube-static-configs/</link>
      <pubDate>Sun, 20 Jan 2019 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/apiserver-in-minikube-static-configs/</guid>
      <description>If you want to provide extra flags to the kube-apiserver that runs inside minikube how do you do it? You can use the minikube&amp;rsquo;s --extra-config flag with apiserver.&amp;lt;apiserver flag&amp;gt;=&amp;lt;value&amp;gt;, for e.g. if you want to enable RBAC authorization mode you do it as follows:
--extra-config=apiserver.authorization-mode=RBAC So this is a no brainer when doing it for flags whose value can be given right away, like the one above. But what if you want to provide value which is a file path.</description>
    </item>
    
    <item>
      <title>Recreate Kubernetes CVE-2017-1002101</title>
      <link>https://suraj.io/post/cve-2017-1002101-subpath-volume-mount-recreate/</link>
      <pubDate>Mon, 14 Jan 2019 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/cve-2017-1002101-subpath-volume-mount-recreate/</guid>
      <description>A volume mount CVE was discovered in Kubernetes 1.9 and older which allowed access to node file system using emptyDir volume mount using subpath. The official description goes as follows:
 In Kubernetes versions 1.3.x, 1.4.x, 1.5.x, 1.6.x and prior to versions 1.7.14, 1.8.9 and 1.9.4 containers using subpath volume mounts with any volume type (including non-privileged pods, subject to file permissions) can access files/directories outside of the volume, including the host&amp;rsquo;s filesystem.</description>
    </item>
    
    <item>
      <title>Add new Node to k8s cluster with Bootstrap token</title>
      <link>https://suraj.io/post/add-new-k8s-node-bootstrap-token/</link>
      <pubDate>Wed, 24 Oct 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/add-new-k8s-node-bootstrap-token/</guid>
      <description>Few days back I wrote a blog about adding new node to the cluster using the static token file. The problem with that approach is that you need to restart kube-apiserver providing it the path to the token file. Here we will see how to use the bootstrap token, which is very dynamic in nature and can be controlled by using Kubernetes resources like secrets.
So if you are following Kubernetes the Hard Way to set up the cluster here are the changes you should do to adapt it to run with bootstrap token.</description>
    </item>
    
    <item>
      <title>PodSecurityPolicy on existing Kubernetes clusters</title>
      <link>https://suraj.io/post/psp-on-existing-cluster/</link>
      <pubDate>Tue, 23 Oct 2018 00:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/psp-on-existing-cluster/</guid>
      <description>I enabled PodSecurityPolicy on a minikube cluster by appending PodSecurityPolicy to the apiserver flag in minikube like this:
--extra-config=apiserver.enable-admission-plugins=Initializers,NamespaceLifecycle,\  LimitRanger,ServiceAccount,DefaultStorageClass,DefaultTolerationSeconds,\  NodeRestriction,MutatingAdmissionWebhook,ValidatingAdmissionWebhook,\  ResourceQuota,PodSecurityPolicy Ideally when you have PSP enabled and if you don&amp;rsquo;t define any PSP and authorize it with right RBAC no pod will start in the cluster. But what I saw was that there were some pods still running in kube-system namespace.
$ kubectl -n kube-system get pods NAME READY STATUS RESTARTS AGE coredns-576cbf47c7-g2t8v 1/1 Running 4 5d11h etcd-minikube 1/1 Running 2 5d11h heapster-bn5xp 1/1 Running 2 5d11h influxdb-grafana-qzpv4 2/2 Running 4 5d11h kube-addon-manager-minikube 1/1 Running 2 5d11h kube-controller-manager-minikube 1/1 Running 1 4d20h kube-scheduler-minikube 1/1 Running 2 5d11h kubernetes-dashboard-5bb6f7c8c6-9d564 1/1 Running 8 5d11h storage-provisioner 1/1 Running 7 5d11h Which got me thinking what is wrong with the way PSPs work.</description>
    </item>
    
    <item>
      <title>Road to CKA</title>
      <link>https://suraj.io/post/road-to-cka/</link>
      <pubDate>Sun, 21 Oct 2018 00:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/road-to-cka/</guid>
      <description>I passed CKA exam with 92% marks on 19th October 2018.
A lot of folks are curious about how to prepare and what resources to follow. Here is my list of things to do and list of resources that might help you on successful CKA exam.
The duration of exam is three hours, which is enough time if you do good practice. The exam is pretty straight forward and tests your Kubernetes hands-on knowledge, so whatever you read please try to do it on a real cluster.</description>
    </item>
    
    <item>
      <title>Add new Node to k8s cluster with cert rotation</title>
      <link>https://suraj.io/post/add-new-k8s-node-cert-rotate/</link>
      <pubDate>Tue, 16 Oct 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/add-new-k8s-node-cert-rotate/</guid>
      <description>The setup here is created by following Kubernetes the Hard Way by Kelsey Hightower. So if you are following along in this then do all the setup till the step Bootstrapping the Kubernetes Worker Nodes. In this just don&amp;rsquo;t start the kubelet, start other services like containerd and kube-proxy.
master node Following the docs of TLS Bootstrapping, let&amp;rsquo;s first create the token authentication file. Create a file with following content:</description>
    </item>
    
    <item>
      <title>Adding new worker to existing Kubernetes cluster</title>
      <link>https://suraj.io/post/add-new-k8s-node-manually/</link>
      <pubDate>Sun, 23 Sep 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/add-new-k8s-node-manually/</guid>
      <description>To setup a multi-node Kubernetes cluster just run this script and you will have a cluster with 3 masters and 3 workers.
$ kubectl get nodes -o wide NAME STATUS ROLES AGE VERSION INTERNAL-IP EXTERNAL-IP OS-IMAGE KERNEL-VERSION CONTAINER-RUNTIME worker-0 Ready &amp;lt;none&amp;gt; 1h v1.11.2 192.168.199.20 &amp;lt;none&amp;gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-1 Ready &amp;lt;none&amp;gt; 1h v1.11.2 192.168.199.21 &amp;lt;none&amp;gt; Ubuntu 18.04.1 LTS 4.15.0-33-generic cri-o://1.11.2 worker-2 Ready &amp;lt;none&amp;gt; 1h v1.11.2 192.168.199.22 &amp;lt;none&amp;gt; Ubuntu 18.</description>
    </item>
    
    <item>
      <title>Single node Kubernetes Cluster on Fedora with SELinux enabled</title>
      <link>https://suraj.io/post/single-node-k8s-fedora-selinux/</link>
      <pubDate>Tue, 11 Sep 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/single-node-k8s-fedora-selinux/</guid>
      <description>Start a single node fedora machine, using whatever method but I have used this Vagrantfile to do it:
# -*- mode: ruby -*- # vi: set ft=ruby : Vagrant.configure(&amp;#34;2&amp;#34;) do |config| config.vm.define &amp;#34;fedora&amp;#34; do |fedora| fedora.vm.box = &amp;#34;fedora/28-cloud-base&amp;#34; config.vm.hostname = &amp;#34;fedora&amp;#34; end config.vm.provider &amp;#34;virtualbox&amp;#34; do |virtualbox, override| virtualbox.memory = 4096 virtualbox.cpus = 4 end config.vm.provision &amp;#34;shell&amp;#34;, privileged: false, inline: &amp;lt;&amp;lt;-SHELL  echo &amp;#39;127.0.0.1 localhost&amp;#39; | cat - /etc/hosts &amp;gt; temp &amp;amp;&amp;amp; sudo mv temp /etc/hosts SHELL end Now start it and ssh into it:</description>
    </item>
    
    <item>
      <title>HostPath volumes and it&#39;s problems</title>
      <link>https://suraj.io/post/k8s-hostpat-nuke-nodes/</link>
      <pubDate>Mon, 10 Sep 2018 01:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/k8s-hostpat-nuke-nodes/</guid>
      <description>This post will demonstrate how Kubernetes HostPath volumes can help you get access to the Kubernetes nodes. Atleast you can play with the filesystem of the node on which you pod is scheduled on. You can get access to other containers running on the host, certificates of the kubelet, etc.
I have a 3-master and 3-node cluster and setup using this script, running in a Vagrant environment.
All the nodes are in ready state:</description>
    </item>
    
    <item>
      <title>Change namespaces in Kubernetes</title>
      <link>https://suraj.io/post/changing-k8s-ns/</link>
      <pubDate>Mon, 02 Jul 2018 08:00:51 +0530</pubDate>
      
      <guid>https://suraj.io/post/changing-k8s-ns/</guid>
      <description>There is no easy way to change namespace in Kubernetes using kubectl command line utility. But here are some commands that you can alias in your bashrc file so that it&amp;rsquo;s just a single command that you can use to change the namespace in the Kubernetes cluster.
Change namespace Let&amp;rsquo;s see step by step what goes in to change the namespace. So the first step is to find the context.</description>
    </item>
    
    <item>
      <title>Using private container registries from minikube</title>
      <link>https://suraj.io/post/private-registry-from-minikube/</link>
      <pubDate>Fri, 06 Oct 2017 19:32:33 +0530</pubDate>
      
      <guid>https://suraj.io/post/private-registry-from-minikube/</guid>
      <description>I am doing Kubernetes native development using minikube. And for doing that I had to download a Container image that is available in internally hosted private container registry.
On the configuration side of doing that you will need to create Kubernetes Secret of type docker-registry. And now refer that secret you just created in your Pod manifest under pod.spec.imagePullSecrets. For more info follow the tutorial in Kubernetes docs on Pull an Image from a Private Registry.</description>
    </item>
    
    <item>
      <title>Static Pods using Kubelet on Fedora</title>
      <link>https://suraj.io/post/static-pods/</link>
      <pubDate>Sat, 23 Sep 2017 13:10:14 +0530</pubDate>
      
      <guid>https://suraj.io/post/static-pods/</guid>
      <description>I wanted to try out Standalone Kubelet Tutorial of Kelsey Hightower by myself but I could not follow it as it is, because it was firstly on GCE and secondly it uses CoreOS, but since I am very familiar to Fedora I thought of following that tutorial on it. To get a quick setup of a fresh Fedora machine use Vagrant. I have used Vagrantfile available here.
This blog is only replacement of section Install the Standalone Kubelet in tutorial.</description>
    </item>
    
    <item>
      <title>Kubernetes Learning resources</title>
      <link>https://suraj.io/post/k8s-learning-resources/</link>
      <pubDate>Thu, 17 Aug 2017 23:12:18 +0530</pubDate>
      
      <guid>https://suraj.io/post/k8s-learning-resources/</guid>
      <description>Following is the list of all the places you can learn Kubernetes from:
 Scalable Microservices with Kubernetes - Video tutorial Fundamentals of Containers, Kubernetes, and Red Hat OpenShift - Video tutorial Kubernetes By Example - DIY tutorial Learn Kubernetes using Interactive Browser-Based Scenarios - DIY tutorial in your own web browser Interactive Learning Portal for OpenShift - DIY tutorial in your own web browser Kubernetes docs - Textual DIY docs Kubernetes API reference v1.</description>
    </item>
    
    <item>
      <title>Enabling local development with Kubernetes</title>
      <link>https://suraj.io/post/enabling-local-development-with-k8s/</link>
      <pubDate>Sun, 23 Apr 2017 15:57:07 +0530</pubDate>
      
      <guid>https://suraj.io/post/enabling-local-development-with-k8s/</guid>
      <description>I want to show how you can enable Kubernetes in your day to day development workflow. So that you get the feel of production deployment locally from day 1.
I have a flask application which I am working on. The basic directory structure looks like this:
$ ll total 24 -rw-rw-r--. 1 foo foo 427 Apr 23 16:23 app.py -rw-rw-r--. 1 foo foo 201 Apr 23 16:55 docker-compose.yml -rw-rw-r--. 1 foo foo 363 Apr 23 16:21 Dockerfile -rwxrwxr-x.</description>
    </item>
    
    <item>
      <title>Quick PV for local Kubernetes cluster</title>
      <link>https://suraj.io/post/quick-pv-for-local-k8s/</link>
      <pubDate>Tue, 18 Apr 2017 23:56:15 +0530</pubDate>
      
      <guid>https://suraj.io/post/quick-pv-for-local-k8s/</guid>
      <description>I do lot of Kubernetes related work either on minikube or local OpenShift cluster setup in a VM. Often I need to create a PersistentVolumeClaim a.k.a. pvc. But to use pvc you have to have a PersistentVolume or pv defined.
Enter into the machine running k8s If using minikube you can do
minikube ssh Create a local directory for storage mkdir /tmp/pv0001 chmod 777 /tmp/pv0001 If you are on a machine that has SELinux enabled do the following</description>
    </item>
    
    <item>
      <title>k8s on CRI-O - single node</title>
      <link>https://suraj.io/post/using-crio-with-k8s-single-node/</link>
      <pubDate>Sat, 08 Apr 2017 00:11:37 +0530</pubDate>
      
      <guid>https://suraj.io/post/using-crio-with-k8s-single-node/</guid>
      <description>Here is a single node Kubernetes on CRI-O. This setup is done on Fedora 25.
Installing OS dependencies dnf -y install \  go \  git \  btrfs-progs-devel \  device-mapper-devel \  glib2-devel \  glibc-devel \  glibc-static \  gpgme-devel \  libassuan-devel \  libgpg-error-devel \  libseccomp-devel \  libselinux-devel \  pkgconfig \  wget \  etcd \  iptables Creating go environment cd ~ mkdir -p ~/go export GOPATH=~/go export GOBIN=$GOPATH/bin export PATH=$PATH:$GOBIN echo &amp;#39;GOPATH=~/go&amp;#39; &amp;gt;&amp;gt; ~/.</description>
    </item>
    
  </channel>
</rss>